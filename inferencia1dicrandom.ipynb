{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T10:54:31.25298Z",
     "start_time": "2020-10-25T10:54:29.952487Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-05T23:54:15.866421Z",
     "iopub.status.busy": "2020-12-05T23:54:15.865722Z",
     "iopub.status.idle": "2020-12-05T23:54:18.676448Z",
     "shell.execute_reply": "2020-12-05T23:54:18.674776Z"
    },
    "papermill": {
     "duration": 2.88073,
     "end_time": "2020-12-05T23:54:18.676583",
     "exception": false,
     "start_time": "2020-12-05T23:54:15.795853",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../input/iterative-stratification/iterative-stratification-master')\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import copy\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.preprocessing import StandardScaler, QuantileTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from notebook import notebookapp\n",
    "import urllib\n",
    "import json\n",
    "import ipykernel\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.stats import norm\n",
    "# from sklearn.naive_bayes import ComplementNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "# from scipy.stats import ks_2samp\n",
    "import pickle\n",
    "import gc\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import bz2\n",
    "import pickle\n",
    "import _pickle as cPickle\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.options.display.max_columns = 1000\n",
    "\n",
    "TEST_PRIVATE = False\n",
    "TEST_LOCAL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.056495,
     "end_time": "2020-12-05T23:54:18.790686",
     "exception": false,
     "start_time": "2020-12-05T23:54:18.734191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T10:54:31.261272Z",
     "start_time": "2020-10-25T10:54:31.255107Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-05T23:54:18.916256Z",
     "iopub.status.busy": "2020-12-05T23:54:18.915408Z",
     "iopub.status.idle": "2020-12-05T23:54:18.918295Z",
     "shell.execute_reply": "2020-12-05T23:54:18.917791Z"
    },
    "papermill": {
     "duration": 0.070082,
     "end_time": "2020-12-05T23:54:18.918388",
     "exception": false,
     "start_time": "2020-12-05T23:54:18.848306",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "        \"\"\"\n",
    "        :param predicted:   The predicted probabilities as floats between 0-1\n",
    "        :param actual:      The binary labels. Either 0 or 1.\n",
    "        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "        \"\"\"\n",
    "        \n",
    "        p1 = actual * np.log(predicted+eps)\n",
    "        p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "        loss = p0 + p1\n",
    "\n",
    "        return -loss.mean()\n",
    "    \n",
    "def log_loss_multi(y_true, y_pred):\n",
    "    M = y_true.shape[1]\n",
    "    results = np.zeros(M)\n",
    "    for i in range(M):\n",
    "        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "    return results.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.085698,
     "end_time": "2020-12-05T23:54:19.083447",
     "exception": false,
     "start_time": "2020-12-05T23:54:18.997749",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### DataBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T23:54:19.262823Z",
     "iopub.status.busy": "2020-12-05T23:54:19.261916Z",
     "iopub.status.idle": "2020-12-05T23:54:19.265601Z",
     "shell.execute_reply": "2020-12-05T23:54:19.266233Z"
    },
    "papermill": {
     "duration": 0.09345,
     "end_time": "2020-12-05T23:54:19.266406",
     "exception": false,
     "start_time": "2020-12-05T23:54:19.172956",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    num_folds = 10\n",
    "    use_var_enc = True\n",
    "    variance_thres = 0.60\n",
    "    quantile_transform = True\n",
    "    use_rankgauss = True\n",
    "    \n",
    "    selec_top = True\n",
    "    original_feats = True #False remove also top features\n",
    "    \n",
    "    use_pca = True\n",
    "    pca_comp_genes = 29 #30\n",
    "    pca_comp_cells = 4 #18\n",
    "    \n",
    "    use_fastica = False\n",
    "    fastica_comp_genes = 10\n",
    "    fastica_comp_cells = 5\n",
    "    \n",
    "    use_kridge = True\n",
    "    use_xgb = False\n",
    "    use_bayes = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.079884,
     "end_time": "2020-12-05T23:54:19.427903",
     "exception": false,
     "start_time": "2020-12-05T23:54:19.348019",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Load Databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-25T10:54:38.042114Z",
     "start_time": "2020-10-25T10:54:31.287322Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-05T23:54:19.617840Z",
     "iopub.status.busy": "2020-12-05T23:54:19.617013Z",
     "iopub.status.idle": "2020-12-05T23:54:28.094877Z",
     "shell.execute_reply": "2020-12-05T23:54:28.094249Z"
    },
    "papermill": {
     "duration": 8.583202,
     "end_time": "2020-12-05T23:54:28.094998",
     "exception": false,
     "start_time": "2020-12-05T23:54:19.511796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23814, 1083)\n",
      "(21948, 207)\n",
      "(3982, 207)\n",
      "(21948, 1082) (3624, 875)\n"
     ]
    }
   ],
   "source": [
    "seed_everything(seed=42)\n",
    "\n",
    "train_features = pd.read_csv('../input/lish-moa/train_features.csv')\n",
    "train_targets_scored = pd.read_csv('../input/lish-moa/train_targets_scored.csv')\n",
    "train_targets_nonscored = pd.read_csv('../input/lish-moa/train_targets_nonscored.csv')\n",
    "train_drug = pd.read_csv('../input/lish-moa/train_drug.csv')\n",
    "\n",
    "drug_dict = dict(zip(train_drug['sig_id'],train_drug['drug_id']))\n",
    "\n",
    "test_features = pd.read_csv('../input/lish-moa/test_features.csv')\n",
    "\n",
    "if TEST_PRIVATE:\n",
    "    id1 = [test_features['sig_id'].values]\n",
    "    for other_index in ['A', 'B', 'C']:\n",
    "        id1.append(id1[0]+other_index)\n",
    "    test_features = pd.concat([test_features, test_features, test_features, test_features],sort=False).reset_index(drop=True)\n",
    "    test_features['sig_id'] = np.concatenate(id1)\n",
    "    print(test_features.shape)\n",
    "\n",
    "sample_submission = pd.read_csv('../input/lish-moa/sample_submission.csv')\n",
    "\n",
    "train_orig = train_features.merge(train_targets_scored, on='sig_id')\n",
    "train_orig = train_orig.merge(train_drug, on='sig_id', how='left')\n",
    "\n",
    "train_noctl = train_orig[train_orig['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1).reset_index(drop=True)\n",
    "train_ctl = train_orig[train_orig['cp_type']=='ctl_vehicle'].drop('cp_type', axis=1).reset_index(drop=True)\n",
    "\n",
    "test_noctl = test_features[test_features['cp_type']!='ctl_vehicle'].drop('cp_type', axis=1).reset_index(drop=True)\n",
    "test_ctl = test_features[test_features['cp_type']=='ctl_vehicle'].drop('cp_type', axis=1).reset_index(drop=True)\n",
    "\n",
    "target = train_noctl[train_targets_scored.columns]\n",
    "\n",
    "GENES = [col for col in train_features.columns if col.startswith('g-')]\n",
    "CELLS = [col for col in train_features.columns if col.startswith('c-')]\n",
    "target_cols = target.drop('sig_id', axis=1).columns.values.tolist()\n",
    "other_feats = []\n",
    "\n",
    "# folds = train_noctl.copy()\n",
    "# mskf = MultilabelStratifiedKFold(n_splits=CFG.num_folds, shuffle=True, random_state=43)\n",
    "\n",
    "# for f, (t_idx, v_idx) in enumerate(mskf.split(X=train_noctl, y=target)):\n",
    "#     folds.loc[v_idx, 'kfold'] = int(f)\n",
    "\n",
    "# folds['kfold'] = folds['kfold'].astype(int)\n",
    "\n",
    "folds = train_noctl.copy()\n",
    "\n",
    "print(train_orig.shape)\n",
    "print(target.shape)\n",
    "print(sample_submission.shape)\n",
    "print(folds.shape, test_noctl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058521,
     "end_time": "2020-12-05T23:54:28.212546",
     "exception": false,
     "start_time": "2020-12-05T23:54:28.154025",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Create Kridge Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-05T23:54:28.345771Z",
     "iopub.status.busy": "2020-12-05T23:54:28.345145Z",
     "iopub.status.idle": "2020-12-06T00:01:09.035350Z",
     "shell.execute_reply": "2020-12-06T00:01:09.035978Z"
    },
    "papermill": {
     "duration": 400.766118,
     "end_time": "2020-12-06T00:01:09.036177",
     "exception": false,
     "start_time": "2020-12-05T23:54:28.270059",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe0d773d7e74599a214c7aae3ab962c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=206.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "(21948, 154) (3624, 154)\n"
     ]
    }
   ],
   "source": [
    "min_num_ones = 20\n",
    "folds_kridge = pd.read_csv('../input/kridgefeats25oct/folds.csv')\n",
    "test_kridge = test_noctl[['sig_id']].copy()\n",
    "\n",
    "colsk_selec = []\n",
    "min_num_ones = 20\n",
    "for ncol, col_target in enumerate(tqdm(target_cols)):\n",
    "    name_feat = f'kridge_{col_target}'\n",
    "    if np.sum(folds[col_target].sum())>=min_num_ones:\n",
    "        colsk_selec.append(name_feat)\n",
    "        # Save all\n",
    "        with bz2.BZ2File(f'../input/kridgefeats25oct/feats__{name_feat}.pbz2','rb') as file:\n",
    "            model = cPickle.load(file)\n",
    "            feats_trn_uncorr = cPickle.load(file)\n",
    "            \n",
    "#         with open(f'../input/kridgefeats/model__{name_feat}__{col_target}.pkl', 'rb') as file:\n",
    "#             model = pickle.load(file)\n",
    "#             feats_trn_uncorr = pickle.load(file)\n",
    "\n",
    "        # With Test DB\n",
    "        X_trn = folds[feats_trn_uncorr].astype(float).values\n",
    "        X_test = test_noctl[feats_trn_uncorr].astype(float).values\n",
    "        y = folds[col_target].astype(float).values \n",
    "\n",
    "        scaler = StandardScaler()\n",
    "        X_trn = scaler.fit_transform(X_trn)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        # Training matrix has been removed to save memory\n",
    "        model.X_fit_ = X_trn\n",
    "    #     model = KernelRidge(alpha = alpha, kernel = 'rbf')\n",
    "    #     model.fit(X_trn, y)\n",
    "        test_kridge[name_feat] = model.predict(X_test)\n",
    "\n",
    "kridge_cols = ['sig_id']+colsk_selec\n",
    "print(folds_kridge[kridge_cols].shape, test_kridge[kridge_cols].shape)\n",
    "\n",
    "# Comprueba con original\n",
    "if TEST_LOCAL and not TEST_PRIVATE:\n",
    "    test_feat_orig = pd.read_csv('../input/kridgefeats25oct/test.csv')\n",
    "    print(np.sum(test_feat_orig[colsk_selec].values - test_kridge[colsk_selec].values))\n",
    "    del test_feat_orig\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.949Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:09.159135Z",
     "iopub.status.busy": "2020-12-06T00:01:09.158552Z",
     "iopub.status.idle": "2020-12-06T00:01:09.162905Z",
     "shell.execute_reply": "2020-12-06T00:01:09.162391Z"
    },
    "papermill": {
     "duration": 0.067223,
     "end_time": "2020-12-06T00:01:09.163006",
     "exception": false,
     "start_time": "2020-12-06T00:01:09.095783",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# min_num_ones = 20\n",
    "# folds_kridge = pd.read_csv('../input/kridgefeats/folds.csv')\n",
    "# test_kridge = test_noctl['sig_id'].copy()\n",
    "\n",
    "# colsk_selec = []\n",
    "# min_num_ones = 20\n",
    "# for ncol, col_target in enumerate(tqdm(target_cols)):\n",
    "#     name_feat = f'kridge_{col_target}'\n",
    "#     if np.sum(folds[col_target].sum())>=min_num_ones:\n",
    "#         colsk_selec.append(name_feat)\n",
    "#         # Save all\n",
    "#         with open(f'../input/kridgefeats/model__{name_feat}__{col_target}.pkl', 'rb') as file:\n",
    "#             model = pickle.load(file)\n",
    "#             feats_trn_uncorr = pickle.load(file)\n",
    "\n",
    "#         # With Test DB\n",
    "#         X_trn = folds[feats_trn_uncorr].astype(float).values\n",
    "#         X_test = test_noctl[feats_trn_uncorr].astype(float).values\n",
    "#         y = folds[col_target].astype(float).values \n",
    "\n",
    "#         scaler = StandardScaler()\n",
    "#         X_trn = scaler.fit_transform(X_trn)\n",
    "#         X_test = scaler.transform(X_test)\n",
    "\n",
    "#     #     model = KernelRidge(alpha = alpha, kernel = 'rbf')\n",
    "#     #     model.fit(X_trn, y)\n",
    "#         test_kridge[name_feat] = model.predict(X_test)\n",
    "\n",
    "# kridge_cols = ['sig_id']+colsk_selec\n",
    "# print(folds_kridge[kridge_cols].shape, test_kridge[kridge_cols].shape)\n",
    "\n",
    "# # Comprueba con original\n",
    "# if False:\n",
    "#     test_feat_orig = pd.read_csv('../input/kridgefeats/test.csv')\n",
    "#     print(np.sum(test_feat_orig[colsk_selec].values - test_kridge[colsk_selec].values))\n",
    "#     del test_feat_orig\n",
    "#     gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.954Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:09.289529Z",
     "iopub.status.busy": "2020-12-06T00:01:09.287576Z",
     "iopub.status.idle": "2020-12-06T00:01:09.290296Z",
     "shell.execute_reply": "2020-12-06T00:01:09.290793Z"
    },
    "papermill": {
     "duration": 0.070049,
     "end_time": "2020-12-06T00:01:09.290924",
     "exception": false,
     "start_time": "2020-12-06T00:01:09.220875",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create KRIDGE Features\n",
    "# def trimm_correlated(df_in, threshold):\n",
    "#     df_corr = df_in.corr(method='pearson', min_periods=1)\n",
    "#     df_not_correlated = ~(df_corr.mask(np.tril(np.ones([len(df_corr)]*2, dtype=bool))).abs() > threshold).any()\n",
    "#     un_corr_idx = df_not_correlated.loc[df_not_correlated[df_not_correlated.index] == True].index\n",
    "#     return un_corr_idx\n",
    "\n",
    "# from scipy.stats import norm\n",
    "# from sklearn.naive_bayes import ComplementNB\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.metrics import roc_auc_score\n",
    "# from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from sklearn.kernel_ridge import KernelRidge\n",
    "# from scipy.stats import ks_2samp\n",
    "# import pickle\n",
    "\n",
    "# folds_feat = folds.copy()\n",
    "# test_feat = test_noctl.copy()\n",
    "\n",
    "# folds_feat['cp_dose'] = (folds_feat['cp_dose']=='D2').astype(float)\n",
    "# folds_feat['cp_time'] /= 72.0\n",
    "\n",
    "# test_feat['cp_dose'] = (test_feat['cp_dose']=='D2').astype(float)\n",
    "# test_feat['cp_time'] /= 72.0\n",
    "\n",
    "# # C_logistic = 0.01\n",
    "# thr= 0.05 #p-value thr\n",
    "# thr2 = 0.93 #Correlation Thr\n",
    "\n",
    "# # for thr in [0.05]: #1, 0.03, 0.05, 0.07, 0.09]:\n",
    "# #     for thr2 in [0.90, 0.93, 0.95, 0.97, 1.00]:\n",
    "# res = []\n",
    "# res_best = []\n",
    "# features_models = []\n",
    "# list_target_cols = []\n",
    "# list_new_cols = []\n",
    "# scaler = StandardScaler()\n",
    "# list_AUC = []\n",
    "# list_loss = []\n",
    "\n",
    "# for ncol, col_target in enumerate(tqdm(target_cols)):\n",
    "# #             if ncol<=50: #00:\n",
    "#     list_target_cols.append(col_target)\n",
    "#     name_feat = f'kridge_{col_target}'\n",
    "#     list_new_cols.append(name_feat)\n",
    "\n",
    "#     # Compare distribution of positives and negatives with kolgomorov\n",
    "#     pvalues = []\n",
    "#     for col_feat in GENES+CELLS:\n",
    "#         values_orig = train_noctl[col_feat].values\n",
    "#         target_orig = train_noctl[col_target].values\n",
    "#         target = target_orig[values_orig> -4]\n",
    "#         values = values_orig[values_orig> -4]\n",
    "#         positive = values[target==1]\n",
    "#         negative = values[target==0]\n",
    "#         if len(positive)>0:\n",
    "#             pvalue = ks_2samp(positive, negative)[1]\n",
    "#         else:\n",
    "#             pvalue = 1.0\n",
    "#         pvalues.append(pvalue)\n",
    "#     pvalues = np.array(pvalues)\n",
    "\n",
    "#     # Search best model with different thresholds\n",
    "#     best_auc = 0.0\n",
    "#     best_loss = 999999.9\n",
    "#     best_thr = 0.0 # thr=pvalue\n",
    "#     best_thr2 = 0.0 #th2=correlation\n",
    "#     best_lenfeats = 0\n",
    "#     best_lenfeats_uncorr = 0\n",
    "#     best_list_feats = []\n",
    "#     alpha = 100\n",
    "#     folds_feat[name_feat] = 0.0\n",
    "#     feats_trn = np.array(GENES+CELLS)[pvalues<thr]\n",
    "#     feats_trn_uncorr = trimm_correlated(folds_feat[feats_trn], thr2).tolist()\n",
    "#     if len(feats_trn_uncorr)>0:\n",
    "#         X_trn = folds_feat.loc[:, feats_trn_uncorr].astype(float).values\n",
    "#         X_trn = scaler.fit_transform(X_trn)\n",
    "\n",
    "#         for fold in range(CFG.num_folds):\n",
    "#             X_trn_fold = X_trn[folds_feat['kfold']!=fold, :]\n",
    "#             X_val_fold = X_trn[folds_feat['kfold']==fold, :]\n",
    "#             y_trn_fold = folds_feat.loc[folds_feat['kfold']!=fold, col_target].astype(float).values\n",
    "#             if np.sum(y_trn_fold)>10:\n",
    "#                 model = KernelRidge(alpha = alpha, kernel = 'rbf')\n",
    "# #                 model = LogisticRegression(C=C_logistic)\n",
    "#                 model.fit(X_trn_fold, y_trn_fold)\n",
    "# #                 folds_feat.loc[folds_feat['kfold']==fold, name_feat] = model.predict_proba(X_val_fold)[:,1]\n",
    "#                 folds_feat.loc[folds_feat['kfold']==fold, name_feat] = model.predict(X_val_fold)\n",
    "\n",
    "#         auc = roc_auc_score(folds_feat[col_target].values, folds_feat[name_feat].values)\n",
    "#         list_AUC.append(auc)\n",
    "#         loss = log_loss(folds_feat[col_target].values, folds_feat[name_feat].values)\n",
    "#         list_loss.append(loss)\n",
    "#         lenfeats = len(feats_trn)\n",
    "#         lenfeats_uncorr = len(feats_trn_uncorr)\n",
    "#         res.append(dict({'ncol':ncol, 'col':col_target, 'thr':thr, 'thr2':thr2, \\\n",
    "#                          'lenfeats':lenfeats, 'lenfeats_uncorr':lenfeats_uncorr, \\\n",
    "#                          'sumones':folds_feat[col_target].sum(), 'AUC':auc, 'Mean_AUC':np.mean(list_AUC), \\\n",
    "#                          'loss':loss, 'Mean_loss':np.mean(list_loss)}))\n",
    "# #                         print(f'{ncol+1}/{len(target_cols)}: col={col_target} thr={thr} thr2={thr2} lenfeats={lenfeats} lenfeats_uncorr={lenfeats_uncorr} sum_ones={folds_feat[col_target].sum()} loss={loss} AUC={auc} Mean_AUC={np.mean(list_AUC)}')\n",
    "#         if loss < best_loss:\n",
    "#             best_auc = auc\n",
    "#             best_loss = loss\n",
    "#             best_thr = thr\n",
    "#             best_thr2 = thr2\n",
    "#             best_lenfeats = lenfeats\n",
    "#             best_lenfeats_uncorr = lenfeats_uncorr\n",
    "#             preds = folds_feat[name_feat].values\n",
    "#             best_list_feats = feats_trn_uncorr\n",
    "\n",
    "#         # Get best\n",
    "#         loss = best_loss\n",
    "#         auc = best_auc\n",
    "#         thr = best_thr\n",
    "#         thr2 = best_thr2\n",
    "#         lenfeats = best_lenfeats\n",
    "#         lenfeats_uncorr = best_lenfeats_uncorr\n",
    "#         folds_feat[name_feat] = preds\n",
    "#         feats_trn_uncorr = best_list_feats\n",
    "\n",
    "#         # Provisional mlogloss\n",
    "#         valid_results_score = train_targets_scored[['sig_id']].merge(folds_feat[['sig_id']+list_new_cols], on='sig_id', how='left').fillna(0)\n",
    "#         y_true = train_targets_scored[list_target_cols].values\n",
    "#         valid_results_score.columns = ['sig_id']+list_target_cols\n",
    "#         y_pred = valid_results_score[list_target_cols].values\n",
    "#         score = 0\n",
    "#         for i in range(len(list_target_cols)):\n",
    "#             score_ = log_loss(y_true[:, i], y_pred[:, i])\n",
    "#             score += score_ \n",
    "#         score /= len(list_target_cols)\n",
    "# #         print(C_logistic, thr, thr2, np.mean(list_AUC))\n",
    "#         print(f'BEST {ncol+1}/{len(target_cols)}: col={col_target} thr={thr} thr2={thr2} lenfeats={lenfeats} lenfeats_uncorr={lenfeats_uncorr} sum_ones={folds_feat[col_target].sum()} loss={loss} AUC={auc} Mean_AUC={np.mean(list_AUC)} mlogloss={score}')\n",
    "        \n",
    "#         # With Test DB\n",
    "#         X_trn = folds_feat[feats_trn_uncorr].astype(float).values\n",
    "#         X_test = test_feat[feats_trn_uncorr].astype(float).values\n",
    "#         y = folds_feat[col_target].astype(float).values \n",
    "\n",
    "#         scaler = StandardScaler()\n",
    "#         X_trn = scaler.fit_transform(X_trn)\n",
    "#         X_test = scaler.transform(X_test)\n",
    "\n",
    "#         model = KernelRidge(alpha = alpha, kernel = 'rbf')\n",
    "# #         model = LogisticRegression(C=C_logistic)\n",
    "#         model.fit(X_trn, y)\n",
    "# #         test_feat[name_feat] = model.predict_proba(X_test)[:,1]\n",
    "#         test_feat[name_feat] = model.predict(X_test)\n",
    "\n",
    "#         # Save all\n",
    "#         with open(output_dir / f'model__{name_feat}__{col_target}.pkl', 'wb') as file:\n",
    "#             pickle.dump(model, file)\n",
    "#             pickle.dump(feats_trn_uncorr, file)\n",
    "        \n",
    "#         folds_feat.to_csv( output_dir / 'folds.csv',index=False)\n",
    "#         test_feat.to_csv(output_dir / 'test.csv',index=False)\n",
    "        \n",
    "#         res_best.append(dict({'ncol':ncol, 'col':col_target, 'thr':thr, 'thr2':thr2, \\\n",
    "#                      'lenfeats':lenfeats, 'lenfeats_uncorr':lenfeats_uncorr, \\\n",
    "#                      'sumones':folds_feat[col_target].sum(), 'AUC':auc, 'loss':loss, 'mlogloss':score}))\n",
    "#         res_df = pd.DataFrame(res_best)\n",
    "#         res_df.to_csv(output_dir / 'res.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.057928,
     "end_time": "2020-12-06T00:01:09.409313",
     "exception": false,
     "start_time": "2020-12-06T00:01:09.351385",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### RankGauss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:09.535451Z",
     "iopub.status.busy": "2020-12-06T00:01:09.534171Z",
     "iopub.status.idle": "2020-12-06T00:01:17.895539Z",
     "shell.execute_reply": "2020-12-06T00:01:17.894760Z"
    },
    "papermill": {
     "duration": 8.428346,
     "end_time": "2020-12-06T00:01:17.895650",
     "exception": false,
     "start_time": "2020-12-06T00:01:09.467304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#From: https://www.kaggle.com/kushal1506/moa-pytorch-0-01859-rankgauss-pca-nn\n",
    "folds_cp = folds.copy()\n",
    "test_noctl_cp = test_noctl.copy()\n",
    "\n",
    "if CFG.use_rankgauss:\n",
    "    for col in (GENES + CELLS):\n",
    "        transformer = QuantileTransformer(n_quantiles=100,random_state=42, output_distribution=\"normal\")\n",
    "        vec_len = len(folds_cp[col].values)\n",
    "        vec_len_test = len(test_noctl_cp[col].values)\n",
    "        raw_vec = folds_cp[col].values.reshape(vec_len, 1)\n",
    "        transformer.fit(raw_vec)\n",
    "\n",
    "        folds_cp[col] = transformer.transform(raw_vec).reshape(1, vec_len)[0]\n",
    "        test_noctl_cp[col] = transformer.transform(test_noctl_cp[col].values.reshape(vec_len_test, 1)).reshape(1, vec_len_test)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.058852,
     "end_time": "2020-12-06T00:01:18.018895",
     "exception": false,
     "start_time": "2020-12-06T00:01:17.960043",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### KMeans Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:18.152325Z",
     "iopub.status.busy": "2020-12-06T00:01:18.151584Z",
     "iopub.status.idle": "2020-12-06T00:01:39.706035Z",
     "shell.execute_reply": "2020-12-06T00:01:39.705548Z"
    },
    "papermill": {
     "duration": 21.627698,
     "end_time": "2020-12-06T00:01:39.706141",
     "exception": false,
     "start_time": "2020-12-06T00:01:18.078443",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1089) (3624, 882)\n",
      "(21948, 1096) (3624, 889)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import gc\n",
    "\n",
    "def create_cluster(train, test, features, kind = 'g', n_clusters = 35):\n",
    "    train_ = train[features].copy()\n",
    "    test_ = test[features].copy()\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    train_ = scaler.fit_transform(train_)\n",
    "    test_ = scaler.transform(test_)\n",
    "    model = KMeans(n_clusters = n_clusters, random_state = 123).fit(train_)\n",
    "    name_cols = [f'kmeans_{kind}_{i}' for i in range(n_clusters)]\n",
    "    train = pd.concat([train, pd.DataFrame(model.transform(train_), columns=name_cols)], axis=1)\n",
    "    test = pd.concat([test, pd.DataFrame(model.transform(test_), columns=name_cols)], axis=1)\n",
    "    print(train.shape, test.shape)\n",
    "    return train, test\n",
    "    \n",
    "folds_cp2 = folds.copy()\n",
    "test_noctl_cp2 = test_noctl.copy()\n",
    "folds_cp2, test_noctl_cp2 = create_cluster(folds_cp2,test_noctl_cp2, GENES, kind = 'g', n_clusters = 7)\n",
    "folds_cp2, test_noctl_cp2 = create_cluster(folds_cp2,test_noctl_cp2, CELLS, kind = 'c', n_clusters = 7)\n",
    "\n",
    "folds_3models = folds_cp2[list(folds_cp2.columns.values[np.where(folds_cp2.columns=='kmeans_g_0')[0][0]:])]\n",
    "test_noctl_3model = test_noctl_cp2[list(folds_cp2.columns.values[np.where(folds_cp2.columns=='kmeans_g_0')[0][0]:])]\n",
    "del folds_cp2, test_noctl_cp2\n",
    "gc.collect()\n",
    "\n",
    "# train_features ,test_features=fe_cluster(train_features,test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.059635,
     "end_time": "2020-12-06T00:01:39.824794",
     "exception": false,
     "start_time": "2020-12-06T00:01:39.765159",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### PCA Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:39.954800Z",
     "iopub.status.busy": "2020-12-06T00:01:39.954040Z",
     "iopub.status.idle": "2020-12-06T00:01:42.096676Z",
     "shell.execute_reply": "2020-12-06T00:01:42.096077Z"
    },
    "papermill": {
     "duration": 2.212072,
     "end_time": "2020-12-06T00:01:42.096803",
     "exception": false,
     "start_time": "2020-12-06T00:01:39.884731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1111) (3624, 904)\n",
      "(21948, 1115) (3624, 908)\n"
     ]
    }
   ],
   "source": [
    "if CFG.use_pca:\n",
    "    seed_everything(seed=42)\n",
    "    etiq = ['G','C']\n",
    "    num_pca = [CFG.pca_comp_genes, CFG.pca_comp_cells]\n",
    "    for niter, cols in enumerate([GENES, CELLS]):\n",
    "        # PCA for train with folds\n",
    "        num_comp = num_pca[niter]\n",
    "        columns_pca = [f'pca_{etiq[niter]}-{i}' for i in range(num_comp)]\n",
    "        other_feats += columns_pca\n",
    "\n",
    "        # PCA for train\n",
    "        pca = PCA(n_components=num_comp, random_state=42).fit(folds_cp[cols])\n",
    "        train_pca = pca.transform(folds_cp[cols])\n",
    "        train_pca = pd.DataFrame(train_pca, columns=columns_pca)\n",
    "        folds = pd.concat((folds, train_pca), axis=1)\n",
    "        \n",
    "        # PCA for train\n",
    "        test_pca = pca.transform(test_noctl_cp[cols])\n",
    "        test_pca = pd.DataFrame(test_pca, columns=columns_pca)\n",
    "        test_noctl = pd.concat((test_noctl, test_pca), axis=1)\n",
    "        print(folds.shape, test_noctl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.958Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:42.223315Z",
     "iopub.status.busy": "2020-12-06T00:01:42.222733Z",
     "iopub.status.idle": "2020-12-06T00:01:42.226929Z",
     "shell.execute_reply": "2020-12-06T00:01:42.226350Z"
    },
    "papermill": {
     "duration": 0.068969,
     "end_time": "2020-12-06T00:01:42.227033",
     "exception": false,
     "start_time": "2020-12-06T00:01:42.158064",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if CFG.use_pca:\n",
    "#     seed_everything(seed=42)\n",
    "#     etiq = ['G','C']\n",
    "#     num_pca = [CFG.pca_comp_genes, CFG.pca_comp_cells]\n",
    "#     for niter, cols in enumerate([GENES, CELLS]):\n",
    "#         # PCA for train with folds\n",
    "#         train_pca = []\n",
    "#         train_pca_sig_id = []\n",
    "#         num_comp = num_pca[niter]\n",
    "#         columns_pca = [f'pca_{etiq[niter]}-{i}' for i in range(num_comp)]\n",
    "#         other_feats += columns_pca\n",
    "#         for fold in tqdm(range(CFG.num_folds)):\n",
    "#             pca = PCA(n_components=num_comp, random_state=42).fit(folds.loc[folds['kfold']!=fold, cols])\n",
    "#             train_pca.append(pca.transform(folds.loc[folds['kfold']==fold, cols]))\n",
    "#             train_pca_sig_id.append(folds.loc[folds['kfold']==fold, 'sig_id'])\n",
    "#         train_pca = np.concatenate(train_pca)\n",
    "#         train_pca = pd.DataFrame(train_pca, columns=columns_pca)\n",
    "#         train_pca['sig_id'] = np.concatenate(train_pca_sig_id)\n",
    "#         folds = pd.merge(folds, train_pca, on='sig_id')\n",
    "\n",
    "#         # PCA for test\n",
    "#         pca = PCA(n_components=num_comp, random_state=42).fit(folds[cols])\n",
    "#         test_pca = pca.transform(test_noctl[cols])\n",
    "#         test_pca = pd.DataFrame(test_pca, columns=columns_pca)\n",
    "#         test_noctl = pd.concat((test_noctl, test_pca), axis=1)\n",
    "#         print(folds.shape, test_noctl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063011,
     "end_time": "2020-12-06T00:01:42.351544",
     "exception": false,
     "start_time": "2020-12-06T00:01:42.288533",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Select Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.961Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:42.479900Z",
     "iopub.status.busy": "2020-12-06T00:01:42.478110Z",
     "iopub.status.idle": "2020-12-06T00:01:42.480516Z",
     "shell.execute_reply": "2020-12-06T00:01:42.480931Z"
    },
    "papermill": {
     "duration": 0.069142,
     "end_time": "2020-12-06T00:01:42.481050",
     "exception": false,
     "start_time": "2020-12-06T00:01:42.411908",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.utils import check_random_state  # type: ignore\n",
    "\n",
    "# ### from eli5\n",
    "# def iter_shuffled(X, columns_to_shuffle=None, pre_shuffle=False, random_state=None):\n",
    "#     rng = check_random_state(random_state)\n",
    "\n",
    "#     if columns_to_shuffle is None:\n",
    "#         columns_to_shuffle = range(X.shape[1])\n",
    "\n",
    "#     if pre_shuffle:\n",
    "#         X_shuffled = X.copy()\n",
    "#         rng.shuffle(X_shuffled)\n",
    "\n",
    "#     X_res = X.copy()\n",
    "#     for columns in tqdm(columns_to_shuffle):\n",
    "#         if pre_shuffle:\n",
    "#             X_res[:, columns] = X_shuffled[:, columns]\n",
    "#         else:\n",
    "#             rng.shuffle(X_res[:, columns])\n",
    "#         yield X_res\n",
    "#         X_res[:, columns] = X[:, columns]\n",
    "\n",
    "# def get_score_importances(\n",
    "#         score_func,  # type: Callable[[Any, Any], float]\n",
    "#         X,\n",
    "#         y,\n",
    "#         n_iter=5,  # type: int\n",
    "#         columns_to_shuffle=None,\n",
    "#         random_state=None\n",
    "#     ):\n",
    "#     rng = check_random_state(random_state)\n",
    "#     base_score = score_func(X, y)\n",
    "#     scores_decreases = []\n",
    "#     for i in range(n_iter):\n",
    "#         scores_shuffled = _get_scores_shufled(\n",
    "#             score_func, X, y, columns_to_shuffle=columns_to_shuffle,\n",
    "#             random_state=rng, base_score=base_score\n",
    "#         )\n",
    "#         scores_decreases.append(scores_shuffled)\n",
    "\n",
    "#     return base_score, scores_decreases\n",
    "\n",
    "# def _get_scores_shufled(score_func, X, y, base_score, columns_to_shuffle=None,\n",
    "#                         random_state=None):\n",
    "#     Xs = iter_shuffled(X, columns_to_shuffle, random_state=random_state)\n",
    "#     res = []\n",
    "#     for X_shuffled in Xs:\n",
    "#         res.append(-score_func(X_shuffled, y) + base_score)\n",
    "#     return res\n",
    "\n",
    "# def metric(y_true, y_pred):\n",
    "#     metrics = []\n",
    "#     for i in range(y_pred.shape[1]):\n",
    "#         if y_true[:, i].sum() > 1:\n",
    "#             metrics.append(log_loss(y_true[:, i], y_pred[:, i].astype(float)))\n",
    "#     return np.mean(metrics)   \n",
    "\n",
    "# perm_imp = np.zeros(train.shape[1])\n",
    "# all_res = []\n",
    "# for n, (tr, te) in enumerate(KFold(n_splits=7, random_state=0, shuffle=True).split(train_targets)):\n",
    "#     print(f'Fold {n}')\n",
    "\n",
    "#     model = create_model(len(train.columns))\n",
    "#     checkpoint_path = f'repeat:{seed}_Fold:{n}.hdf5'\n",
    "#     reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, epsilon=1e-4, mode='min')\n",
    "#     cb_checkpt = ModelCheckpoint(checkpoint_path, monitor = 'val_loss', verbose = 0, save_best_only = True,\n",
    "#                                      save_weights_only = True, mode = 'min')\n",
    "#     model.fit(train.values[tr],\n",
    "#                   train_targets.values[tr],\n",
    "#                   validation_data=(train.values[te], train_targets.values[te]),\n",
    "#                   epochs=35, batch_size=128,\n",
    "#                   callbacks=[reduce_lr_loss, cb_checkpt], verbose=2\n",
    "#                  )\n",
    "        \n",
    "#     model.load_weights(checkpoint_path)\n",
    "        \n",
    "#     def _score(X, y):\n",
    "#         pred = model.predict(X)\n",
    "#         return metric(y, pred)\n",
    "\n",
    "#     base_score, local_imp = get_score_importances(_score, train.values[te], train_targets.values[te], n_iter=1, random_state=0)\n",
    "#     all_res.append(local_imp)\n",
    "#     perm_imp += np.mean(local_imp, axis=0)\n",
    "#     print('')\n",
    "    \n",
    "# top_feats = np.argwhere(perm_imp < 0).flatten()\n",
    "# top_feats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.965Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:42.635724Z",
     "iopub.status.busy": "2020-12-06T00:01:42.615074Z",
     "iopub.status.idle": "2020-12-06T00:01:42.866314Z",
     "shell.execute_reply": "2020-12-06T00:01:42.867019Z"
    },
    "papermill": {
     "duration": 0.325351,
     "end_time": "2020-12-06T00:01:42.867235",
     "exception": false,
     "start_time": "2020-12-06T00:01:42.541884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1025) (3624, 819)\n"
     ]
    }
   ],
   "source": [
    "if CFG.selec_top:\n",
    "    seed_everything(seed=42)\n",
    "    # https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2/data\n",
    "    top_feats = [  1,   2,   3,   4,   5,   6,   7,   9,  11,  14,  15,  16,  17,\n",
    "            18,  19,  20,  21,  22,  23,  24,  25,  26,  27,  29,  30,  31,\n",
    "            32,  33,  35,  36,  37,  38,  39,  40,  41,  42,  43,  44,  46,\n",
    "            47,  48,  49,  50,  51,  52,  53,  54,  55,  56,  58,  59,  60,\n",
    "            61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,  73,\n",
    "            74,  75,  76,  78,  79,  80,  81,  82,  83,  84,  86,  87,  88,\n",
    "            89,  90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101,\n",
    "           102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114,\n",
    "           115, 116, 117, 118, 120, 121, 122, 123, 124, 125, 126, 127, 128,\n",
    "           129, 130, 131, 132, 133, 136, 137, 138, 139, 140, 141, 142, 143,\n",
    "           144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157,\n",
    "           158, 159, 160, 161, 162, 163, 164, 165, 166, 167, 168, 169, 170,\n",
    "           171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183,\n",
    "           184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 197,\n",
    "           198, 199, 200, 202, 203, 204, 205, 206, 208, 209, 210, 211, 212,\n",
    "           213, 214, 215, 216, 217, 218, 219, 220, 221, 223, 224, 225, 226,\n",
    "           227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237, 238, 239,\n",
    "           240, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251, 252, 253,\n",
    "           254, 255, 256, 257, 258, 259, 260, 261, 262, 263, 264, 265, 266,\n",
    "           267, 268, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280,\n",
    "           281, 282, 283, 284, 285, 286, 287, 288, 289, 290, 291, 292, 294,\n",
    "           295, 296, 298, 300, 301, 302, 303, 304, 305, 306, 307, 308, 309,\n",
    "           310, 311, 312, 314, 315, 316, 317, 318, 319, 320, 321, 322, 323,\n",
    "           324, 325, 326, 327, 328, 329, 330, 331, 332, 333, 334, 335, 336,\n",
    "           337, 338, 339, 340, 341, 342, 343, 344, 345, 346, 347, 348, 349,\n",
    "           350, 351, 352, 353, 354, 355, 356, 357, 358, 359, 360, 361, 362,\n",
    "           363, 364, 365, 366, 367, 368, 369, 370, 371, 374, 375, 376, 377,\n",
    "           378, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 390, 391,\n",
    "           392, 393, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404,\n",
    "           405, 406, 407, 408, 409, 411, 412, 413, 414, 415, 416, 417, 418,\n",
    "           419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431,\n",
    "           432, 434, 435, 436, 437, 438, 439, 440, 442, 443, 444, 445, 446,\n",
    "           447, 448, 449, 450, 453, 454, 456, 457, 458, 459, 460, 461, 462,\n",
    "           463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475,\n",
    "           476, 477, 478, 479, 481, 482, 483, 484, 485, 486, 487, 488, 489,\n",
    "           490, 491, 492, 493, 494, 495, 496, 498, 500, 501, 502, 503, 505,\n",
    "           506, 507, 509, 510, 511, 512, 513, 514, 515, 518, 519, 520, 521,\n",
    "           522, 523, 524, 525, 526, 527, 528, 530, 531, 532, 534, 535, 536,\n",
    "           538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 551,\n",
    "           552, 554, 557, 559, 560, 561, 562, 565, 566, 567, 568, 569, 570,\n",
    "           571, 572, 573, 574, 575, 577, 578, 580, 581, 582, 583, 584, 585,\n",
    "           586, 587, 588, 589, 590, 591, 592, 593, 594, 595, 596, 597, 599,\n",
    "           600, 601, 602, 606, 607, 608, 609, 611, 612, 613, 615, 616, 617,\n",
    "           618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
    "           631, 632, 633, 634, 635, 636, 637, 638, 639, 641, 642, 643, 644,\n",
    "           645, 646, 647, 648, 649, 650, 651, 652, 654, 655, 656, 658, 659,\n",
    "           660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 671, 672,\n",
    "           673, 674, 675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685,\n",
    "           686, 687, 688, 689, 691, 692, 693, 694, 695, 696, 697, 699, 700,\n",
    "           701, 702, 704, 705, 707, 708, 709, 710, 711, 713, 714, 716, 717,\n",
    "           718, 720, 721, 723, 724, 725, 726, 727, 728, 729, 730, 731, 732,\n",
    "           733, 734, 735, 737, 738, 739, 740, 742, 743, 744, 745, 746, 747,\n",
    "           748, 749, 750, 751, 752, 753, 754, 755, 756, 757, 759, 760, 761,\n",
    "           762, 763, 764, 765, 766, 767, 768, 769, 770, 771, 772, 773, 774,\n",
    "           775, 776, 777, 779, 780, 781, 782, 783, 784, 785, 786, 787, 788,\n",
    "           789, 790, 792, 793, 794, 795, 796, 797, 798, 800, 801, 802, 803,\n",
    "           804, 805, 806, 808, 809, 811, 813, 814, 815, 816, 817, 818, 819,\n",
    "           821, 822, 823, 825, 826, 827, 828, 829, 830, 831, 832, 834, 835,\n",
    "           837, 838, 839, 840, 841, 842, 845, 846, 847, 848, 850, 851, 852,\n",
    "           854, 855, 856, 858, 859, 860, 861, 862, 864, 866, 867, 868, 869,\n",
    "           870, 871, 872, 873, 874]\n",
    "    # print(len(top_feats))\n",
    "    selected_features = np.array(['cp_type','cp_time', 'cp_dose']+GENES+CELLS)[top_feats].tolist()\n",
    "    GENES = [col for col in selected_features if 'g-' in col]\n",
    "    CELLS = [col for col in selected_features if 'c-' in col]\n",
    "    numeric_cols = other_feats + GENES + CELLS\n",
    "    folds = folds[['sig_id','cp_time','cp_dose']+numeric_cols+target_cols]\n",
    "    test_noctl = test_noctl[['sig_id','cp_time','cp_dose']+numeric_cols]\n",
    "    print(folds.shape, test_noctl.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:43.002821Z",
     "iopub.status.busy": "2020-12-06T00:01:43.001560Z",
     "iopub.status.idle": "2020-12-06T00:01:43.071546Z",
     "shell.execute_reply": "2020-12-06T00:01:43.070992Z"
    },
    "papermill": {
     "duration": 0.137221,
     "end_time": "2020-12-06T00:01:43.071663",
     "exception": false,
     "start_time": "2020-12-06T00:01:42.934442",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds_novar_enc = folds.copy()\n",
    "test_noctl_novar_enc = test_noctl.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062198,
     "end_time": "2020-12-06T00:01:43.196312",
     "exception": false,
     "start_time": "2020-12-06T00:01:43.134114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Reduce Dataset using Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.969Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:43.330408Z",
     "iopub.status.busy": "2020-12-06T00:01:43.329740Z",
     "iopub.status.idle": "2020-12-06T00:01:43.333210Z",
     "shell.execute_reply": "2020-12-06T00:01:43.333673Z"
    },
    "papermill": {
     "duration": 0.075374,
     "end_time": "2020-12-06T00:01:43.333787",
     "exception": false,
     "start_time": "2020-12-06T00:01:43.258413",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class VarianceThreshold:\n",
    "    def __init__(self, threshold):\n",
    "        self.threshold = threshold\n",
    "    def fit(self, df, cont_cols):\n",
    "        self.cont_cols = cont_cols\n",
    "        self.var = folds[cont_cols].var()\n",
    "        good_cols = self.var[self.var > self.threshold]\n",
    "        self.index = good_cols.index.to_list()\n",
    "        self.dropcols = [x for x in cont_cols if x not in self.var[self.var > self.threshold].index.to_list()]\n",
    "        self.validcols = [x for x in cont_cols if x in self.var[self.var > self.threshold].index.to_list()]\n",
    "    def transform(self, df):\n",
    "        return df.drop(self.dropcols, axis=1)\n",
    "    def fit_transform(self, df, cont_cols):\n",
    "        self.fit(df, cont_cols)\n",
    "        return self.transform(df), self.validcols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.972Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:43.465423Z",
     "iopub.status.busy": "2020-12-06T00:01:43.462987Z",
     "iopub.status.idle": "2020-12-06T00:01:44.109758Z",
     "shell.execute_reply": "2020-12-06T00:01:44.109001Z"
    },
    "papermill": {
     "duration": 0.714774,
     "end_time": "2020-12-06T00:01:44.109896",
     "exception": false,
     "start_time": "2020-12-06T00:01:43.395122",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance Threshold: 0.6\n",
      "(21948, 1025) (3624, 819)\n",
      "(21948, 1011) (3624, 805)\n"
     ]
    }
   ],
   "source": [
    "if CFG.use_var_enc:\n",
    "    cont_cols_ini = [i for i in test_noctl.columns if i not in ['sig_id', 'cp_time', 'cp_dose']]\n",
    "    print('Variance Threshold:', CFG.variance_thres)\n",
    "    print(folds.shape, test_noctl.shape)\n",
    "    VarThres = VarianceThreshold(CFG.variance_thres)\n",
    "    folds, cont_cols = VarThres.fit_transform(folds, cont_cols_ini)\n",
    "    test_noctl = VarThres.transform(test_noctl)\n",
    "    print(folds.shape, test_noctl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.062068,
     "end_time": "2020-12-06T00:01:44.235928",
     "exception": false,
     "start_time": "2020-12-06T00:01:44.173860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Include kridge features  removing with low num of ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.975Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:44.381561Z",
     "iopub.status.busy": "2020-12-06T00:01:44.375267Z",
     "iopub.status.idle": "2020-12-06T00:01:44.728026Z",
     "shell.execute_reply": "2020-12-06T00:01:44.728571Z"
    },
    "papermill": {
     "duration": 0.430529,
     "end_time": "2020-12-06T00:01:44.728722",
     "exception": false,
     "start_time": "2020-12-06T00:01:44.298193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1164) (3624, 958)\n"
     ]
    }
   ],
   "source": [
    "if CFG.use_kridge:\n",
    "    folds = pd.merge(folds, folds_kridge[kridge_cols], on='sig_id')\n",
    "    test_noctl = pd.merge(test_noctl, test_kridge[kridge_cols], on='sig_id')\n",
    "    print(folds.shape, test_noctl.shape)\n",
    "    \n",
    "    folds_novar_enc = pd.merge(folds_novar_enc, folds_kridge[kridge_cols], on='sig_id')\n",
    "    test_noctl_novar_enc =  pd.merge(test_noctl_novar_enc, test_kridge[kridge_cols], on='sig_id')\n",
    "    \n",
    "    del folds_kridge\n",
    "    del test_kridge\n",
    "    gc.collect()\n",
    "    \n",
    "# #     seed_everything(seed=42)\n",
    "# #     folds_feat = pd.read_csv('../input/folds_feats.csv')\n",
    "# #     test_feat = pd.read_csv('../input/test_feat.csv')\n",
    "# #     kridge_cols = ['sig_id']+[col for col in folds_feat if 'kridge' in col]\n",
    "# #     folds = pd.merge(folds, folds_feat[kridge_cols], on='sig_id')\n",
    "# #     test_noctl = pd.merge(test_noctl, test_feat[kridge_cols], on='sig_id')\n",
    "# #     print(folds.shape, test_noctl.shape)\n",
    "    \n",
    "#     min_num_ones = 20\n",
    "#     folds_feat = pd.read_csv('../results/_29_Create_KRIDGE_feats_16oct_e30/folds.csv')\n",
    "#     test_feat = pd.read_csv('../results/_29_Create_KRIDGE_feats_16oct_e30/test.csv')\n",
    "    \n",
    "#     colsk = [col.replace('kridge_','') for col in folds_feat if 'kridge' in col]\n",
    "#     colsk_selec = []\n",
    "#     target_selec = []\n",
    "#     for colk in colsk:\n",
    "#         if np.sum(folds[colk].sum())>=min_num_ones:\n",
    "#             colsk_selec += ['kridge_'+colk]\n",
    "#             target_selec += [colk]\n",
    "#     kridge_cols = ['sig_id']+colsk_selec\n",
    "\n",
    "#     folds = pd.merge(folds, folds_feat[kridge_cols], on='sig_id')\n",
    "#     test_noctl = pd.merge(test_noctl, test_feat[kridge_cols], on='sig_id')\n",
    "#     print(len(colsk), len(colsk_selec), folds.shape, test_noctl.shape)\n",
    "    \n",
    "#     kridge_cols = [col for col in folds if 'kridge_' in col]\n",
    "# #     print('logloss=',log_loss_multi(folds[target_selec].values, folds[kridge_cols].values))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064207,
     "end_time": "2020-12-06T00:01:44.856928",
     "exception": false,
     "start_time": "2020-12-06T00:01:44.792721",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Quantile Transform of Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.979Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:45.029930Z",
     "iopub.status.busy": "2020-12-06T00:01:45.028710Z",
     "iopub.status.idle": "2020-12-06T00:01:58.859830Z",
     "shell.execute_reply": "2020-12-06T00:01:58.859315Z"
    },
    "papermill": {
     "duration": 13.939854,
     "end_time": "2020-12-06T00:01:58.859958",
     "exception": false,
     "start_time": "2020-12-06T00:01:44.920104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if CFG.quantile_transform:\n",
    "    qt = QuantileTransformer(output_distribution='normal')\n",
    "    folds[cont_cols] = qt.fit_transform(folds[cont_cols])\n",
    "    test_noctl[cont_cols] = qt.transform(test_noctl[cont_cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.063827,
     "end_time": "2020-12-06T00:01:58.988237",
     "exception": false,
     "start_time": "2020-12-06T00:01:58.924410",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 1. ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064079,
     "end_time": "2020-12-06T00:01:59.115150",
     "exception": false,
     "start_time": "2020-12-06T00:01:59.051071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.983Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:59.309382Z",
     "iopub.status.busy": "2020-12-06T00:01:59.308519Z",
     "iopub.status.idle": "2020-12-06T00:01:59.325081Z",
     "shell.execute_reply": "2020-12-06T00:01:59.325678Z"
    },
    "papermill": {
     "duration": 0.132494,
     "end_time": "2020-12-06T00:01:59.325845",
     "exception": false,
     "start_time": "2020-12-06T00:01:59.193351",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class MoADataset:\n",
    "    def __init__(self, features, targets):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float),\n",
    "            'y' : torch.tensor(self.targets[idx, :], dtype=torch.float)            \n",
    "        }\n",
    "        return dct\n",
    "    \n",
    "class TestDataset:\n",
    "    def __init__(self, features):\n",
    "        self.features = features\n",
    "        \n",
    "    def __len__(self):\n",
    "        return (self.features.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        dct = {\n",
    "            'x' : torch.tensor(self.features[idx, :], dtype=torch.float)\n",
    "        }\n",
    "        return dct\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.985Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:59.512038Z",
     "iopub.status.busy": "2020-12-06T00:01:59.511198Z",
     "iopub.status.idle": "2020-12-06T00:01:59.521193Z",
     "shell.execute_reply": "2020-12-06T00:01:59.521985Z"
    },
    "papermill": {
     "duration": 0.106342,
     "end_time": "2020-12-06T00:01:59.522164",
     "exception": false,
     "start_time": "2020-12-06T00:01:59.415822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.988Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:01:59.716456Z",
     "iopub.status.busy": "2020-12-06T00:01:59.715648Z",
     "iopub.status.idle": "2020-12-06T00:01:59.734567Z",
     "shell.execute_reply": "2020-12-06T00:01:59.735605Z"
    },
    "papermill": {
     "duration": 0.124759,
     "end_time": "2020-12-06T00:01:59.735760",
     "exception": false,
     "start_time": "2020-12-06T00:01:59.611001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_fn(model, optimizer, scheduler, loss_fn, dataloader, device):\n",
    "    model.train()\n",
    "    final_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "#         print(inputs.shape)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if not scheduler.__class__ ==  torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "            scheduler.step()\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        y_true.append(targets.cpu().detach().numpy())\n",
    "        y_pred.append(outputs.sigmoid().cpu().detach().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    control_loss = log_loss_multi(y_true, y_pred)\n",
    "    final_loss /= len(dataloader)\n",
    "    return final_loss, control_loss\n",
    "\n",
    "def valid_fn(model, loss_fn, dataloader, device):\n",
    "    model.eval()\n",
    "    final_loss = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data in dataloader:\n",
    "        inputs, targets = data['x'].to(device), data['y'].to(device)\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        final_loss += loss.item()\n",
    "        y_true.append(targets.cpu().detach().numpy())\n",
    "        y_pred.append(outputs.sigmoid().cpu().detach().numpy())\n",
    "        \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    control_loss = log_loss_multi(y_true, y_pred)\n",
    "    final_loss /= len(dataloader)\n",
    "    return final_loss, control_loss, y_true, y_pred\n",
    "\n",
    "def inference_fn(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    for data in dataloader:\n",
    "        inputs = data['x'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs)\n",
    "        y_pred.append(outputs.sigmoid().detach().cpu().numpy())\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.092116,
     "end_time": "2020-12-06T00:01:59.920673",
     "exception": false,
     "start_time": "2020-12-06T00:01:59.828557",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.991Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:00.119461Z",
     "iopub.status.busy": "2020-12-06T00:02:00.118633Z",
     "iopub.status.idle": "2020-12-06T00:02:00.127428Z",
     "shell.execute_reply": "2020-12-06T00:02:00.128508Z"
    },
    "papermill": {
     "duration": 0.114152,
     "end_time": "2020-12-06T00:02:00.128655",
     "exception": false,
     "start_time": "2020-12-06T00:02:00.014503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets, hidden_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.batch_norm1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1 = nn.Dropout(drop1_feat) #0.20\n",
    "        self.dense1 = nn.utils.weight_norm(nn.Linear(num_features, hidden_size))\n",
    "        \n",
    "        self.batch_norm2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(drop2_feat) #0.20\n",
    "        self.dense2 = nn.utils.weight_norm(nn.Linear(hidden_size, hidden_size))\n",
    "        \n",
    "        self.batch_norm3 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout3 = nn.Dropout(drop3_feat) #0.25\n",
    "        self.dense3 = nn.utils.weight_norm(nn.Linear(hidden_size, num_targets))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.dropout1(x)\n",
    "#         x = F.relu(self.dense1(x))\n",
    "        x = F.leaky_relu(self.dense1(x))\n",
    "        \n",
    "        x = self.batch_norm2(x)\n",
    "        x = self.dropout2(x)\n",
    "#         x = F.relu(self.dense2(x))\n",
    "        x = F.leaky_relu(self.dense2(x))\n",
    "        \n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.086546,
     "end_time": "2020-12-06T00:02:00.309260",
     "exception": false,
     "start_time": "2020-12-06T00:02:00.222714",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Add Kmeans Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:00.450035Z",
     "iopub.status.busy": "2020-12-06T00:02:00.448739Z",
     "iopub.status.idle": "2020-12-06T00:02:00.600899Z",
     "shell.execute_reply": "2020-12-06T00:02:00.601400Z"
    },
    "papermill": {
     "duration": 0.2255,
     "end_time": "2020-12-06T00:02:00.601544",
     "exception": false,
     "start_time": "2020-12-06T00:02:00.376044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21948, 1178) (3624, 972)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = pd.concat((folds, folds_3models), axis=1)\n",
    "test_noctl = pd.concat((test_noctl, test_noctl_3model), axis=1)\n",
    "print(folds.shape, test_noctl.shape)\n",
    "del folds_3models, test_noctl_3model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.064927,
     "end_time": "2020-12-06T00:02:00.732665",
     "exception": false,
     "start_time": "2020-12-06T00:02:00.667738",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Preprocessing Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.993Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:00.869541Z",
     "iopub.status.busy": "2020-12-06T00:02:00.868721Z",
     "iopub.status.idle": "2020-12-06T00:02:00.871592Z",
     "shell.execute_reply": "2020-12-06T00:02:00.871096Z"
    },
    "papermill": {
     "duration": 0.074112,
     "end_time": "2020-12-06T00:02:00.871689",
     "exception": false,
     "start_time": "2020-12-06T00:02:00.797577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.995Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:01.009720Z",
     "iopub.status.busy": "2020-12-06T00:02:01.008490Z",
     "iopub.status.idle": "2020-12-06T00:02:01.316961Z",
     "shell.execute_reply": "2020-12-06T00:02:01.318782Z"
    },
    "papermill": {
     "duration": 0.382449,
     "end_time": "2020-12-06T00:02:01.318964",
     "exception": false,
     "start_time": "2020-12-06T00:02:00.936515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "folds = process_data(folds)\n",
    "test_noctl = process_data(test_noctl)\n",
    "\n",
    "feature_cols = [c for c in folds.columns if c not in target_cols]\n",
    "if not CFG.original_feats: feature_cols = [c for c in feature_cols if c not in GENES+CELLS]\n",
    "feature_cols_ini = [c for c in feature_cols if c not in ['kfold','sig_id']]#, 'cp_time_24', 'cp_time_48', 'cp_time_72', 'cp_dose_D1', 'cp_dose_D2','drug_id']] #,'cp_dose','cp_time']]\n",
    "len(feature_cols_ini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:01.458157Z",
     "iopub.status.busy": "2020-12-06T00:02:01.457355Z",
     "iopub.status.idle": "2020-12-06T00:02:01.461184Z",
     "shell.execute_reply": "2020-12-06T00:02:01.460724Z"
    },
    "papermill": {
     "duration": 0.075653,
     "end_time": "2020-12-06T00:02:01.461296",
     "exception": false,
     "start_time": "2020-12-06T00:02:01.385643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "955"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols_ini_nocps = [c for c in feature_cols if c not in ['kfold','sig_id', 'cp_time_24', 'cp_time_48', 'cp_time_72', 'cp_dose_D1', 'cp_dose_D2','drug_id']]\n",
    "feature_cols_ini_nocps = [c for c in feature_cols_ini_nocps if 'kmeans_' not in c]\n",
    "len(feature_cols_ini_nocps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:01.618459Z",
     "iopub.status.busy": "2020-12-06T00:02:01.608395Z",
     "iopub.status.idle": "2020-12-06T00:02:01.622559Z",
     "shell.execute_reply": "2020-12-06T00:02:01.623186Z"
    },
    "papermill": {
     "duration": 0.095853,
     "end_time": "2020-12-06T00:02:01.623309",
     "exception": false,
     "start_time": "2020-12-06T00:02:01.527456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pca_G-0',\n",
       " 'pca_G-1',\n",
       " 'pca_G-2',\n",
       " 'pca_G-3',\n",
       " 'pca_G-4',\n",
       " 'pca_G-5',\n",
       " 'pca_G-6',\n",
       " 'pca_G-7',\n",
       " 'pca_G-8',\n",
       " 'pca_G-9',\n",
       " 'pca_G-10',\n",
       " 'pca_G-11',\n",
       " 'pca_G-12',\n",
       " 'pca_G-13',\n",
       " 'pca_G-14',\n",
       " 'pca_G-15',\n",
       " 'pca_G-16',\n",
       " 'pca_G-17',\n",
       " 'pca_G-18',\n",
       " 'pca_G-19',\n",
       " 'pca_G-20',\n",
       " 'pca_G-21',\n",
       " 'pca_G-22',\n",
       " 'pca_G-23',\n",
       " 'pca_G-24',\n",
       " 'pca_G-25',\n",
       " 'pca_G-26',\n",
       " 'pca_G-27',\n",
       " 'pca_G-28',\n",
       " 'pca_C-0',\n",
       " 'pca_C-1',\n",
       " 'pca_C-2',\n",
       " 'pca_C-3',\n",
       " 'g-0',\n",
       " 'g-1',\n",
       " 'g-2',\n",
       " 'g-3',\n",
       " 'g-4',\n",
       " 'g-6',\n",
       " 'g-8',\n",
       " 'g-11',\n",
       " 'g-12',\n",
       " 'g-13',\n",
       " 'g-14',\n",
       " 'g-16',\n",
       " 'g-17',\n",
       " 'g-19',\n",
       " 'g-20',\n",
       " 'g-21',\n",
       " 'g-22',\n",
       " 'g-24',\n",
       " 'g-26',\n",
       " 'g-27',\n",
       " 'g-28',\n",
       " 'g-29',\n",
       " 'g-30',\n",
       " 'g-32',\n",
       " 'g-33',\n",
       " 'g-34',\n",
       " 'g-35',\n",
       " 'g-36',\n",
       " 'g-37',\n",
       " 'g-38',\n",
       " 'g-39',\n",
       " 'g-40',\n",
       " 'g-41',\n",
       " 'g-43',\n",
       " 'g-44',\n",
       " 'g-45',\n",
       " 'g-46',\n",
       " 'g-47',\n",
       " 'g-48',\n",
       " 'g-49',\n",
       " 'g-50',\n",
       " 'g-51',\n",
       " 'g-52',\n",
       " 'g-53',\n",
       " 'g-55',\n",
       " 'g-56',\n",
       " 'g-57',\n",
       " 'g-58',\n",
       " 'g-59',\n",
       " 'g-60',\n",
       " 'g-61',\n",
       " 'g-62',\n",
       " 'g-63',\n",
       " 'g-64',\n",
       " 'g-65',\n",
       " 'g-66',\n",
       " 'g-67',\n",
       " 'g-68',\n",
       " 'g-69',\n",
       " 'g-70',\n",
       " 'g-71',\n",
       " 'g-72',\n",
       " 'g-73',\n",
       " 'g-75',\n",
       " 'g-76',\n",
       " 'g-77',\n",
       " 'g-78',\n",
       " 'g-79',\n",
       " 'g-80',\n",
       " 'g-81',\n",
       " 'g-83',\n",
       " 'g-84',\n",
       " 'g-85',\n",
       " 'g-86',\n",
       " 'g-87',\n",
       " 'g-88',\n",
       " 'g-89',\n",
       " 'g-90',\n",
       " 'g-91',\n",
       " 'g-92',\n",
       " 'g-93',\n",
       " 'g-94',\n",
       " 'g-95',\n",
       " 'g-96',\n",
       " 'g-97',\n",
       " 'g-98',\n",
       " 'g-99',\n",
       " 'g-100',\n",
       " 'g-101',\n",
       " 'g-102',\n",
       " 'g-103',\n",
       " 'g-105',\n",
       " 'g-106',\n",
       " 'g-107',\n",
       " 'g-108',\n",
       " 'g-109',\n",
       " 'g-110',\n",
       " 'g-111',\n",
       " 'g-112',\n",
       " 'g-113',\n",
       " 'g-114',\n",
       " 'g-115',\n",
       " 'g-117',\n",
       " 'g-118',\n",
       " 'g-119',\n",
       " 'g-120',\n",
       " 'g-121',\n",
       " 'g-122',\n",
       " 'g-123',\n",
       " 'g-124',\n",
       " 'g-125',\n",
       " 'g-126',\n",
       " 'g-127',\n",
       " 'g-128',\n",
       " 'g-129',\n",
       " 'g-130',\n",
       " 'g-133',\n",
       " 'g-134',\n",
       " 'g-135',\n",
       " 'g-136',\n",
       " 'g-137',\n",
       " 'g-138',\n",
       " 'g-139',\n",
       " 'g-140',\n",
       " 'g-141',\n",
       " 'g-142',\n",
       " 'g-143',\n",
       " 'g-144',\n",
       " 'g-146',\n",
       " 'g-147',\n",
       " 'g-148',\n",
       " 'g-149',\n",
       " 'g-150',\n",
       " 'g-151',\n",
       " 'g-152',\n",
       " 'g-154',\n",
       " 'g-155',\n",
       " 'g-156',\n",
       " 'g-157',\n",
       " 'g-158',\n",
       " 'g-159',\n",
       " 'g-160',\n",
       " 'g-161',\n",
       " 'g-162',\n",
       " 'g-163',\n",
       " 'g-164',\n",
       " 'g-165',\n",
       " 'g-166',\n",
       " 'g-167',\n",
       " 'g-168',\n",
       " 'g-169',\n",
       " 'g-170',\n",
       " 'g-171',\n",
       " 'g-172',\n",
       " 'g-173',\n",
       " 'g-174',\n",
       " 'g-175',\n",
       " 'g-176',\n",
       " 'g-177',\n",
       " 'g-178',\n",
       " 'g-179',\n",
       " 'g-180',\n",
       " 'g-181',\n",
       " 'g-182',\n",
       " 'g-183',\n",
       " 'g-185',\n",
       " 'g-186',\n",
       " 'g-187',\n",
       " 'g-188',\n",
       " 'g-189',\n",
       " 'g-190',\n",
       " 'g-191',\n",
       " 'g-192',\n",
       " 'g-194',\n",
       " 'g-195',\n",
       " 'g-196',\n",
       " 'g-197',\n",
       " 'g-199',\n",
       " 'g-200',\n",
       " 'g-201',\n",
       " 'g-202',\n",
       " 'g-203',\n",
       " 'g-205',\n",
       " 'g-206',\n",
       " 'g-207',\n",
       " 'g-208',\n",
       " 'g-209',\n",
       " 'g-210',\n",
       " 'g-211',\n",
       " 'g-212',\n",
       " 'g-213',\n",
       " 'g-214',\n",
       " 'g-215',\n",
       " 'g-217',\n",
       " 'g-218',\n",
       " 'g-220',\n",
       " 'g-221',\n",
       " 'g-222',\n",
       " 'g-223',\n",
       " 'g-224',\n",
       " 'g-225',\n",
       " 'g-226',\n",
       " 'g-227',\n",
       " 'g-228',\n",
       " 'g-229',\n",
       " 'g-230',\n",
       " 'g-231',\n",
       " 'g-232',\n",
       " 'g-233',\n",
       " 'g-234',\n",
       " 'g-235',\n",
       " 'g-236',\n",
       " 'g-237',\n",
       " 'g-239',\n",
       " 'g-240',\n",
       " 'g-241',\n",
       " 'g-242',\n",
       " 'g-243',\n",
       " 'g-244',\n",
       " 'g-245',\n",
       " 'g-246',\n",
       " 'g-247',\n",
       " 'g-248',\n",
       " 'g-249',\n",
       " 'g-250',\n",
       " 'g-251',\n",
       " 'g-252',\n",
       " 'g-253',\n",
       " 'g-254',\n",
       " 'g-255',\n",
       " 'g-256',\n",
       " 'g-257',\n",
       " 'g-258',\n",
       " 'g-259',\n",
       " 'g-260',\n",
       " 'g-261',\n",
       " 'g-262',\n",
       " 'g-263',\n",
       " 'g-264',\n",
       " 'g-265',\n",
       " 'g-266',\n",
       " 'g-268',\n",
       " 'g-269',\n",
       " 'g-270',\n",
       " 'g-271',\n",
       " 'g-272',\n",
       " 'g-273',\n",
       " 'g-274',\n",
       " 'g-275',\n",
       " 'g-276',\n",
       " 'g-277',\n",
       " 'g-278',\n",
       " 'g-279',\n",
       " 'g-280',\n",
       " 'g-281',\n",
       " 'g-282',\n",
       " 'g-283',\n",
       " 'g-284',\n",
       " 'g-285',\n",
       " 'g-286',\n",
       " 'g-287',\n",
       " 'g-288',\n",
       " 'g-289',\n",
       " 'g-291',\n",
       " 'g-292',\n",
       " 'g-293',\n",
       " 'g-295',\n",
       " 'g-297',\n",
       " 'g-298',\n",
       " 'g-299',\n",
       " 'g-300',\n",
       " 'g-301',\n",
       " 'g-302',\n",
       " 'g-303',\n",
       " 'g-304',\n",
       " 'g-305',\n",
       " 'g-306',\n",
       " 'g-308',\n",
       " 'g-309',\n",
       " 'g-311',\n",
       " 'g-312',\n",
       " 'g-313',\n",
       " 'g-314',\n",
       " 'g-315',\n",
       " 'g-316',\n",
       " 'g-317',\n",
       " 'g-318',\n",
       " 'g-319',\n",
       " 'g-320',\n",
       " 'g-321',\n",
       " 'g-322',\n",
       " 'g-323',\n",
       " 'g-324',\n",
       " 'g-325',\n",
       " 'g-326',\n",
       " 'g-327',\n",
       " 'g-328',\n",
       " 'g-329',\n",
       " 'g-330',\n",
       " 'g-332',\n",
       " 'g-333',\n",
       " 'g-334',\n",
       " 'g-335',\n",
       " 'g-336',\n",
       " 'g-337',\n",
       " 'g-338',\n",
       " 'g-339',\n",
       " 'g-340',\n",
       " 'g-341',\n",
       " 'g-342',\n",
       " 'g-343',\n",
       " 'g-344',\n",
       " 'g-345',\n",
       " 'g-346',\n",
       " 'g-347',\n",
       " 'g-348',\n",
       " 'g-349',\n",
       " 'g-350',\n",
       " 'g-351',\n",
       " 'g-352',\n",
       " 'g-353',\n",
       " 'g-354',\n",
       " 'g-355',\n",
       " 'g-356',\n",
       " 'g-357',\n",
       " 'g-358',\n",
       " 'g-359',\n",
       " 'g-360',\n",
       " 'g-361',\n",
       " 'g-362',\n",
       " 'g-363',\n",
       " 'g-364',\n",
       " 'g-365',\n",
       " 'g-366',\n",
       " 'g-367',\n",
       " 'g-368',\n",
       " 'g-371',\n",
       " 'g-372',\n",
       " 'g-373',\n",
       " 'g-374',\n",
       " 'g-375',\n",
       " 'g-376',\n",
       " 'g-377',\n",
       " 'g-378',\n",
       " 'g-379',\n",
       " 'g-380',\n",
       " 'g-381',\n",
       " 'g-382',\n",
       " 'g-383',\n",
       " 'g-384',\n",
       " 'g-385',\n",
       " 'g-387',\n",
       " 'g-388',\n",
       " 'g-389',\n",
       " 'g-390',\n",
       " 'g-391',\n",
       " 'g-392',\n",
       " 'g-393',\n",
       " 'g-394',\n",
       " 'g-395',\n",
       " 'g-396',\n",
       " 'g-397',\n",
       " 'g-398',\n",
       " 'g-399',\n",
       " 'g-400',\n",
       " 'g-401',\n",
       " 'g-402',\n",
       " 'g-403',\n",
       " 'g-404',\n",
       " 'g-405',\n",
       " 'g-406',\n",
       " 'g-408',\n",
       " 'g-409',\n",
       " 'g-410',\n",
       " 'g-411',\n",
       " 'g-412',\n",
       " 'g-413',\n",
       " 'g-414',\n",
       " 'g-415',\n",
       " 'g-416',\n",
       " 'g-417',\n",
       " 'g-418',\n",
       " 'g-419',\n",
       " 'g-420',\n",
       " 'g-421',\n",
       " 'g-422',\n",
       " 'g-423',\n",
       " 'g-424',\n",
       " 'g-425',\n",
       " 'g-426',\n",
       " 'g-427',\n",
       " 'g-428',\n",
       " 'g-429',\n",
       " 'g-431',\n",
       " 'g-432',\n",
       " 'g-433',\n",
       " 'g-434',\n",
       " 'g-436',\n",
       " 'g-437',\n",
       " 'g-439',\n",
       " 'g-440',\n",
       " 'g-441',\n",
       " 'g-442',\n",
       " 'g-443',\n",
       " 'g-444',\n",
       " 'g-445',\n",
       " 'g-446',\n",
       " 'g-447',\n",
       " 'g-450',\n",
       " 'g-451',\n",
       " 'g-453',\n",
       " 'g-454',\n",
       " 'g-455',\n",
       " 'g-456',\n",
       " 'g-457',\n",
       " 'g-458',\n",
       " 'g-459',\n",
       " 'g-460',\n",
       " 'g-461',\n",
       " 'g-462',\n",
       " 'g-463',\n",
       " 'g-464',\n",
       " 'g-465',\n",
       " 'g-466',\n",
       " 'g-467',\n",
       " 'g-468',\n",
       " 'g-469',\n",
       " 'g-470',\n",
       " 'g-471',\n",
       " 'g-472',\n",
       " 'g-473',\n",
       " 'g-474',\n",
       " 'g-475',\n",
       " 'g-476',\n",
       " 'g-478',\n",
       " 'g-479',\n",
       " 'g-480',\n",
       " 'g-482',\n",
       " 'g-483',\n",
       " 'g-484',\n",
       " 'g-485',\n",
       " 'g-486',\n",
       " 'g-487',\n",
       " 'g-488',\n",
       " 'g-489',\n",
       " 'g-490',\n",
       " 'g-491',\n",
       " 'g-492',\n",
       " 'g-493',\n",
       " 'g-495',\n",
       " 'g-497',\n",
       " 'g-498',\n",
       " 'g-499',\n",
       " 'g-500',\n",
       " 'g-502',\n",
       " 'g-503',\n",
       " 'g-504',\n",
       " 'g-506',\n",
       " 'g-507',\n",
       " 'g-508',\n",
       " 'g-509',\n",
       " 'g-510',\n",
       " 'g-511',\n",
       " 'g-512',\n",
       " 'g-515',\n",
       " 'g-516',\n",
       " 'g-517',\n",
       " 'g-518',\n",
       " 'g-519',\n",
       " 'g-520',\n",
       " 'g-521',\n",
       " 'g-522',\n",
       " 'g-523',\n",
       " 'g-524',\n",
       " 'g-525',\n",
       " 'g-527',\n",
       " 'g-528',\n",
       " 'g-529',\n",
       " 'g-531',\n",
       " 'g-532',\n",
       " 'g-533',\n",
       " 'g-535',\n",
       " 'g-537',\n",
       " 'g-538',\n",
       " 'g-539',\n",
       " 'g-540',\n",
       " 'g-541',\n",
       " 'g-542',\n",
       " 'g-543',\n",
       " 'g-544',\n",
       " 'g-546',\n",
       " 'g-547',\n",
       " 'g-548',\n",
       " 'g-549',\n",
       " 'g-551',\n",
       " 'g-554',\n",
       " 'g-556',\n",
       " 'g-557',\n",
       " 'g-558',\n",
       " 'g-559',\n",
       " 'g-562',\n",
       " 'g-563',\n",
       " 'g-564',\n",
       " 'g-565',\n",
       " 'g-566',\n",
       " 'g-567',\n",
       " 'g-568',\n",
       " 'g-569',\n",
       " 'g-570',\n",
       " 'g-571',\n",
       " 'g-572',\n",
       " 'g-574',\n",
       " 'g-575',\n",
       " 'g-577',\n",
       " 'g-578',\n",
       " 'g-579',\n",
       " 'g-580',\n",
       " 'g-581',\n",
       " 'g-582',\n",
       " 'g-583',\n",
       " 'g-584',\n",
       " 'g-585',\n",
       " 'g-586',\n",
       " 'g-587',\n",
       " 'g-588',\n",
       " 'g-589',\n",
       " 'g-590',\n",
       " 'g-591',\n",
       " 'g-592',\n",
       " 'g-593',\n",
       " 'g-594',\n",
       " 'g-596',\n",
       " 'g-597',\n",
       " 'g-598',\n",
       " 'g-599',\n",
       " 'g-603',\n",
       " 'g-604',\n",
       " 'g-605',\n",
       " 'g-606',\n",
       " 'g-608',\n",
       " 'g-609',\n",
       " 'g-610',\n",
       " 'g-612',\n",
       " 'g-613',\n",
       " 'g-614',\n",
       " 'g-615',\n",
       " 'g-616',\n",
       " 'g-617',\n",
       " 'g-618',\n",
       " 'g-619',\n",
       " 'g-620',\n",
       " 'g-621',\n",
       " 'g-622',\n",
       " 'g-623',\n",
       " 'g-624',\n",
       " 'g-625',\n",
       " 'g-626',\n",
       " 'g-627',\n",
       " 'g-628',\n",
       " 'g-629',\n",
       " 'g-630',\n",
       " 'g-631',\n",
       " 'g-632',\n",
       " 'g-633',\n",
       " 'g-634',\n",
       " 'g-635',\n",
       " 'g-636',\n",
       " 'g-638',\n",
       " 'g-639',\n",
       " 'g-640',\n",
       " 'g-641',\n",
       " 'g-642',\n",
       " 'g-643',\n",
       " 'g-644',\n",
       " 'g-645',\n",
       " 'g-646',\n",
       " 'g-647',\n",
       " 'g-648',\n",
       " 'g-649',\n",
       " 'g-651',\n",
       " 'g-652',\n",
       " 'g-653',\n",
       " 'g-655',\n",
       " 'g-656',\n",
       " 'g-657',\n",
       " 'g-658',\n",
       " 'g-659',\n",
       " 'g-660',\n",
       " 'g-661',\n",
       " 'g-662',\n",
       " 'g-663',\n",
       " 'g-664',\n",
       " 'g-665',\n",
       " 'g-666',\n",
       " 'g-667',\n",
       " 'g-668',\n",
       " 'g-669',\n",
       " 'g-670',\n",
       " 'g-671',\n",
       " 'g-672',\n",
       " 'g-673',\n",
       " 'g-674',\n",
       " 'g-675',\n",
       " 'g-676',\n",
       " 'g-677',\n",
       " 'g-678',\n",
       " 'g-679',\n",
       " 'g-680',\n",
       " 'g-681',\n",
       " 'g-682',\n",
       " 'g-683',\n",
       " 'g-684',\n",
       " 'g-685',\n",
       " 'g-686',\n",
       " 'g-688',\n",
       " 'g-689',\n",
       " 'g-690',\n",
       " 'g-691',\n",
       " 'g-692',\n",
       " 'g-693',\n",
       " 'g-694',\n",
       " 'g-696',\n",
       " 'g-697',\n",
       " 'g-698',\n",
       " 'g-699',\n",
       " 'g-701',\n",
       " 'g-702',\n",
       " 'g-704',\n",
       " 'g-705',\n",
       " 'g-706',\n",
       " 'g-707',\n",
       " 'g-708',\n",
       " 'g-710',\n",
       " 'g-711',\n",
       " 'g-713',\n",
       " 'g-714',\n",
       " 'g-715',\n",
       " 'g-717',\n",
       " 'g-720',\n",
       " 'g-721',\n",
       " 'g-722',\n",
       " 'g-723',\n",
       " 'g-724',\n",
       " 'g-725',\n",
       " 'g-726',\n",
       " 'g-727',\n",
       " 'g-728',\n",
       " 'g-729',\n",
       " 'g-730',\n",
       " 'g-731',\n",
       " 'g-732',\n",
       " 'g-734',\n",
       " 'g-735',\n",
       " 'g-736',\n",
       " 'g-737',\n",
       " 'g-739',\n",
       " 'g-740',\n",
       " 'g-741',\n",
       " 'g-742',\n",
       " 'g-743',\n",
       " 'g-744',\n",
       " 'g-745',\n",
       " 'g-746',\n",
       " 'g-747',\n",
       " 'g-748',\n",
       " 'g-749',\n",
       " 'g-750',\n",
       " 'g-751',\n",
       " 'g-752',\n",
       " 'g-753',\n",
       " 'g-754',\n",
       " 'g-757',\n",
       " 'g-758',\n",
       " 'g-759',\n",
       " 'g-760',\n",
       " 'g-761',\n",
       " 'g-762',\n",
       " 'g-763',\n",
       " 'g-764',\n",
       " 'g-765',\n",
       " 'g-766',\n",
       " 'g-767',\n",
       " 'g-768',\n",
       " 'g-769',\n",
       " 'g-770',\n",
       " 'g-771',\n",
       " 'c-0',\n",
       " 'c-1',\n",
       " 'c-2',\n",
       " 'c-4',\n",
       " 'c-5',\n",
       " 'c-6',\n",
       " 'c-7',\n",
       " 'c-8',\n",
       " 'c-9',\n",
       " 'c-10',\n",
       " 'c-11',\n",
       " 'c-12',\n",
       " 'c-13',\n",
       " 'c-14',\n",
       " 'c-15',\n",
       " 'c-17',\n",
       " 'c-18',\n",
       " 'c-19',\n",
       " 'c-20',\n",
       " 'c-21',\n",
       " 'c-22',\n",
       " 'c-23',\n",
       " 'c-25',\n",
       " 'c-26',\n",
       " 'c-27',\n",
       " 'c-28',\n",
       " 'c-29',\n",
       " 'c-30',\n",
       " 'c-31',\n",
       " 'c-33',\n",
       " 'c-34',\n",
       " 'c-36',\n",
       " 'c-38',\n",
       " 'c-39',\n",
       " 'c-40',\n",
       " 'c-41',\n",
       " 'c-42',\n",
       " 'c-43',\n",
       " 'c-44',\n",
       " 'c-46',\n",
       " 'c-47',\n",
       " 'c-48',\n",
       " 'c-50',\n",
       " 'c-51',\n",
       " 'c-52',\n",
       " 'c-53',\n",
       " 'c-54',\n",
       " 'c-55',\n",
       " 'c-56',\n",
       " 'c-57',\n",
       " 'c-59',\n",
       " 'c-60',\n",
       " 'c-62',\n",
       " 'c-63',\n",
       " 'c-64',\n",
       " 'c-65',\n",
       " 'c-66',\n",
       " 'c-67',\n",
       " 'c-70',\n",
       " 'c-71',\n",
       " 'c-72',\n",
       " 'c-73',\n",
       " 'c-75',\n",
       " 'c-76',\n",
       " 'c-77',\n",
       " 'c-79',\n",
       " 'c-80',\n",
       " 'c-81',\n",
       " 'c-83',\n",
       " 'c-84',\n",
       " 'c-85',\n",
       " 'c-86',\n",
       " 'c-87',\n",
       " 'c-89',\n",
       " 'c-91',\n",
       " 'c-92',\n",
       " 'c-93',\n",
       " 'c-94',\n",
       " 'c-95',\n",
       " 'c-96',\n",
       " 'c-97',\n",
       " 'c-98',\n",
       " 'c-99',\n",
       " 'kridge_acat_inhibitor',\n",
       " 'kridge_acetylcholine_receptor_agonist',\n",
       " 'kridge_acetylcholine_receptor_antagonist',\n",
       " 'kridge_acetylcholinesterase_inhibitor',\n",
       " 'kridge_adenosine_receptor_agonist',\n",
       " 'kridge_adenosine_receptor_antagonist',\n",
       " 'kridge_adrenergic_receptor_agonist',\n",
       " 'kridge_adrenergic_receptor_antagonist',\n",
       " 'kridge_akt_inhibitor',\n",
       " 'kridge_alk_inhibitor',\n",
       " 'kridge_androgen_receptor_agonist',\n",
       " 'kridge_androgen_receptor_antagonist',\n",
       " 'kridge_anesthetic_-_local',\n",
       " 'kridge_angiogenesis_inhibitor',\n",
       " 'kridge_angiotensin_receptor_antagonist',\n",
       " 'kridge_anti-inflammatory',\n",
       " 'kridge_antibiotic',\n",
       " 'kridge_antioxidant',\n",
       " 'kridge_antiprotozoal',\n",
       " 'kridge_antiviral',\n",
       " 'kridge_apoptosis_stimulant',\n",
       " 'kridge_aromatase_inhibitor',\n",
       " 'kridge_atpase_inhibitor',\n",
       " 'kridge_aurora_kinase_inhibitor',\n",
       " 'kridge_bacterial_30s_ribosomal_subunit_inhibitor',\n",
       " 'kridge_bacterial_50s_ribosomal_subunit_inhibitor',\n",
       " 'kridge_bacterial_antifolate',\n",
       " 'kridge_bacterial_cell_wall_synthesis_inhibitor',\n",
       " 'kridge_bacterial_dna_gyrase_inhibitor',\n",
       " 'kridge_bacterial_dna_inhibitor',\n",
       " 'kridge_bcl_inhibitor',\n",
       " 'kridge_bcr-abl_inhibitor',\n",
       " 'kridge_benzodiazepine_receptor_agonist',\n",
       " 'kridge_beta_amyloid_inhibitor',\n",
       " 'kridge_bromodomain_inhibitor',\n",
       " 'kridge_btk_inhibitor',\n",
       " 'kridge_calcium_channel_blocker',\n",
       " 'kridge_cannabinoid_receptor_agonist',\n",
       " 'kridge_cannabinoid_receptor_antagonist',\n",
       " 'kridge_carbonic_anhydrase_inhibitor',\n",
       " 'kridge_casein_kinase_inhibitor',\n",
       " 'kridge_cc_chemokine_receptor_antagonist',\n",
       " 'kridge_cdk_inhibitor',\n",
       " 'kridge_chelating_agent',\n",
       " 'kridge_chk_inhibitor',\n",
       " 'kridge_chloride_channel_blocker',\n",
       " 'kridge_cholesterol_inhibitor',\n",
       " 'kridge_cholinergic_receptor_antagonist',\n",
       " 'kridge_corticosteroid_agonist',\n",
       " 'kridge_cyclooxygenase_inhibitor',\n",
       " 'kridge_cytochrome_p450_inhibitor',\n",
       " 'kridge_dihydrofolate_reductase_inhibitor',\n",
       " 'kridge_dipeptidyl_peptidase_inhibitor',\n",
       " 'kridge_dna_alkylating_agent',\n",
       " 'kridge_dna_inhibitor',\n",
       " 'kridge_dopamine_receptor_agonist',\n",
       " 'kridge_dopamine_receptor_antagonist',\n",
       " 'kridge_egfr_inhibitor',\n",
       " 'kridge_estrogen_receptor_agonist',\n",
       " 'kridge_estrogen_receptor_antagonist',\n",
       " 'kridge_faah_inhibitor',\n",
       " 'kridge_fatty_acid_receptor_agonist',\n",
       " 'kridge_fgfr_inhibitor',\n",
       " 'kridge_flt3_inhibitor',\n",
       " 'kridge_fungal_squalene_epoxidase_inhibitor',\n",
       " 'kridge_gaba_receptor_agonist',\n",
       " 'kridge_gaba_receptor_antagonist',\n",
       " 'kridge_gamma_secretase_inhibitor',\n",
       " 'kridge_glucocorticoid_receptor_agonist',\n",
       " 'kridge_glutamate_receptor_agonist',\n",
       " 'kridge_glutamate_receptor_antagonist',\n",
       " 'kridge_gsk_inhibitor',\n",
       " 'kridge_hcv_inhibitor',\n",
       " 'kridge_hdac_inhibitor',\n",
       " 'kridge_histamine_receptor_agonist',\n",
       " 'kridge_histamine_receptor_antagonist',\n",
       " 'kridge_histone_lysine_demethylase_inhibitor',\n",
       " 'kridge_histone_lysine_methyltransferase_inhibitor',\n",
       " 'kridge_hiv_inhibitor',\n",
       " 'kridge_hmgcr_inhibitor',\n",
       " 'kridge_hsp_inhibitor',\n",
       " 'kridge_igf-1_inhibitor',\n",
       " 'kridge_ikk_inhibitor',\n",
       " 'kridge_imidazoline_receptor_agonist',\n",
       " 'kridge_immunosuppressant',\n",
       " 'kridge_insulin_secretagogue',\n",
       " 'kridge_insulin_sensitizer',\n",
       " 'kridge_integrin_inhibitor',\n",
       " 'kridge_jak_inhibitor',\n",
       " 'kridge_kit_inhibitor',\n",
       " 'kridge_leukotriene_receptor_antagonist',\n",
       " 'kridge_lipoxygenase_inhibitor',\n",
       " 'kridge_mdm_inhibitor',\n",
       " 'kridge_mek_inhibitor',\n",
       " 'kridge_membrane_integrity_inhibitor',\n",
       " 'kridge_mineralocorticoid_receptor_antagonist',\n",
       " 'kridge_monoamine_oxidase_inhibitor',\n",
       " 'kridge_mtor_inhibitor',\n",
       " 'kridge_mucolytic_agent',\n",
       " 'kridge_neuropeptide_receptor_antagonist',\n",
       " 'kridge_nfkb_inhibitor',\n",
       " 'kridge_nitric_oxide_donor',\n",
       " 'kridge_nitric_oxide_synthase_inhibitor',\n",
       " 'kridge_opioid_receptor_agonist',\n",
       " 'kridge_opioid_receptor_antagonist',\n",
       " 'kridge_orexin_receptor_antagonist',\n",
       " 'kridge_p38_mapk_inhibitor',\n",
       " 'kridge_p-glycoprotein_inhibitor',\n",
       " 'kridge_parp_inhibitor',\n",
       " 'kridge_pdgfr_inhibitor',\n",
       " 'kridge_phosphodiesterase_inhibitor',\n",
       " 'kridge_phospholipase_inhibitor',\n",
       " 'kridge_pi3k_inhibitor',\n",
       " 'kridge_pkc_inhibitor',\n",
       " 'kridge_potassium_channel_activator',\n",
       " 'kridge_potassium_channel_antagonist',\n",
       " 'kridge_ppar_receptor_agonist',\n",
       " 'kridge_ppar_receptor_antagonist',\n",
       " 'kridge_progesterone_receptor_agonist',\n",
       " 'kridge_prostaglandin_inhibitor',\n",
       " 'kridge_prostanoid_receptor_antagonist',\n",
       " 'kridge_proteasome_inhibitor',\n",
       " 'kridge_protein_kinase_inhibitor',\n",
       " 'kridge_protein_synthesis_inhibitor',\n",
       " 'kridge_radiopaque_medium',\n",
       " 'kridge_raf_inhibitor',\n",
       " 'kridge_retinoid_receptor_agonist',\n",
       " 'kridge_rho_associated_kinase_inhibitor',\n",
       " 'kridge_ribonucleoside_reductase_inhibitor',\n",
       " 'kridge_rna_polymerase_inhibitor',\n",
       " 'kridge_serotonin_receptor_agonist',\n",
       " 'kridge_serotonin_receptor_antagonist',\n",
       " 'kridge_serotonin_reuptake_inhibitor',\n",
       " 'kridge_sigma_receptor_agonist',\n",
       " 'kridge_sigma_receptor_antagonist',\n",
       " 'kridge_smoothened_receptor_antagonist',\n",
       " 'kridge_sodium_channel_inhibitor',\n",
       " 'kridge_sphingosine_receptor_agonist',\n",
       " 'kridge_src_inhibitor',\n",
       " 'kridge_tachykinin_antagonist',\n",
       " 'kridge_tgf-beta_receptor_inhibitor',\n",
       " 'kridge_thymidylate_synthase_inhibitor',\n",
       " 'kridge_tlr_agonist',\n",
       " 'kridge_tnf_inhibitor',\n",
       " 'kridge_topoisomerase_inhibitor',\n",
       " 'kridge_trpv_agonist',\n",
       " 'kridge_trpv_antagonist',\n",
       " 'kridge_tubulin_inhibitor',\n",
       " 'kridge_tyrosine_kinase_inhibitor',\n",
       " 'kridge_vegfr_inhibitor',\n",
       " 'kridge_vitamin_b',\n",
       " 'kridge_vitamin_d_receptor_agonist',\n",
       " 'kridge_wnt_inhibitor',\n",
       " 'kmeans_g_0',\n",
       " 'kmeans_g_1',\n",
       " 'kmeans_g_2',\n",
       " 'kmeans_g_3',\n",
       " 'kmeans_g_4',\n",
       " 'kmeans_g_5',\n",
       " 'kmeans_g_6',\n",
       " 'kmeans_c_0',\n",
       " 'kmeans_c_1',\n",
       " 'kmeans_c_2',\n",
       " 'kmeans_c_3',\n",
       " 'kmeans_c_4',\n",
       " 'kmeans_c_5',\n",
       " 'kmeans_c_6',\n",
       " 'cp_time_24',\n",
       " 'cp_time_48',\n",
       " 'cp_time_72',\n",
       " 'cp_dose_D1',\n",
       " 'cp_dose_D2']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_cols_ini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:29.998Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:01.765602Z",
     "iopub.status.busy": "2020-12-06T00:02:01.764659Z",
     "iopub.status.idle": "2020-12-06T00:02:03.186319Z",
     "shell.execute_reply": "2020-12-06T00:02:03.187022Z"
    },
    "papermill": {
     "duration": 1.496035,
     "end_time": "2020-12-06T00:02:03.187204",
     "exception": false,
     "start_time": "2020-12-06T00:02:01.691169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pca_G-0 -3.165934989349514e-05 1.000022942477717\n",
      "pca_G-1 2.319063523304048e-05 0.9995899673134725\n",
      "pca_G-2 -5.652831749957289e-05 0.9998962469475887\n",
      "pca_G-3 0.0001490332009944081 1.0001572798115808\n",
      "pca_G-4 3.974275730490028e-05 0.9997665499591255\n",
      "pca_G-5 7.361036404477721e-05 0.9997785695567266\n",
      "pca_G-6 -3.722530770014762e-05 0.999715481686792\n",
      "pca_G-7 -4.176801412012035e-05 1.000241038168113\n",
      "pca_G-8 -3.368941635790669e-05 0.9998038447404771\n",
      "pca_G-9 5.615480974268078e-05 0.9997265629134643\n",
      "pca_G-10 -9.040019011924138e-05 0.9996772931738682\n",
      "pca_G-11 -6.078795307598129e-05 0.9997559892025472\n",
      "pca_G-12 -5.8613588758447095e-05 0.9994258399589678\n",
      "pca_G-13 -9.893185187003101e-05 0.9997849369040596\n",
      "pca_G-14 -6.201536405669977e-05 0.9997168847993001\n",
      "pca_G-15 -8.278464554968567e-05 0.999498292430778\n",
      "pca_G-16 -3.8333825188847665e-06 1.0007720562770313\n",
      "pca_G-17 -1.2060393554003437e-05 0.9995915837385392\n",
      "pca_G-18 -6.393268281031218e-05 1.0000080139572307\n",
      "pca_G-19 4.308745677197324e-05 0.9999375282232515\n",
      "pca_G-20 1.7704523145822335e-05 0.9999570958275731\n",
      "pca_G-21 -8.041781144052346e-05 0.9999804331381231\n",
      "pca_G-22 -5.485857327528849e-05 0.9995079062061677\n",
      "pca_G-23 -5.548156977286693e-05 0.9999853361194291\n",
      "pca_G-24 -2.8665351269704107e-05 0.9999622169561349\n",
      "pca_G-25 4.812149638492838e-05 0.9999803933858662\n",
      "pca_G-26 -0.00010797373222806547 0.9998528902952653\n",
      "pca_G-27 1.1238177293011613e-06 0.9994367067747953\n",
      "pca_G-28 2.7147614066845576e-07 0.999715123689365\n",
      "pca_C-0 -4.622811888495886e-05 0.9998971013558381\n",
      "pca_C-1 -2.4489414777803194e-05 0.9995720862132829\n",
      "pca_C-2 8.463152331796008e-05 0.9999324783445507\n",
      "pca_C-3 -6.815162067455375e-05 1.000053966679531\n",
      "g-0 0.0010856840847241374 1.0053220657129336\n",
      "g-1 -9.277820775970884e-05 1.000071001417792\n",
      "g-2 1.870428083275578e-05 0.9996307098585744\n",
      "g-3 -6.743828774588668e-06 1.000304307927906\n",
      "g-4 -0.00011262560407790162 0.9998436207945154\n",
      "g-6 6.471265619678452e-05 0.9995006551305287\n",
      "g-8 -0.011403304405636175 1.0445484320125424\n",
      "g-11 0.0001961747023225419 1.0009660114183692\n",
      "g-12 0.0002842104596479016 1.0008482393765796\n",
      "g-13 0.0015180329256341124 1.006389983269425\n",
      "g-14 -0.00037747159866311896 1.0019571802699656\n",
      "g-16 -0.0007190261635998846 1.002551338203055\n",
      "g-17 -0.0007079417796940588 1.002295967144567\n",
      "g-19 0.00010174843695813431 0.9998053548037309\n",
      "g-20 -0.00037477357653301947 1.0010816447682198\n",
      "g-21 2.3587022735505027e-05 0.9995912574206381\n",
      "g-22 8.696281549299983e-05 0.9998808161558901\n",
      "g-24 -0.0006756881163977918 1.0019179750409806\n",
      "g-26 -8.224523594409243e-05 0.9994860100835429\n",
      "g-27 -3.060099877633643e-05 0.999495146495556\n",
      "g-28 -4.263828430815671e-05 0.9995110498959296\n",
      "g-29 -3.65379557327136e-05 1.0005094929002283\n",
      "g-30 -0.00046745034274186555 1.0010483219418729\n",
      "g-32 -3.0315826365312747e-05 0.9997077635992525\n",
      "g-33 0.0005630549861009437 1.001692972074615\n",
      "g-34 0.001000606409649143 1.0033835095704036\n",
      "g-35 0.00011939699874578661 0.9998846675951207\n",
      "g-36 -0.0016049315402136164 1.0063484826096036\n",
      "g-37 -0.04229189852614354 1.1494492525518762\n",
      "g-38 0.029672702399398048 1.1081188838841267\n",
      "g-39 7.642194038556853e-05 0.9996106005727056\n",
      "g-40 -0.0019900857166518598 1.0081602977828834\n",
      "g-41 -0.0001047206531035147 1.0003455626307733\n",
      "g-43 0.0002830996256156844 1.0013046611243137\n",
      "g-44 1.7655779362868488e-05 0.9996377053284484\n",
      "g-45 -0.00014802394876134938 0.9998500445403651\n",
      "g-46 0.00317478667389809 1.0134084811174775\n",
      "g-47 5.764889397374842e-06 0.9998075604923247\n",
      "g-48 0.0017587464838719135 1.0070580075306623\n",
      "g-49 0.0002668705194959271 1.000545154232074\n",
      "g-50 -0.04788654617065789 1.1668562314051318\n",
      "g-51 6.694826431515453e-05 0.9999402813420075\n",
      "g-52 0.00015685792402927767 1.000266590978425\n",
      "g-53 0.0005305498604109714 1.002350463076067\n",
      "g-55 -0.0016375468134878118 1.0061107936854048\n",
      "g-56 -0.0007056709479265914 1.0022791676168508\n",
      "g-57 0.0014332241386355096 1.0067898338926113\n",
      "g-58 -0.020400310850086232 1.0764466093861662\n",
      "g-59 -0.000843240085739643 1.0037206290165925\n",
      "g-60 -3.124835276506994e-05 0.9993669334003209\n",
      "g-61 -0.0008758470537922545 1.0032478118693073\n",
      "g-62 -0.0060265607484797385 1.0240609833718686\n",
      "g-63 -0.007013261650213163 1.0278491730925832\n",
      "g-64 -4.512015122799767e-05 0.9994005553313573\n",
      "g-65 0.000650514722757141 1.0021489847046487\n",
      "g-66 4.4002447508597585e-05 0.9994001333066097\n",
      "g-67 -0.0582401878296116 1.1984533241212991\n",
      "g-68 0.00019236402727461292 1.0002964103424896\n",
      "g-69 0.00011084278050651603 0.9995541417142952\n",
      "g-70 -0.00014962180074589728 0.9999653125292162\n",
      "g-71 2.7472271302442193e-05 0.999692478561705\n",
      "g-72 -0.016730579654090683 1.0642939108171057\n",
      "g-73 -3.479065307940459e-05 0.9998231279442537\n",
      "g-75 -0.046891146713930125 1.1638544694858486\n",
      "g-76 9.119759576756511e-05 0.9992947715237521\n",
      "g-77 -2.786652522099436e-05 0.999912797251454\n",
      "g-78 7.355126293737198e-05 0.9999740850115219\n",
      "g-79 -6.261453226069835e-06 0.9994825278680407\n",
      "g-80 -8.610831821143288e-05 0.9998336441061584\n",
      "g-81 9.464916197524829e-05 0.9999792141628024\n",
      "g-83 0.0003078093026542147 1.0012011706168724\n",
      "g-84 -2.8503078366341273e-05 0.9997557408071786\n",
      "g-85 0.00011171679497606717 0.9998442904729642\n",
      "g-86 0.005534960354342711 1.0215932658782625\n",
      "g-87 0.00010201238253429359 1.0001826265954856\n",
      "g-88 0.0003502058777455489 1.0004775429596844\n",
      "g-89 4.9251903009597415e-05 0.9994004760705005\n",
      "g-90 0.000345968834606953 1.000539214263172\n",
      "g-91 0.0031553309822552286 1.0126860851271955\n",
      "g-92 3.9267940539615756e-05 0.9996543220078714\n",
      "g-93 0.00031884937817752094 1.000991525909543\n",
      "g-94 2.8334574827492127e-06 0.9993255574611908\n",
      "g-95 0.0002384812033239887 1.0008715698702229\n",
      "g-96 -0.0034735075231412615 1.0137362209118126\n",
      "g-97 0.004869180718094366 1.0191649742184181\n",
      "g-98 -0.003166341254543079 1.0123775546344933\n",
      "g-99 9.235096567171328e-06 0.9995208664289419\n",
      "g-100 0.013323393542922023 1.0508809515862862\n",
      "g-101 -4.215131751458992e-06 0.9994209213366492\n",
      "g-102 0.0010647875568484096 1.0041192238227594\n",
      "g-103 1.1720392395340204e-05 1.0001158839406463\n",
      "g-105 9.736168477250882e-05 0.9994666638699105\n",
      "g-106 0.0031782457464485728 1.012531101707252\n",
      "g-107 9.282501821053541e-05 1.000253888164456\n",
      "g-108 6.759460232761108e-05 0.9996644411568933\n",
      "g-109 0.00017256130201848637 1.0001150115562458\n",
      "g-110 -0.00013794362144140578 1.0004028861088827\n",
      "g-111 0.000797970441279844 1.0027509605150953\n",
      "g-112 -0.0003003081094234059 1.000717405413546\n",
      "g-113 -0.016796658885664766 1.0641758727305632\n",
      "g-114 8.056324977194203e-05 1.0000887624117683\n",
      "g-115 6.646922510192416e-05 0.9999652538342313\n",
      "g-117 0.00026721341569538787 1.0011991621298604\n",
      "g-118 -6.438587961776273e-06 0.9995854427368738\n",
      "g-119 -5.5615478969907615e-05 0.9995825755829153\n",
      "g-120 3.38776968537411e-05 0.9994342757934891\n",
      "g-121 -0.0029099441618257293 1.011736753910446\n",
      "g-122 -4.938543638532895e-07 0.999350152695265\n",
      "g-123 0.025968048683746376 1.0956308562300787\n",
      "g-124 8.86944650514474e-06 0.9994664743127173\n",
      "g-125 3.648801004871146e-05 0.9994742710302168\n",
      "g-126 0.0012306780256100599 1.0047458972739265\n",
      "g-127 8.086669717195677e-05 0.9993799967628639\n",
      "g-128 -0.015487283076513933 1.0599039933323005\n",
      "g-129 7.908404247730615e-05 0.9995557850527161\n",
      "g-130 0.004059604949360723 1.0160369147196413\n",
      "g-133 -5.358702891297025e-05 1.0057211668213502\n",
      "g-134 0.00020647339861691957 1.0002022538054365\n",
      "g-135 0.00012052006941045475 0.9999331057396923\n",
      "g-136 0.00023625691717269856 1.000472073894955\n",
      "g-137 -4.692439931591635e-06 0.9997132981333372\n",
      "g-138 -0.00029156523050832405 1.0026937513107232\n",
      "g-139 0.0008794221079788842 1.002973998091867\n",
      "g-140 3.298870280189798e-05 1.000044392776022\n",
      "g-141 2.005498762414164e-06 0.9996183977611525\n",
      "g-142 0.001160789615751812 1.0049336381958376\n",
      "g-143 -6.823090426450914e-05 0.999890784899179\n",
      "g-144 0.0036070013812506928 1.0142115068692281\n",
      "g-146 0.0018613055317740657 1.0080051191101327\n",
      "g-147 0.005773155580427365 1.0231528989385008\n",
      "g-148 0.0016832962826859748 1.0066668778889463\n",
      "g-149 -0.00010387416645374212 0.9998517344174879\n",
      "g-150 -9.688760551834485e-05 0.9997128766624496\n",
      "g-151 -5.163287673454723e-05 0.9993718475200601\n",
      "g-152 -0.004434531994674995 1.0173887847958123\n",
      "g-154 -8.392016991726123e-05 1.000774988925209\n",
      "g-155 0.002901143623529897 1.0120907353893887\n",
      "g-156 -6.39056106440837e-06 0.9995205584759289\n",
      "g-157 0.00026111526543776175 1.0003025964652994\n",
      "g-158 0.020868328981763568 1.078353586478225\n",
      "g-159 6.957883479297214e-05 0.9998113788950226\n",
      "g-160 -0.001230885426992516 1.0045745331044627\n",
      "g-161 -0.0003909731110172782 1.0014051424288488\n",
      "g-162 2.9652564887606208e-05 0.9994707656952833\n",
      "g-163 0.0052172982961836084 1.0210378301739103\n",
      "g-164 0.003472997798357876 1.0135250819447172\n",
      "g-165 -4.561704414670081e-05 0.9995182061173319\n",
      "g-166 -0.0002522156432084023 1.0004342382288247\n",
      "g-167 2.8284783804481296e-05 0.9997784149746936\n",
      "g-168 0.0001629196276407496 1.0004874093487492\n",
      "g-169 0.0002629623968822474 1.000606228926152\n",
      "g-170 0.006747635154014747 1.0270899147293622\n",
      "g-171 0.0003436528781872168 1.0005130354352583\n",
      "g-172 -0.00012235029428508016 1.0006371195437436\n",
      "g-173 2.6992162657032383e-05 0.9999788171004688\n",
      "g-174 -0.00013087246448437347 0.999945057398805\n",
      "g-175 -3.9067732330446444e-05 1.000020131573068\n",
      "g-176 -0.00017328139505793494 0.9999286642281758\n",
      "g-177 0.001410422249935155 1.0061868232288202\n",
      "g-178 -0.008087793412978888 1.0449888794697642\n",
      "g-179 -5.029253057939618e-05 0.9993806272350073\n",
      "g-180 3.945865184751963e-05 0.99929262340729\n",
      "g-181 0.0011882434368988596 1.0054248087707789\n",
      "g-182 0.00020221811195915461 1.000032650551435\n",
      "g-183 -0.003474393767338591 1.014353631736765\n",
      "g-185 -0.019374370877587328 1.072806690378079\n",
      "g-186 -0.0005919838314349526 1.002026747685688\n",
      "g-187 0.00011889047820751423 0.9996323166799992\n",
      "g-188 0.00013862502092864613 0.9996672712327398\n",
      "g-189 -0.0023595614240339803 1.0108570904065237\n",
      "g-190 2.377338507918543e-06 0.999687941368727\n",
      "g-191 -1.8469638225974083e-05 0.9997880710365935\n",
      "g-192 0.00010064893766004415 0.9999783274804145\n",
      "g-194 -0.00018718787945757798 1.0006105055574819\n",
      "g-195 -0.013852815983566531 1.0532927617389494\n",
      "g-196 -0.0006110470286010054 1.0015185249210554\n",
      "g-197 0.0035337059096291912 1.013584532484019\n",
      "g-199 -0.00012132593180523838 0.9999246631925712\n",
      "g-200 -5.390627061139611e-05 0.999744241665109\n",
      "g-201 0.010318955831430719 1.0400095166891867\n",
      "g-202 -0.011705118187446958 1.0452692045182317\n",
      "g-203 0.00015822317058976405 0.9998033941910466\n",
      "g-205 -2.0336886546779832e-05 0.9993481599633784\n",
      "g-206 -5.2236448396437755e-06 0.9993807051468245\n",
      "g-207 -6.42168030457151e-05 0.9996756956232318\n",
      "g-208 0.011885103957316704 1.0471444675514614\n",
      "g-209 0.00014482179215395745 1.0002940892778733\n",
      "g-210 0.0008783337807099783 1.002825444749206\n",
      "g-211 6.391463749276528e-05 0.9996842489804277\n",
      "g-212 9.224765209031916e-05 0.9996428538182857\n",
      "g-213 -0.0002921543950907238 1.0069009975519547\n",
      "g-214 -0.0001804455615557744 0.9997551990463769\n",
      "g-215 0.003087975648939873 1.0129114210697607\n",
      "g-217 0.000602635102418152 1.0019229256208246\n",
      "g-218 8.070329132220592e-05 1.0003132064380595\n",
      "g-220 0.0005206479322687917 1.0013850920464693\n",
      "g-221 0.0001512341237971588 0.999803552883958\n",
      "g-222 3.386583980062608e-05 0.9995942937252232\n",
      "g-223 5.117170907299554e-05 0.9998056734078712\n",
      "g-224 0.00013026484886814637 1.0002738202156654\n",
      "g-225 9.159687291055491e-05 0.9998949257479143\n",
      "g-226 0.005975771390920721 1.0242010576410914\n",
      "g-227 -5.436600152316314e-05 0.9997489172276043\n",
      "g-228 -0.07053023931896793 1.2339359551528015\n",
      "g-229 0.00018242654506448253 1.0014103094001108\n",
      "g-230 -8.857295171586114e-06 0.9997474183674196\n",
      "g-231 -0.0014161978396154786 1.0084025409454305\n",
      "g-232 0.0009594765627547391 1.003085047938825\n",
      "g-233 6.794113434663655e-06 0.9996609219778191\n",
      "g-234 -0.0007301118061879706 1.0023102804998032\n",
      "g-235 0.0013796868273937483 1.0053506177677012\n",
      "g-236 6.342416031103277e-05 0.999653482383446\n",
      "g-237 5.347518003569064e-05 0.999846164483604\n",
      "g-239 -0.000707976651539762 1.002449554302606\n",
      "g-240 -5.94645945391285e-05 0.9994815146581919\n",
      "g-241 -1.1669951241427181e-05 1.0006121772951428\n",
      "g-242 0.00018647943436079787 1.0000842372794654\n",
      "g-243 0.0028526861649098593 1.011365936770313\n",
      "g-244 0.0010668685461170332 1.00371576167664\n",
      "g-245 0.0008615490166291545 1.0041132899570309\n",
      "g-246 -9.603245502479327e-05 0.9994022701259614\n",
      "g-247 5.4306772723675944e-05 0.999654259072547\n",
      "g-248 0.022437351933455557 1.0836072490671116\n",
      "g-249 0.0008810509667273127 1.0035499425972252\n",
      "g-250 -0.0082525993230229 1.0319433672585148\n",
      "g-251 0.0032714497685269314 1.0129134626890097\n",
      "g-252 -0.00029023884335977086 1.0007025631163022\n",
      "g-253 -4.327941817463055e-05 0.9999249012936968\n",
      "g-254 0.0005814167988832864 1.001379743479954\n",
      "g-255 -2.8525628288827034e-05 0.9997904563258775\n",
      "g-256 5.923389939493955e-05 0.9997046975533365\n",
      "g-257 -0.008979425621929913 1.0351627194390682\n",
      "g-258 -3.434386586522635e-05 1.0002265577063092\n",
      "g-259 0.0042916702120582104 1.0170692211654415\n",
      "g-260 -2.2031021906578737e-05 0.9994025404677448\n",
      "g-261 0.008692383121523895 1.03375259712838\n",
      "g-262 -0.0001094125836054775 0.9995777904382646\n",
      "g-263 0.0005615771177902724 1.00163158083855\n",
      "g-264 0.003155134918078895 1.0141121028759676\n",
      "g-265 1.2724489246886146e-06 0.9998776675304919\n",
      "g-266 0.002712225189862549 1.0110027765038243\n",
      "g-268 -9.654675140817036e-06 0.9996580204346418\n",
      "g-269 2.9223085651713103e-05 0.9995866583847739\n",
      "g-270 0.014380680966700017 1.0557621586064112\n",
      "g-271 -0.0009680861558384219 1.0033142698939366\n",
      "g-272 -0.0008291038175720221 1.002800078593067\n",
      "g-273 8.533620124453553e-05 0.9996744145723591\n",
      "g-274 0.00014736707839960305 0.9995925252433048\n",
      "g-275 -0.00015277835081961523 1.0000739695999084\n",
      "g-276 -0.0006557949344621804 1.0019662200769188\n",
      "g-277 -3.981898261366737e-05 0.9995070549980107\n",
      "g-278 2.9161879856708644e-05 0.9996877812299574\n",
      "g-279 7.560401111037593e-05 0.9996394954467505\n",
      "g-280 -2.648731916184922e-05 1.000013495437661\n",
      "g-281 -0.0003311067818042026 1.0013453190578827\n",
      "g-282 7.714941917663853e-05 0.999542795058577\n",
      "g-283 -1.2550048464995538e-06 0.99940213587705\n",
      "g-284 4.916604308394578e-05 0.9996872810254137\n",
      "g-285 -3.7474257825070295e-05 0.9995845282065474\n",
      "g-286 6.392894446979533e-05 0.9995662980595516\n",
      "g-287 -9.566318196287805e-06 0.9994515402004863\n",
      "g-288 1.2329694082772764e-05 0.9993721676594493\n",
      "g-289 0.000584954557941153 1.0015568854865793\n",
      "g-291 -0.00401262742893555 1.016359632035133\n",
      "g-292 0.00011906947899355499 0.9993651693676407\n",
      "g-293 -5.3226006935209604e-05 0.9998864545476072\n",
      "g-295 0.0006495238595955457 1.0022247290657258\n",
      "g-297 0.00020066263287311066 1.0009928479444525\n",
      "g-298 -0.0036727829217132334 1.0145447819641213\n",
      "g-299 -0.00020927321724633138 1.0007979414077852\n",
      "g-300 0.0010462904681200732 1.037071350299664\n",
      "g-301 -0.0001252834322541869 0.999741683274226\n",
      "g-302 4.530570333326828e-05 0.9994108382407803\n",
      "g-303 2.9033004605797912e-05 0.9996050183750633\n",
      "g-304 -0.0004888240720097249 1.0014320779767978\n",
      "g-305 6.455995806287291e-05 0.9999589901750734\n",
      "g-306 -0.0009535643939246406 1.0039362229247724\n",
      "g-308 0.001406682655653368 1.0061125432927465\n",
      "g-309 5.293906122206065e-06 0.9994311681796135\n",
      "g-311 7.372780988721356e-05 0.9996348993260218\n",
      "g-312 0.00011445317217758985 1.0001349057553774\n",
      "g-313 2.2312364053472735e-06 0.9996513076153787\n",
      "g-314 -0.0003184291353777989 1.000365234432352\n",
      "g-315 -0.00015740317678363276 1.0001642841884515\n",
      "g-316 -6.822280890138964e-05 1.000212314763275\n",
      "g-317 0.001490298033116478 1.005741481672964\n",
      "g-318 -3.3171467518295776e-05 0.9993828182492148\n",
      "g-319 0.000772316150350661 1.002501287050322\n",
      "g-320 0.0002769056225413036 1.0003523887752965\n",
      "g-321 -0.0003553272435718142 1.001457246357642\n",
      "g-322 4.2535816989171754e-05 0.999891020342755\n",
      "g-323 -8.142614490053337e-07 0.9994201592877662\n",
      "g-324 -3.432208101855812e-05 0.9998404774940016\n",
      "g-325 7.195096427912297e-06 0.9994907266387946\n",
      "g-326 8.110567361788739e-05 0.9994509300714407\n",
      "g-327 -0.0015439894747857792 1.005832842008127\n",
      "g-328 0.034976259835646256 1.126193323910819\n",
      "g-329 -0.0003914260668121301 1.0010929139151965\n",
      "g-330 0.0001057816272136442 0.9997524605946985\n",
      "g-332 -0.008893507280506932 1.0354001293472803\n",
      "g-333 0.0007186859787208428 1.0029922288834205\n",
      "g-334 0.005049234316711668 1.020019584095348\n",
      "g-335 7.81283807029421e-05 0.9998826827633236\n",
      "g-336 0.0005393306246223805 1.0022218679529697\n",
      "g-337 0.0010266407284247924 1.0034928937371235\n",
      "g-338 0.001647341741478239 1.0088446639559445\n",
      "g-339 -0.0001498999052254011 0.9997077681716772\n",
      "g-340 -2.8690512018190153e-05 0.9998233743226782\n",
      "g-341 -7.726917988975683e-05 1.0000239515921856\n",
      "g-342 0.00010482590201385753 1.001653535930268\n",
      "g-343 0.00021782431324667592 1.0009175681930327\n",
      "g-344 0.000838409410188903 1.0040121618128581\n",
      "g-345 -2.2891266857884906e-05 0.9995048340953253\n",
      "g-346 -4.9687613079101646e-05 0.9995761342799505\n",
      "g-347 5.868634474142987e-05 1.0001849404060055\n",
      "g-348 0.0008765350412267515 1.00367300004093\n",
      "g-349 0.011836534249555967 1.0465032970460915\n",
      "g-350 6.51038424009514e-05 0.9996183697117251\n",
      "g-351 0.00012112067747259536 0.9996881590326961\n",
      "g-352 0.00024072454771726254 1.0012283481582094\n",
      "g-353 -0.00690668046178414 1.0273215522441645\n",
      "g-354 3.2526932709420135e-05 0.9996528677918999\n",
      "g-355 -2.0936761223858962e-05 0.9996379517290467\n",
      "g-356 1.21527134901138e-06 0.999486417141116\n",
      "g-357 -8.646680993179657e-05 0.9996579244177048\n",
      "g-358 -7.3721403767665e-05 1.000455870473529\n",
      "g-359 0.0004304316404263352 1.0017849051317895\n",
      "g-360 -0.0005962268388331847 1.0022747446667162\n",
      "g-361 4.194479680197013e-05 0.9994007146191233\n",
      "g-362 -7.154826966351469e-05 0.9997957417193334\n",
      "g-363 8.183099929097522e-05 0.9998663365419872\n",
      "g-364 -0.0021339211622788683 1.0089933448426038\n",
      "g-365 2.7032350012401305e-06 0.9998200877444012\n",
      "g-366 -0.00011277229674940055 0.9995790728640291\n",
      "g-367 -6.30731581803649e-05 1.0014813798452682\n",
      "g-368 0.00358636320766716 1.0140529134137517\n",
      "g-371 -0.0003020163302889841 1.0011595559553705\n",
      "g-372 -8.10082531845802e-05 1.000020617312105\n",
      "g-373 -0.0004978995655304952 1.0014510864926092\n",
      "g-374 0.0023831551861756973 1.0096388863704588\n",
      "g-375 -0.0024557983064470803 1.009651892359786\n",
      "g-376 -0.00016085750120705577 1.0002752896020008\n",
      "g-377 0.0008676630554911895 1.0033292347642562\n",
      "g-378 0.00014372319922649018 1.0004423755638552\n",
      "g-379 -0.0006016056488181628 1.0026134404186509\n",
      "g-380 4.303025394266947e-05 0.9992098011678314\n",
      "g-381 -2.584069821793623e-05 0.9993874238856361\n",
      "g-382 1.300920409661336e-05 0.9998842304265093\n",
      "g-383 3.438162860764616e-05 0.9996263197513651\n",
      "g-384 -0.00013844867808949195 0.999806591005266\n",
      "g-385 -0.016306802186374064 1.0623341140415565\n",
      "g-387 -0.00010027484756840901 0.9999579865786397\n",
      "g-388 -0.0004403658807918749 1.0019830437237915\n",
      "g-389 0.00025468074881871593 1.0010034019844134\n",
      "g-390 -0.0006298663875964384 1.0020308936053686\n",
      "g-391 2.486428413058829e-05 0.9992145061081426\n",
      "g-392 0.027233692138379693 1.100298202693916\n",
      "g-393 0.002199363549732731 1.008291928770062\n",
      "g-394 0.0005714355153124714 1.0030322364161774\n",
      "g-395 0.0007323085459530719 1.0024620902503552\n",
      "g-396 -0.00010671461584594512 0.999508331024957\n",
      "g-397 1.5164661283060334e-07 1.000017107995745\n",
      "g-398 4.799423984529166e-06 0.9993814553523346\n",
      "g-399 -0.00014428422718045287 0.9997382272611802\n",
      "g-400 -0.0004540262398579202 1.0020555008623948\n",
      "g-401 -1.4240787344113952e-05 0.9997259392812112\n",
      "g-402 0.0008488231799692274 1.0031424093351688\n",
      "g-403 3.557389686275486e-05 1.0001145942574905\n",
      "g-404 -2.9405159528587288e-05 0.9994101641167118\n",
      "g-405 -0.0003638872737892606 1.0010296928472364\n",
      "g-406 -0.01660686640860657 1.0638508372905684\n",
      "g-408 -9.409669645184e-05 1.000014422751131\n",
      "g-409 0.0005133276151380897 1.0020422083695635\n",
      "g-410 7.869480176540845e-05 0.9998800850587323\n",
      "g-411 -0.020530056957663308 1.0768896423688314\n",
      "g-412 5.507471180582764e-05 0.9994593717679344\n",
      "g-413 -0.0001468403541331751 1.0001925170946624\n",
      "g-414 -0.0003599127067282363 1.000930217463099\n",
      "g-415 -0.0002686857295471402 1.0002494045867107\n",
      "g-416 -6.791963316977216e-05 0.9999761015104424\n",
      "g-417 0.04398780408857785 1.1545418812926918\n",
      "g-418 3.542462677839846e-05 1.0004048143200053\n",
      "g-419 0.000319205986986933 1.000742944935079\n",
      "g-420 0.00010336036421055288 0.9996854542113559\n",
      "g-421 -0.0007872990623085061 1.0025248490880707\n",
      "g-422 0.00024766310526672927 1.0005347789924632\n",
      "g-423 -0.003350512613591671 1.012815787059476\n",
      "g-424 0.0029019921398072916 1.0112421366200424\n",
      "g-425 -6.310806149709559e-05 0.999864405147701\n",
      "g-426 -1.3444900450342463e-05 0.9997293936459648\n",
      "g-427 0.0008912246550443706 1.00312686712729\n",
      "g-428 1.263852251129537e-05 1.000515290135725\n",
      "g-429 1.359700517569553e-06 1.0001169213933616\n",
      "g-431 -0.00030565390329633933 1.0010724020477333\n",
      "g-432 8.867503877749868e-05 0.9996213396026729\n",
      "g-433 -0.0003008722062034469 1.001082736125666\n",
      "g-434 -0.0006973488195214741 1.0025297806601539\n",
      "g-436 0.0005412387972183424 1.0022733039742893\n",
      "g-437 0.0005907286603137434 1.0019898801761133\n",
      "g-439 -0.002594840749189877 1.0106779856328931\n",
      "g-440 -8.825043848798583e-06 0.9997117841287427\n",
      "g-441 -3.9715689515057425e-05 0.9995354762963454\n",
      "g-442 -2.5811323255381414e-05 0.9992433897013812\n",
      "g-443 0.002530374074339745 1.0095044113138099\n",
      "g-444 -0.00019094454004772127 0.9998360652598973\n",
      "g-445 6.117664785564954e-05 0.9994082201265851\n",
      "g-446 7.565499923645232e-05 0.9994718429423735\n",
      "g-447 -2.8803769597370493e-05 0.9996188988097574\n",
      "g-450 1.9415217612710353e-06 1.0001817666895594\n",
      "g-451 4.0852561583865215e-06 0.99953930741478\n",
      "g-453 7.138559709272564e-05 0.9998593625478177\n",
      "g-454 -2.9734428773853967e-05 0.9998796297641166\n",
      "g-455 0.0002817040349619655 1.0003107273787153\n",
      "g-456 0.0001989299050910492 1.0001721193856699\n",
      "g-457 0.0005783302630084813 1.0022353078879624\n",
      "g-458 -0.00028680369825089853 1.0005930479937855\n",
      "g-459 0.00012429283255129827 1.000719684221549\n",
      "g-460 0.027630196669846868 1.1015099056928217\n",
      "g-461 6.535284663455515e-05 0.9998551616837038\n",
      "g-462 2.790290326574537e-06 0.9997118553338279\n",
      "g-463 5.1632799199051355e-05 0.999450978617518\n",
      "g-464 -0.0010336307672908017 1.0041323493764618\n",
      "g-465 -0.0003191185261244178 1.0007312727189672\n",
      "g-466 -4.887456034078021e-05 1.0002088770191377\n",
      "g-467 0.0019051639943279436 1.0069518443965884\n",
      "g-468 0.002498344476254863 1.0102069093762276\n",
      "g-469 2.504889100182906e-05 0.999978805539926\n",
      "g-470 -0.0007234063569350836 1.0029001766459047\n",
      "g-471 4.964168126376138e-05 0.999491307943952\n",
      "g-472 0.00010193898413013756 0.9997517905516897\n",
      "g-473 -2.786877442490178e-05 0.9994184806517955\n",
      "g-474 -0.0003998502480648469 1.001179099592604\n",
      "g-475 7.349167891374024e-05 0.9995556400679239\n",
      "g-476 -0.00011043157772435002 0.9998676542360789\n",
      "g-478 0.0025221918905548447 1.010346828482672\n",
      "g-479 -0.0004623868197601816 1.0016872612327163\n",
      "g-480 0.00012639210183894783 1.0004761041321513\n",
      "g-482 -9.891974213368013e-05 1.0000484421927671\n",
      "g-483 6.0197653077119144e-05 0.9994832101940986\n",
      "g-484 0.0016769357097242117 1.006590252734028\n",
      "g-485 0.0001131890994283466 0.9997593830718018\n",
      "g-486 -0.0015791598555560048 1.0060941345512284\n",
      "g-487 0.00024427921170536176 1.0011618238342477\n",
      "g-488 -7.930839625627945e-05 1.0002924072086778\n",
      "g-489 -0.014751993396967644 1.0563330139643423\n",
      "g-490 -5.447790631277157e-05 0.9996782530073073\n",
      "g-491 0.00011897045479987976 0.999375631744683\n",
      "g-492 9.304652205488155e-06 0.99982156052764\n",
      "g-493 6.179203612421085e-05 0.9993696124563041\n",
      "g-495 0.0003286836358088165 1.003847232923598\n",
      "g-497 -0.0019277043127259581 1.0072345471444681\n",
      "g-498 -0.00030513706266969276 1.00072391438703\n",
      "g-499 0.000219629783409143 1.0011849077283237\n",
      "g-500 0.0028921377256851547 1.01118741095697\n",
      "g-502 9.302660605217356e-05 0.9993062370926855\n",
      "g-503 -0.003791477862646369 1.0155205738055764\n",
      "g-504 0.0047363583601962 1.0187324891886158\n",
      "g-506 -0.0006595322382316542 1.0025039714820554\n",
      "g-507 9.504246957456226e-05 0.9994195080549797\n",
      "g-508 -0.019444940462419603 1.0734756738587812\n",
      "g-509 -1.7429965475702816e-05 0.9996534696569218\n",
      "g-510 -3.781630505388506e-05 0.9996131363700202\n",
      "g-511 -5.254918627913637e-05 0.9995948431231269\n",
      "g-512 0.007474090424264645 1.0294700011670264\n",
      "g-515 -6.075117804511175e-06 0.9994584523798372\n",
      "g-516 -2.425769122430012e-05 0.9997166151299935\n",
      "g-517 -3.359355867958254e-05 0.9997609620696435\n",
      "g-518 -0.00015550974187018824 0.9996927355022596\n",
      "g-519 3.8213413752338104e-05 0.9999571035382738\n",
      "g-520 -0.00021182454130909138 1.0004654263031891\n",
      "g-521 -0.0005903053390229004 1.0025993630282108\n",
      "g-522 -0.001997062113772155 1.0082288098237344\n",
      "g-523 2.223002940589028e-05 0.9992318986085611\n",
      "g-524 0.00010271366313710555 0.9995332146428711\n",
      "g-525 -6.987779419810898e-06 0.999588843940178\n",
      "g-527 -5.9420358088872116e-05 1.0001824063515343\n",
      "g-528 -4.429825286767605e-05 0.9995187814094372\n",
      "g-529 0.00037811472351807827 1.0004789563291068\n",
      "g-531 2.459112457515227e-05 0.999849537377119\n",
      "g-532 3.587409919009628e-05 0.9994260243691578\n",
      "g-533 -6.275761030392053e-05 1.0000271670053298\n",
      "g-535 0.003510439250514877 1.0143391814701046\n",
      "g-537 1.7657368782619555e-05 0.9998690526649204\n",
      "g-538 5.7461523907068184e-05 0.9994797520828214\n",
      "g-539 5.5557498688532184e-05 0.9994138282516694\n",
      "g-540 -0.0007140075716653983 1.0027557618478935\n",
      "g-541 0.008290303704580447 1.0325888111829926\n",
      "g-542 9.162563172889152e-05 0.9994573987396724\n",
      "g-543 3.093407424366546e-05 1.000398927675016\n",
      "g-544 7.444766486779077e-05 1.000241736166708\n",
      "g-546 0.002562788216043993 1.0099998770162422\n",
      "g-547 3.9227145675358125e-05 0.9993907982495849\n",
      "g-548 -0.00029067674349074094 1.001131650468843\n",
      "g-549 -0.0001463167521723697 0.9996870865474702\n",
      "g-551 0.0009338527016331981 1.0034065602731097\n",
      "g-554 -5.5089443357166215e-06 0.9994249020462289\n",
      "g-556 -0.00011081405052461379 1.0001238107852801\n",
      "g-557 0.00012104254218578292 1.0006338026827553\n",
      "g-558 -0.00013350748232151486 1.000417803549404\n",
      "g-559 -0.0002969777057510361 1.0009582864046516\n",
      "g-562 -1.5746584816693964e-05 0.999212118693913\n",
      "g-563 0.00028261851353302036 1.0017450730839557\n",
      "g-564 -0.0003273512061919225 1.00171121766795\n",
      "g-565 -0.0003720023752590128 1.000647664415917\n",
      "g-566 2.0848446967425505e-05 0.9997082970506239\n",
      "g-567 6.142322736894694e-05 0.9992815757897197\n",
      "g-568 -0.010551863502776121 1.0406662513517364\n",
      "g-569 -0.0035982842204573704 1.0139980817846292\n",
      "g-570 5.591690140850064e-05 0.9998486692431237\n",
      "g-571 -0.000319345469452806 1.0012959659519585\n",
      "g-572 0.00010122012569067003 0.9994747539499822\n",
      "g-574 -0.00026138367790378434 1.0001722485405642\n",
      "g-575 5.313660475727031e-05 0.9995079454268339\n",
      "g-577 0.0009177423136130406 1.0031875527673368\n",
      "g-578 0.0012356446582671122 1.004315390537524\n",
      "g-579 -0.00044380902038593554 1.0015349641568028\n",
      "g-580 9.81638308324308e-05 1.0006479121420875\n",
      "g-581 2.0113993841629838e-05 0.9994705466014363\n",
      "g-582 8.431056768401919e-05 1.000107803688515\n",
      "g-583 9.397829699977361e-06 1.0001034867305936\n",
      "g-584 -0.00016703610055773783 0.999876746928769\n",
      "g-585 3.20512739163156e-05 1.000007027621339\n",
      "g-586 -0.00017179854295741684 1.0000591179947405\n",
      "g-587 7.558090928082524e-05 1.0003484414968564\n",
      "g-588 -0.003134765975129879 1.0128211324095577\n",
      "g-589 0.00010735353581110994 0.9999908761954422\n",
      "g-590 -0.00016403595255544529 1.0000476871042183\n",
      "g-591 -4.35290859722794e-05 0.9997994738840406\n",
      "g-592 4.324675199306021e-05 0.9994499803833417\n",
      "g-593 6.36781541577672e-05 0.9994080789349882\n",
      "g-594 6.21733878507514e-05 0.9997213064537115\n",
      "g-596 0.0021240966237747166 1.0085253186313512\n",
      "g-597 -0.0003903715709157206 1.0012961550998358\n",
      "g-598 -1.0712507351092104e-05 1.0000761193129963\n",
      "g-599 0.00011671342698570641 0.9993732928736543\n",
      "g-603 0.0005213411394015745 1.001354113021254\n",
      "g-604 -0.00018876538562111815 1.000861725506544\n",
      "g-605 -4.3647753450114064e-05 0.9999983317030441\n",
      "g-606 1.3958239936616644e-05 0.9996041668592326\n",
      "g-608 9.345883545589737e-05 0.9998506266645817\n",
      "g-609 7.659690586370274e-05 0.9999595915178684\n",
      "g-610 -6.704819315000117e-05 0.9996127023670046\n",
      "g-612 -0.00013572758571480612 1.0005662664890524\n",
      "g-613 -0.00010454638830196813 0.9997072071910319\n",
      "g-614 -0.0003043837787547288 1.0006876233558635\n",
      "g-615 -7.938915532993555e-05 0.9997341455372699\n",
      "g-616 9.226524163999379e-05 0.9994147803151638\n",
      "g-617 -0.00029393723485811476 1.0009744589037015\n",
      "g-618 -0.0005127944081333084 0.9997068649544336\n",
      "g-619 0.0020436007500921815 1.0084804590354972\n",
      "g-620 0.00016535334246327917 0.9998386705685922\n",
      "g-621 -4.6186625599774365e-05 0.9995684775707981\n",
      "g-622 -0.00012447893037171182 0.9997925928671831\n",
      "g-623 0.0002640350306655632 1.0005186119694733\n",
      "g-624 -0.0017261765688886958 1.007206924448293\n",
      "g-625 0.0006741489261658866 1.0033131563144724\n",
      "g-626 2.166602025820823e-05 0.9993977138349182\n",
      "g-627 3.3105328630873066e-05 0.999247846785159\n",
      "g-628 0.0007803621358847711 1.0031013250584568\n",
      "g-629 0.0073746721932813454 1.0290781233322286\n",
      "g-630 -0.0003880034109508269 1.0008804882461266\n",
      "g-631 0.00011016584379294616 1.0001348951299807\n",
      "g-632 -0.002089506331883196 1.008750044656345\n",
      "g-633 -4.8003287993646835e-05 0.9997645924314195\n",
      "g-634 2.2071238949758095e-05 0.9994874253515478\n",
      "g-635 0.0006729028651702327 1.0024436763824855\n",
      "g-636 0.000977048427152382 1.0033161608396712\n",
      "g-638 -0.0006752330313510197 1.0022809525753262\n",
      "g-639 9.251692104305104e-05 0.9994866309479224\n",
      "g-640 0.00016015934740908126 1.0005667535645635\n",
      "g-641 -0.00012384964514847968 1.0000844466950978\n",
      "g-642 5.207526971540263e-05 0.9995517558996737\n",
      "g-643 -0.0015344928806587715 1.0061690214571635\n",
      "g-644 -0.015804575400222298 1.0613080650698228\n",
      "g-645 1.5575995605302705e-05 0.9997561829172104\n",
      "g-646 0.0047715789795347505 1.0187411309769894\n",
      "g-647 0.00019276891284375906 1.000524893090209\n",
      "g-648 0.0010847480496518791 1.0042447762919666\n",
      "g-649 -6.76237129487362e-05 0.9994355020046399\n",
      "g-651 0.0005174773565495283 1.0011837632550482\n",
      "g-652 -0.0007637653940007955 1.0026592314116323\n",
      "g-653 -0.0006698942294054954 1.0020460181112252\n",
      "g-655 0.00013342940977704735 0.999841670453981\n",
      "g-656 -1.852957100571395e-05 0.9998418755804901\n",
      "g-657 -0.00012913725439687106 1.000414721551762\n",
      "g-658 0.0006661606204813722 1.002280050365722\n",
      "g-659 1.5674992361843506e-05 0.9997122009565291\n",
      "g-660 0.00010254432669751679 0.9997759047681152\n",
      "g-661 -0.00020014861931807649 1.0006883367784094\n",
      "g-662 7.015157967856963e-05 0.9994965392198006\n",
      "g-663 0.0001688687480979832 0.9999207470122549\n",
      "g-664 -0.00042005846076367554 1.0009943926043527\n",
      "g-665 -0.0013798136765696932 1.0045421923782922\n",
      "g-666 0.00010301547038907287 0.9998401190485882\n",
      "g-667 -0.0002018860406917672 1.0000207916310362\n",
      "g-668 3.4726471806727e-06 0.9996986042198724\n",
      "g-669 -0.002129757378663416 1.0091844187531396\n",
      "g-670 -0.00038038543436517806 1.0020252994861254\n",
      "g-671 -0.00018948651900980288 1.0009080581091896\n",
      "g-672 -0.009114142675257675 1.0355312533728362\n",
      "g-673 -0.0003664295884627404 1.00084076747467\n",
      "g-674 -0.004972941916024866 1.0195340499741892\n",
      "g-675 6.8033420117136005e-06 0.999995698717916\n",
      "g-676 0.0002420231334699168 1.0009428103694702\n",
      "g-677 -0.000375217299066061 1.0012010115231371\n",
      "g-678 -0.0007993598676973173 1.003684177859586\n",
      "g-679 0.00010371155925009526 1.0001529978394759\n",
      "g-680 6.258098411238148e-05 0.9998282006976745\n",
      "g-681 -0.0002264154825830829 1.0007726968396988\n",
      "g-682 0.00013113400828856762 0.9997174505029446\n",
      "g-683 -0.00030029870935650664 1.0009146265594446\n",
      "g-684 -0.00025609313468864976 1.0005334453992585\n",
      "g-685 -0.002210416574344131 1.0088529833406692\n",
      "g-686 0.0003345925637334223 1.00055663233836\n",
      "g-688 0.00036007079372078505 1.001920614944292\n",
      "g-689 0.0006662041961777782 1.001732235736836\n",
      "g-690 2.6295806168944638e-05 0.9995771289558818\n",
      "g-691 -0.010645216897032414 1.0412847563766718\n",
      "g-692 -1.4209205910125814e-05 0.9993404161548657\n",
      "g-693 0.00045634247105142686 1.001418160274508\n",
      "g-694 -5.3149215454251075e-05 0.9996343943374257\n",
      "g-696 2.939323815785864e-07 0.9993742421303603\n",
      "g-697 0.002078168242046641 1.0086428080174683\n",
      "g-698 -0.00010255452645243649 0.9997576335686673\n",
      "g-699 0.000275933377738717 1.000813412018343\n",
      "g-701 0.0001916740532122643 1.0004447398929948\n",
      "g-702 -0.00010217881409659998 0.9998898221737054\n",
      "g-704 -0.0016631229203128452 1.006404883232669\n",
      "g-705 -0.005931331794694088 1.0234758435901783\n",
      "g-706 8.407874422449386e-05 0.9997050551704515\n",
      "g-707 0.0038351016875123427 0.989193332495882\n",
      "g-708 0.0009653598745621311 1.0041315413312393\n",
      "g-710 0.0008020664694013946 1.0026680398409933\n",
      "g-711 0.0010532946226255177 1.0054139592124343\n",
      "g-713 -0.00015038806981319846 0.9997556412561199\n",
      "g-714 1.9131233885588726e-05 0.999458326832348\n",
      "g-715 -0.000121954619500967 1.000018864491462\n",
      "g-717 0.00032790292818483664 1.0005621104738305\n",
      "g-720 9.155872358690989e-05 0.9999822904496783\n",
      "g-721 0.0024064881941470847 1.0098813246182186\n",
      "g-722 0.00042520438970195044 1.001017336729368\n",
      "g-723 0.007141968627318623 1.0282468986214497\n",
      "g-724 0.0009370668547895865 1.0035427844462217\n",
      "g-725 -7.528412392642562e-05 1.0000156623711591\n",
      "g-726 -0.00035359847457327743 1.0014860229380877\n",
      "g-727 -5.1270196389725915e-05 0.999821989532379\n",
      "g-728 0.00011482695341225876 0.9994228863115465\n",
      "g-729 0.00022268537357137278 1.000716071114148\n",
      "g-730 -2.6170988987591998e-06 0.9995525357816576\n",
      "g-731 0.011013577843920297 1.0441436497402237\n",
      "g-732 4.0754321079578354e-05 0.9995359328972774\n",
      "g-734 -5.600764102769486e-05 0.99961378051442\n",
      "g-735 6.834067846271736e-05 1.0001012761802943\n",
      "g-736 -0.0011250244667897688 1.0046998950350294\n",
      "g-737 -0.0004076437519534919 1.0013160927487816\n",
      "g-739 -0.0003996217802971508 1.0011978224459361\n",
      "g-740 9.003208655479572e-06 0.9994041170550753\n",
      "g-741 0.0001205758174836524 1.000460060860332\n",
      "g-742 -7.914125844717906e-05 1.0000211127959713\n",
      "g-743 -0.00023843662921763746 1.0003443102240477\n",
      "g-744 0.034473179723233806 1.1242653880680158\n",
      "g-745 0.002301575680744719 1.0088062224082575\n",
      "g-746 1.1240085954966099e-05 0.9998962036924991\n",
      "g-747 0.00014284757965433 1.0004067003444816\n",
      "g-748 0.000603015639426562 1.001847938528747\n",
      "g-749 0.00011560321192738093 1.0002607243303634\n",
      "g-750 0.0009061009171935049 1.0037107583044136\n",
      "g-751 0.00016225168730782033 1.0006788103324038\n",
      "g-752 8.265865825905793e-05 0.999879884674311\n",
      "g-753 0.000868828791489181 1.0028985327922193\n",
      "g-754 3.443531686048475e-05 0.9996657854728251\n",
      "g-757 -0.0003744041578685135 1.0014411866841926\n",
      "g-758 0.0008008136060439968 1.003022322113818\n",
      "g-759 -0.0017342714786516263 1.0100254287336459\n",
      "g-760 -0.030083242336263697 1.1102758271686284\n",
      "g-761 -0.0029189593712400583 1.012567111114479\n",
      "g-762 1.1901879835291833e-05 0.9996663267579856\n",
      "g-763 -0.00018203546474650326 1.0000737131900606\n",
      "g-764 0.0009896897494919659 1.0031958161958365\n",
      "g-765 3.419271394997779e-05 0.9995672581293835\n",
      "g-766 2.378141952432783e-05 1.000580771310933\n",
      "g-767 5.77287441739006e-05 0.9998858490456577\n",
      "g-768 0.00025306904737297644 1.0007445145577905\n",
      "g-769 -0.0015845753257253024 1.0068504089944887\n",
      "g-770 -0.0007396645828869475 1.002108446865941\n",
      "g-771 0.0016842447335608897 1.0080677781373275\n",
      "c-0 -0.026012429819970682 1.0964306844668228\n",
      "c-1 -0.019043155519004283 1.0721885686985428\n",
      "c-2 -0.03157677242782929 1.115101314598892\n",
      "c-4 -0.03349047444949758 1.1211868096888382\n",
      "c-5 -0.022166110192194704 1.0828115871192245\n",
      "c-6 -0.0375622070104899 1.1344627879376945\n",
      "c-7 -0.0009469587734326564 1.0039855481530378\n",
      "c-8 -0.02693934151919207 1.0994278359419556\n",
      "c-9 -0.0006645803106134242 1.0023999591964683\n",
      "c-10 -0.03355286739135655 1.1219271854373314\n",
      "c-11 -0.031841003698077214 1.1159799328501399\n",
      "c-12 -0.030025075711200055 1.1096759447771312\n",
      "c-13 -0.035219504184714485 1.127078484219191\n",
      "c-14 -0.002781208702380528 1.010739395721891\n",
      "c-15 -0.025524835013243128 1.0944347062967246\n",
      "c-17 -0.0332942460058295 1.1209823238798\n",
      "c-18 -0.03417224750759046 1.1234472489819707\n",
      "c-19 -0.0043106016287436395 1.0170868453917994\n",
      "c-20 -0.027893996252148563 1.1024225409993516\n",
      "c-21 -0.025476301412586486 1.0946573721060764\n",
      "c-22 -0.0039597248917312925 1.0159215102797616\n",
      "c-23 -0.0016326481541072098 1.0069174993155676\n",
      "c-25 -0.003833681853129623 1.0157069334191922\n",
      "c-26 -0.007621929389728505 1.0299230109306368\n",
      "c-27 -0.004908301906748997 1.0198283753125856\n",
      "c-28 -0.0022072731692686946 1.008484791169346\n",
      "c-29 -0.00030135352169269043 1.0006956532252165\n",
      "c-30 -0.0002976381640594334 1.0007231085186212\n",
      "c-31 -0.0071748109747761385 1.0282234093154918\n",
      "c-33 -0.012391363929638303 1.0480699031903962\n",
      "c-34 -0.004209902217294962 1.016739792832574\n",
      "c-36 -0.0041846744941137146 1.0165651618834661\n",
      "c-38 -0.013798291220843651 1.0535852981636462\n",
      "c-39 -0.003918190038401932 1.0160169505334846\n",
      "c-40 -0.004058889547713381 1.0163706981604914\n",
      "c-41 -0.00589812304310055 1.0243701876942446\n",
      "c-42 -0.005864167348713945 1.023742339683451\n",
      "c-43 -0.004227229753106392 1.017250429343886\n",
      "c-44 -0.004424802876308636 1.017468626151088\n",
      "c-46 -0.001677257597766084 1.0068002705815842\n",
      "c-47 -0.0010763576597357368 1.0037645249360292\n",
      "c-48 -0.0024095283112616273 1.0093133086431936\n",
      "c-50 -0.001765160859255534 1.0070228477312146\n",
      "c-51 -0.026177161977879723 1.0967228762771306\n",
      "c-52 -0.023805801323573603 1.0887045191325997\n",
      "c-53 -0.01628196282322183 1.0624449118414026\n",
      "c-54 -0.011602884811694452 1.0447334662129997\n",
      "c-55 -0.04911061910448687 1.1708397012852683\n",
      "c-56 -0.0017084232219417774 1.006349892371913\n",
      "c-57 -0.012480699656738732 1.048610293518574\n",
      "c-59 -0.05378538269124793 1.1851854795361274\n",
      "c-60 -0.055714284510838326 1.1906343094929392\n",
      "c-62 -0.022102149003610436 1.0829267701492662\n",
      "c-63 -0.054224572109202354 1.186499441140978\n",
      "c-64 -0.014323627538404543 1.055247135580937\n",
      "c-65 -0.04381511008464077 1.1542233733525677\n",
      "c-66 -0.023960454922786183 1.0898765368331766\n",
      "c-67 -0.02589561904145741 1.0958416260543944\n",
      "c-70 -0.05944013593716723 1.2017984966617221\n",
      "c-71 -0.0106252088794668 1.0422208227046756\n",
      "c-72 -0.0433984081146017 1.1529060230825112\n",
      "c-73 -0.03960190891262202 1.141184099379804\n",
      "c-75 -0.03825996449052449 1.1367179431548997\n",
      "c-76 -3.955127688350214e-05 0.9996707437109585\n",
      "c-77 -0.021974428909633477 1.0824539005487803\n",
      "c-79 -0.00897057815328123 1.0352296288130527\n",
      "c-80 -0.046884908315247914 1.1638679981526288\n",
      "c-81 -0.041845113869292266 1.1481541617486228\n",
      "c-83 -0.04699948220713892 1.1644087528713103\n",
      "c-84 -0.04529944096447949 1.1591727960570761\n",
      "c-85 -0.02868597369890429 1.1051920897461431\n",
      "c-86 -0.003269114453650273 1.0130899140101566\n",
      "c-87 -0.009498118049573689 1.0370016621735445\n",
      "c-89 -0.027922356367463146 1.103161457946218\n",
      "c-91 -0.047447625755128765 1.1664228906247511\n",
      "c-92 -0.013176300469613614 1.0513716542996172\n",
      "c-93 -0.0594100631543763 1.2018796644855743\n",
      "c-94 -0.0635164985072589 1.2144928799464976\n",
      "c-95 -0.0051186171592672945 1.020642369173464\n",
      "c-96 -0.04791493486536471 1.1675848808680134\n",
      "c-97 -0.005099993351100715 1.0206973429636723\n",
      "c-98 -0.00804158908389384 1.0317101172578524\n",
      "c-99 -0.0007964373287775516 1.0025837044294421\n",
      "kridge_acat_inhibitor 0.0009679132037396543 0.0015015342229869474\n",
      "kridge_acetylcholine_receptor_agonist 0.008540583180062468 0.005383869626801562\n",
      "kridge_acetylcholine_receptor_antagonist 0.013567723379940996 0.0073957423725010294\n",
      "kridge_acetylcholinesterase_inhibitor 0.003239162125586287 0.0027249210622164086\n",
      "kridge_adenosine_receptor_agonist 0.002364049503644969 0.0024841073530671542\n",
      "kridge_adenosine_receptor_antagonist 0.00436131673960246 0.003152678668504583\n",
      "kridge_adrenergic_receptor_agonist 0.012157259885888583 0.006849779153381017\n",
      "kridge_adrenergic_receptor_antagonist 0.016118744915194425 0.007742852691699518\n",
      "kridge_akt_inhibitor 0.0025402215502825305 0.004102606789990749\n",
      "kridge_alk_inhibitor 0.0016470440063497311 0.002596835284020889\n",
      "kridge_androgen_receptor_agonist 0.002087804644672766 0.0022411503549835936\n",
      "kridge_androgen_receptor_antagonist 0.004001731159342345 0.003137669728236868\n",
      "kridge_anesthetic_-_local 0.003582614506051637 0.0029212275948919068\n",
      "kridge_angiogenesis_inhibitor 0.001613614597610993 0.001764509082284198\n",
      "kridge_angiotensin_receptor_antagonist 0.0017039172502974057 0.0021052685752201796\n",
      "kridge_anti-inflammatory 0.0031474415441816077 0.0034687785318766534\n",
      "kridge_antibiotic 0.0015379409447800934 0.0018809233654511158\n",
      "kridge_antioxidant 0.0032364771553456584 0.002964999804162365\n",
      "kridge_antiprotozoal 0.0016288513539389044 0.002129422853792618\n",
      "kridge_antiviral 0.0009885727857597336 0.0016030174960662354\n",
      "kridge_apoptosis_stimulant 0.0018016765188960638 0.0020543057586071547\n",
      "kridge_aromatase_inhibitor 0.0020417884518113077 0.0022653509423003005\n",
      "kridge_atpase_inhibitor 0.00323530236927094 0.0021341873374037413\n",
      "kridge_aurora_kinase_inhibitor 0.003706342567151754 0.006133546955780766\n",
      "kridge_bacterial_30s_ribosomal_subunit_inhibitor 0.002584569297749782 0.0028684072629635037\n",
      "kridge_bacterial_50s_ribosomal_subunit_inhibitor 0.003416854401191358 0.0032325094778326165\n",
      "kridge_bacterial_antifolate 0.001558680202736882 0.0021527856249974843\n",
      "kridge_bacterial_cell_wall_synthesis_inhibitor 0.008684392045428892 0.0051986625327157905\n",
      "kridge_bacterial_dna_gyrase_inhibitor 0.00398850324031064 0.003400186274315486\n",
      "kridge_bacterial_dna_inhibitor 0.005248963908452536 0.0038974566698429824\n",
      "kridge_bcl_inhibitor 0.0009960210388842495 0.001626997383727522\n",
      "kridge_bcr-abl_inhibitor 0.0013093140785511287 0.001710667636665603\n",
      "kridge_benzodiazepine_receptor_agonist 0.0030078484273587352 0.002854607599516925\n",
      "kridge_beta_amyloid_inhibitor 0.0010360467241411788 0.0015116524534499712\n",
      "kridge_bromodomain_inhibitor 0.0020884272631574432 0.0026761989116421553\n",
      "kridge_btk_inhibitor 0.0011912776390855784 0.00223079394290803\n",
      "kridge_calcium_channel_blocker 0.012533839286073056 0.006325035174075205\n",
      "kridge_cannabinoid_receptor_agonist 0.0018466728137510268 0.0022399646934310948\n",
      "kridge_cannabinoid_receptor_antagonist 0.002438573469965061 0.0026470380656918617\n",
      "kridge_carbonic_anhydrase_inhibitor 0.0015758098628883242 0.00189843446750408\n",
      "kridge_casein_kinase_inhibitor 0.0015524950975176646 0.0020039714809702257\n",
      "kridge_cc_chemokine_receptor_antagonist 0.004401901303348937 0.003404236892371685\n",
      "kridge_cdk_inhibitor 0.00839555724725416 0.02557990007962004\n",
      "kridge_chelating_agent 0.002359389588508494 0.0026121261716377797\n",
      "kridge_chk_inhibitor 0.0006836890806613023 0.0016297091214179176\n",
      "kridge_chloride_channel_blocker 0.0019300775900910868 0.00220065838761178\n",
      "kridge_cholesterol_inhibitor 0.0021073894133025284 0.002260595075961982\n",
      "kridge_cholinergic_receptor_antagonist 0.002297637938536491 0.0024230260514878387\n",
      "kridge_corticosteroid_agonist 0.001397986369102134 0.0038920560710391746\n",
      "kridge_cyclooxygenase_inhibitor 0.01945137054910956 0.009840054197559633\n",
      "kridge_cytochrome_p450_inhibitor 0.004449324235056305 0.003584049667448747\n",
      "kridge_dihydrofolate_reductase_inhibitor 0.0012413271090597065 0.0016010737147082532\n",
      "kridge_dipeptidyl_peptidase_inhibitor 0.0010810783892453665 0.0018068447010415874\n",
      "kridge_dna_alkylating_agent 0.0019470846262414695 0.002413528218628801\n",
      "kridge_dna_inhibitor 0.016154233489834694 0.009687676389179408\n",
      "kridge_dopamine_receptor_agonist 0.005511487892196346 0.0036100175360640682\n",
      "kridge_dopamine_receptor_antagonist 0.019074220993063985 0.00950190473201484\n",
      "kridge_egfr_inhibitor 0.013078218414911493 0.021261188721716343\n",
      "kridge_estrogen_receptor_agonist 0.0070680656014442626 0.004641356670047896\n",
      "kridge_estrogen_receptor_antagonist 0.0020723658988268637 0.002420190392925839\n",
      "kridge_faah_inhibitor 0.0015286545699880368 0.0023023888307614243\n",
      "kridge_fatty_acid_receptor_agonist 0.0011120785945991204 0.001875589690912434\n",
      "kridge_fgfr_inhibitor 0.0019778469904908995 0.0026753340463847677\n",
      "kridge_flt3_inhibitor 0.010173406203159666 0.01167534158038805\n",
      "kridge_fungal_squalene_epoxidase_inhibitor 0.0010711262950586203 0.0018307847326806692\n",
      "kridge_gaba_receptor_agonist 0.004585325725910164 0.0034696740454494917\n",
      "kridge_gaba_receptor_antagonist 0.0072855641636316625 0.004537059183642012\n",
      "kridge_gamma_secretase_inhibitor 0.0023774492043754296 0.0038219230289061015\n",
      "kridge_glucocorticoid_receptor_agonist 0.010598938720169876 0.024804696380069848\n",
      "kridge_glutamate_receptor_agonist 0.0033111985585264785 0.002756019979282823\n",
      "kridge_glutamate_receptor_antagonist 0.016475000861256764 0.007944963207561979\n",
      "kridge_gsk_inhibitor 0.0020482302700472878 0.00321164428878102\n",
      "kridge_hcv_inhibitor 0.003141292941585481 0.002749189800795852\n",
      "kridge_hdac_inhibitor 0.002269844135536301 0.0027738879914294256\n",
      "kridge_histamine_receptor_agonist 0.0025962207433534892 0.0026787834200411326\n",
      "kridge_histamine_receptor_antagonist 0.01090986789387421 0.005757828123822318\n",
      "kridge_histone_lysine_demethylase_inhibitor 0.0009530556082658684 0.002088341130229991\n",
      "kridge_histone_lysine_methyltransferase_inhibitor 0.0014385191871392475 0.0023751217555259357\n",
      "kridge_hiv_inhibitor 0.003215968211994737 0.0032185548268562265\n",
      "kridge_hmgcr_inhibitor 0.011244941380676828 0.016907232120236926\n",
      "kridge_hsp_inhibitor 0.0014035265930553873 0.0029961772653448434\n",
      "kridge_igf-1_inhibitor 0.0015898634749424393 0.0024539861046299133\n",
      "kridge_ikk_inhibitor 0.0009474136123643491 0.0020967778221047035\n",
      "kridge_imidazoline_receptor_agonist 0.0012979920328729266 0.0017341220210970957\n",
      "kridge_immunosuppressant 0.0028930118226201547 0.004558343813877981\n",
      "kridge_insulin_secretagogue 0.0012807851669190738 0.0018394918245603828\n",
      "kridge_insulin_sensitizer 0.0020726383346087564 0.0029920801717151623\n",
      "kridge_integrin_inhibitor 0.001781237360815002 0.002011874119961351\n",
      "kridge_jak_inhibitor 0.0031212517703152246 0.004525452178694793\n",
      "kridge_kit_inhibitor 0.010307255991661901 0.011702835580385757\n",
      "kridge_leukotriene_receptor_antagonist 0.002722520498662371 0.002753582201603988\n",
      "kridge_lipoxygenase_inhibitor 0.002495935363499913 0.002615261950478872\n",
      "kridge_mdm_inhibitor 0.0011106941931158035 0.0025410710008581744\n",
      "kridge_mek_inhibitor 0.0019468020364068414 0.004495704001491777\n",
      "kridge_membrane_integrity_inhibitor 0.00318221736104009 0.002832411209222798\n",
      "kridge_mineralocorticoid_receptor_antagonist 0.0010786779346760858 0.0016423176916392109\n",
      "kridge_monoamine_oxidase_inhibitor 0.003857523195654956 0.0031805505861727855\n",
      "kridge_mtor_inhibitor 0.003499411532597953 0.008652792272060781\n",
      "kridge_mucolytic_agent 0.002109907139433636 0.0023590899886147456\n",
      "kridge_neuropeptide_receptor_antagonist 0.001538305020514117 0.0022628598566543506\n",
      "kridge_nfkb_inhibitor 0.008074365644692336 0.017144720161900554\n",
      "kridge_nitric_oxide_donor 0.0011458119990774016 0.001782665165693352\n",
      "kridge_nitric_oxide_synthase_inhibitor 0.0010817622508018283 0.0016901941959579924\n",
      "kridge_opioid_receptor_agonist 0.0026668378679498396 0.0026901052328494584\n",
      "kridge_opioid_receptor_antagonist 0.003977053387495445 0.003529342886044707\n",
      "kridge_orexin_receptor_antagonist 0.0016582913406513054 0.0019418635929309422\n",
      "kridge_p38_mapk_inhibitor 0.0024590891582119525 0.004090415900944829\n",
      "kridge_p-glycoprotein_inhibitor 0.0010434171375599698 0.0016118514874372764\n",
      "kridge_parp_inhibitor 0.0026481964498243954 0.0035216620596950168\n",
      "kridge_pdgfr_inhibitor 0.01117058929708667 0.011935871388031443\n",
      "kridge_phosphodiesterase_inhibitor 0.01173859773170839 0.0064562323611403235\n",
      "kridge_phospholipase_inhibitor 0.0010821888241449855 0.0016275125529300364\n",
      "kridge_pi3k_inhibitor 0.005140372221558688 0.006026570443458969\n",
      "kridge_pkc_inhibitor 0.001262311958217216 0.001788028553268839\n",
      "kridge_potassium_channel_activator 0.0023409310002964175 0.0025427339539890446\n",
      "kridge_potassium_channel_antagonist 0.00441057925777725 0.0032826790411532945\n",
      "kridge_ppar_receptor_agonist 0.004473062960579132 0.004954824861086921\n",
      "kridge_ppar_receptor_antagonist 0.0013484967518930262 0.0017293689076487715\n",
      "kridge_progesterone_receptor_agonist 0.005373240681970319 0.004511837169018931\n",
      "kridge_prostaglandin_inhibitor 0.0016089487616295036 0.001997463723584315\n",
      "kridge_prostanoid_receptor_antagonist 0.0037390084892911595 0.003056764434799052\n",
      "kridge_proteasome_inhibitor 0.0037468214296390396 0.019524829964047585\n",
      "kridge_protein_kinase_inhibitor 0.0019208420151808612 0.0021870874094146587\n",
      "kridge_protein_synthesis_inhibitor 0.0031632129683042136 0.0016688537424742853\n",
      "kridge_radiopaque_medium 0.002474017850586554 0.002405294994992441\n",
      "kridge_raf_inhibitor 0.005860793457781275 0.01985773200561825\n",
      "kridge_retinoid_receptor_agonist 0.0027455608190394033 0.004305330833084922\n",
      "kridge_rho_associated_kinase_inhibitor 0.0013911185134801705 0.0023808021690726\n",
      "kridge_ribonucleoside_reductase_inhibitor 0.0011767194585221897 0.001717308282530809\n",
      "kridge_rna_polymerase_inhibitor 0.000875146205366125 0.0014064724968496077\n",
      "kridge_serotonin_receptor_agonist 0.010558903035282764 0.005642301419561747\n",
      "kridge_serotonin_receptor_antagonist 0.01797646087141621 0.009757143747173016\n",
      "kridge_serotonin_reuptake_inhibitor 0.0018949889910443525 0.002300320644086614\n",
      "kridge_sigma_receptor_agonist 0.0015917777337558167 0.0018365657505465202\n",
      "kridge_sigma_receptor_antagonist 0.001553263353997244 0.0021656678623405107\n",
      "kridge_smoothened_receptor_antagonist 0.0009601543565825023 0.001537436365169477\n",
      "kridge_sodium_channel_inhibitor 0.011743873320076462 0.006469108943586249\n",
      "kridge_sphingosine_receptor_agonist 0.0011157060631223146 0.0016582711855291704\n",
      "kridge_src_inhibitor 0.0025138056466515123 0.00312933124377052\n",
      "kridge_tachykinin_antagonist 0.002706094961469858 0.002666931752171585\n",
      "kridge_tgf-beta_receptor_inhibitor 0.0009840930686432798 0.0030222779856226172\n",
      "kridge_thymidylate_synthase_inhibitor 0.0014251988946935733 0.0020975473070655054\n",
      "kridge_tlr_agonist 0.0013616395012237448 0.0017215798972667028\n",
      "kridge_tnf_inhibitor 0.001497348904069569 0.002012020942910625\n",
      "kridge_topoisomerase_inhibitor 0.002409611885255165 0.003316283000061553\n",
      "kridge_trpv_agonist 0.0009715069342646681 0.0018651223331342605\n",
      "kridge_trpv_antagonist 0.0020760760612974624 0.0024557434696060565\n",
      "kridge_tubulin_inhibitor 0.01087990212306831 0.028103410965640525\n",
      "kridge_tyrosine_kinase_inhibitor 0.0030453662857099326 0.0030939483366414504\n",
      "kridge_vegfr_inhibitor 0.007230060288047321 0.007157575319779848\n",
      "kridge_vitamin_b 0.0010746405288542605 0.0016682995141242862\n",
      "kridge_vitamin_d_receptor_agonist 0.0015108640846040083 0.0029076890312433605\n",
      "kridge_wnt_inhibitor 0.0012927827590492596 0.0017552977311425628\n",
      "kmeans_g_0 24.499705580367984 14.575528140338626\n",
      "kmeans_g_1 24.27857214739505 14.38895066864198\n",
      "kmeans_g_2 73.8816836101076 5.3898435082926675\n",
      "kmeans_g_3 66.36414050269363 4.633260420256776\n",
      "kmeans_g_4 64.15049063538831 4.565402688389629\n",
      "kmeans_g_5 43.14270500863043 6.00641122962899\n",
      "kmeans_g_6 29.488787156837788 10.718923803996342\n",
      "kmeans_c_0 6.724326219046678 7.43380725980398\n",
      "kmeans_c_1 5.956169520062871 8.33444240368168\n",
      "kmeans_c_2 30.788935100514575 5.180802436044504\n",
      "kmeans_c_3 44.375671075226634 8.188354569600813\n",
      "kmeans_c_4 6.467324242776625 8.816612344226362\n",
      "kmeans_c_5 11.813542348377695 4.868516617621923\n",
      "kmeans_c_6 20.484269929345274 3.1820724785195726\n",
      "cp_time_24 0.3264989976307636 0.4689321935811938\n",
      "cp_time_48 0.3463641334062329 0.47581090834067913\n",
      "cp_time_72 0.3271368689630035 0.4691677076782738\n",
      "cp_dose_D1 0.5101148168398032 0.49989768001091706\n",
      "cp_dose_D2 0.4898851831601968 0.49989768001091706\n"
     ]
    }
   ],
   "source": [
    "# Standarize\n",
    "for colname in feature_cols_ini:\n",
    "    valor_fold = folds[colname].values\n",
    "    valor_tst = test_noctl[colname].values\n",
    "    mean_v = np.mean(valor_fold)\n",
    "    std_v = np.std(valor_fold)\n",
    "    if std_v==0:\n",
    "        std_v=1e-5\n",
    "    folds[colname] = (valor_fold-mean_v)/std_v\n",
    "    test_noctl[colname] = (valor_tst-mean_v)/std_v\n",
    "    print(colname, mean_v, std_v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.112308,
     "end_time": "2020-12-06T00:02:03.419633",
     "exception": false,
     "start_time": "2020-12-06T00:02:03.307325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:30Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:03.622642Z",
     "iopub.status.busy": "2020-12-06T00:02:03.621633Z",
     "iopub.status.idle": "2020-12-06T00:02:03.626039Z",
     "shell.execute_reply": "2020-12-06T00:02:03.626577Z"
    },
    "papermill": {
     "duration": 0.093119,
     "end_time": "2020-12-06T00:02:03.626718",
     "exception": false,
     "start_time": "2020-12-06T00:02:03.533599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def run_training(fold, seed, display=2):\n",
    "#     seed_everything(seed)\n",
    "#     train = folds.copy()\n",
    "#     test_ = test_noctl.copy()\n",
    "    \n",
    "#     trn_idx = train[train['kfold'] != fold].index\n",
    "#     val_idx = train[train['kfold'] == fold].index\n",
    "    \n",
    "#     train_df = train[train['kfold'] != fold].reset_index(drop=True)\n",
    "#     valid_df = train[train['kfold'] == fold].reset_index(drop=True)\n",
    "    \n",
    "#     x_train, y_train  = train_df[feature_cols].values, train_df[target_cols].values\n",
    "#     x_valid, y_valid =  valid_df[feature_cols].values, valid_df[target_cols].values\n",
    "    \n",
    "#     train_dataset = MoADataset(x_train, y_train)\n",
    "#     valid_dataset = MoADataset(x_valid, y_valid)\n",
    "#     trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#     validloader = torch.utils.data.DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     model = Model(\n",
    "#         num_features=num_features,\n",
    "#         num_targets=num_targets,\n",
    "#         hidden_size=hidden_size,\n",
    "#     )\n",
    "#     model.to(DEVICE)\n",
    "    \n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "#     scheduler = optim.lr_scheduler.OneCycleLR(optimizer=optimizer, pct_start=0.1, div_factor=1e3, \n",
    "#                                               max_lr=1e-2, epochs=EPOCHS, steps_per_epoch=len(trainloader))\n",
    "# #     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,patience=3)\n",
    "    \n",
    "# #     loss_fn = nn.BCEWithLogitsLoss()\n",
    "#     loss_val = nn.BCEWithLogitsLoss()\n",
    "#     loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "    \n",
    "#     early_stopping_steps = EARLY_STOPPING_STEPS\n",
    "#     early_step = 0\n",
    "    \n",
    "#     oof = np.zeros((len(train), num_targets))\n",
    "#     best_loss = np.inf\n",
    "#     best_epoch = 0\n",
    "#     res = []\n",
    "#     for epoch in range(EPOCHS):\n",
    "        \n",
    "#         train_loss, mloss_train = train_fn(model, optimizer, scheduler, loss_tr, trainloader, DEVICE)\n",
    "                \n",
    "# #         print(f\"FOLD: {fold}, EPOCH: {epoch}, train_loss: {train_loss}\")\n",
    "#         valid_loss, mloss_valid, y_true, valid_preds = valid_fn(model, loss_val, validloader, DEVICE)\n",
    "    \n",
    "#         if mloss_valid < best_loss:\n",
    "#             best_loss = mloss_valid\n",
    "#             best_epoch = epoch\n",
    "#             oof[val_idx] = valid_preds\n",
    "#             torch.save(model.state_dict(), output_dir / f'ann_model_seed{seed}_fold__{fold}.pth')\n",
    "            \n",
    "#         elif(EARLY_STOP == True):\n",
    "#             early_step += 1\n",
    "#             if (early_step >= early_stopping_steps):\n",
    "#                 break\n",
    "#         if scheduler.__class__ !=  torch.optim.lr_scheduler.ReduceLROnPlateau:\n",
    "#             lr = scheduler.get_last_lr()\n",
    "#         else:\n",
    "#             lr = [0.0]\n",
    "        \n",
    "#         # Save results\n",
    "#         # ------------\n",
    "#         res.append(dict({'epoch':epoch, 'lr':lr[0], 'trn_loss':mloss_train, 'val_loss':mloss_valid, 'best_epoch':best_epoch, 'best_loss':best_loss}))\n",
    "        \n",
    "#         res_df = pd.DataFrame(res)\n",
    "#         res_df.to_csv(output_dir / f'res_seed{seed}_fold{fold}_.csv')\n",
    "        \n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=1 )\n",
    "#         min_val = res_df[['trn_loss','val_loss']].min().min()\n",
    "#         ax.plot(res_df['trn_loss'])\n",
    "#         ax.plot(res_df['val_loss'])\n",
    "#         plt.ylim((min_val,0.020))\n",
    "#         plt.title(f\"logloss in fold={fold} min={res_df['val_loss'].min()}\")\n",
    "#         fig.savefig(output_dir / f'loss_seed{seed}_fold{fold}_.png')\n",
    "#         plt.close(fig)\n",
    "        \n",
    "#         fig, ax = plt.subplots(nrows=1, ncols=1 )\n",
    "#         ax.plot(res_df['lr'])\n",
    "#         plt.title(f\"lr in fold={fold}\")\n",
    "#         fig.savefig(output_dir / f'lr_seed{seed}_fold{fold}_.png')\n",
    "#         plt.close(fig)\n",
    "        \n",
    "#         if display==2:\n",
    "#             print(f\"SEED:{seed} FOLD:{fold}, EPOCH:{epoch:2d}, lr:{lr[0]:.9f} trn_loss:{mloss_train:.6f}, val_loss:{mloss_valid:.6f}, best_epoch:{best_epoch}, best_loss:{best_loss:.6f}\")\n",
    "\n",
    "#     if display==1:\n",
    "#         print(f\"SEED:{seed} FOLD:{fold}, EPOCH:{epoch:2d}, lr:{lr[0]:.9f} trn_loss:{mloss_train:.6f}, val_loss:{mloss_valid:.6f}, best_epoch:{best_epoch}, best_loss:{best_loss:.6f}\")\n",
    "        \n",
    "    \n",
    "#     #--------------------- PREDICTION---------------------\n",
    "#     x_test = test_[feature_cols].values\n",
    "#     testdataset = TestDataset(x_test)\n",
    "#     testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "#     model = Model(\n",
    "#         num_features=num_features,\n",
    "#         num_targets=num_targets,\n",
    "#         hidden_size=hidden_size,\n",
    "#     )\n",
    "    \n",
    "#     model.load_state_dict(torch.load(output_dir / f'ann_model_seed{seed}_fold__{fold}.pth'))\n",
    "#     model.to(DEVICE)\n",
    "    \n",
    "#     predictions_tst = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "#     predictions_tst = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "#     return oof, predictions_tst, best_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:30.003Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:03.829096Z",
     "iopub.status.busy": "2020-12-06T00:02:03.828387Z",
     "iopub.status.idle": "2020-12-06T00:02:03.832968Z",
     "shell.execute_reply": "2020-12-06T00:02:03.833908Z"
    },
    "papermill": {
     "duration": 0.111101,
     "end_time": "2020-12-06T00:02:03.834042",
     "exception": false,
     "start_time": "2020-12-06T00:02:03.722941",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_ANN(fold, seedfold, seedrun, display=2):\n",
    "    seed_everything(seedrun)\n",
    "    test_ = test_noctl.copy()\n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "        hidden_size=hidden_size,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{directorio}/ann_model_seedfold{seedfold}_seedrun{seedrun}_fold__{fold}.pth'))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions_tst = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions_tst = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return predictions_tst "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:30.005Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:04.014536Z",
     "iopub.status.busy": "2020-12-06T00:02:04.013855Z",
     "iopub.status.idle": "2020-12-06T00:02:04.018333Z",
     "shell.execute_reply": "2020-12-06T00:02:04.017831Z"
    },
    "papermill": {
     "duration": 0.084259,
     "end_time": "2020-12-06T00:02:04.018427",
     "exception": false,
     "start_time": "2020-12-06T00:02:03.934168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed_fold, seed_run, display=2):\n",
    "    predictions_kfold = np.zeros((len(test_noctl), len(target_cols)))\n",
    "    for fold in range(NFOLDS):\n",
    "        pred_ = run_inference_ANN(fold, seed_fold, seed_run, display)\n",
    "        predictions_kfold += pred_ / NFOLDS\n",
    "    return predictions_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:30.007Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:04.167996Z",
     "iopub.status.busy": "2020-12-06T00:02:04.167438Z",
     "iopub.status.idle": "2020-12-06T00:02:04.173429Z",
     "shell.execute_reply": "2020-12-06T00:02:04.172960Z"
    },
    "papermill": {
     "duration": 0.082424,
     "end_time": "2020-12-06T00:02:04.173525",
     "exception": false,
     "start_time": "2020-12-06T00:02:04.091101",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.072571,
     "end_time": "2020-12-06T00:02:04.318896",
     "exception": false,
     "start_time": "2020-12-06T00:02:04.246325",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "> ### Train ANN models 29 y 28 de nov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:04.481454Z",
     "iopub.status.busy": "2020-12-06T00:02:04.477719Z",
     "iopub.status.idle": "2020-12-06T00:02:35.313905Z",
     "shell.execute_reply": "2020-12-06T00:02:35.313391Z"
    },
    "papermill": {
     "duration": 30.921311,
     "end_time": "2020-12-06T00:02:35.314019",
     "exception": false,
     "start_time": "2020-12-06T00:02:04.392708",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3993e335724944449fc3171dfbaa1712",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directorio = '../input/annrandom' #'../input/annstratkmean7' #'../input/annrandomnosmooth' #'../input/annstratnosmooth' #\n",
    "folds_cp = folds.copy()\n",
    "feature_cols = feature_cols_ini.copy()\n",
    "NFOLDS = CFG.num_folds\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "hidden_size = 256\n",
    "res = []\n",
    "drop1_feat = 0.05\n",
    "drop2_feat = 0.05\n",
    "drop3_feat = 0.25\n",
    "\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "losses_list = []\n",
    "# SEED = [[0,1],[1,1],[2,1],[3,1]]\n",
    "SEED = np.arange(16)\n",
    "for seed in tqdm(SEED):\n",
    "    seed_fold = seed // 4\n",
    "    seed_run = seed % 4\n",
    "    predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "    predictions += predictions_\n",
    "\n",
    "# FINAL CV LOGLOSS\n",
    "for col in target_cols:\n",
    "    test_noctl[col] = 0.0\n",
    "test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:35.473607Z",
     "iopub.status.busy": "2020-12-06T00:02:35.472158Z",
     "iopub.status.idle": "2020-12-06T00:02:37.833344Z",
     "shell.execute_reply": "2020-12-06T00:02:37.832774Z"
    },
    "papermill": {
     "duration": 2.445355,
     "end_time": "2020-12-06T00:02:37.833461",
     "exception": false,
     "start_time": "2020-12-06T00:02:35.388106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANN_1dic\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.001952</td>\n",
       "      <td>0.003056</td>\n",
       "      <td>0.010624</td>\n",
       "      <td>0.019323</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.002078</td>\n",
       "      <td>0.008484</td>\n",
       "      <td>0.000576</td>\n",
       "      <td>0.009351</td>\n",
       "      <td>0.010256</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.001248</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.007842</td>\n",
       "      <td>0.008646</td>\n",
       "      <td>0.002889</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.004180</td>\n",
       "      <td>0.000809</td>\n",
       "      <td>0.002408</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001159</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.003318</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.005662</td>\n",
       "      <td>0.005636</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.000659</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.005596</td>\n",
       "      <td>0.000596</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.005372</td>\n",
       "      <td>0.010667</td>\n",
       "      <td>0.001946</td>\n",
       "      <td>0.006122</td>\n",
       "      <td>0.008474</td>\n",
       "      <td>0.007262</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.000831</td>\n",
       "      <td>0.003551</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.004378</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.017088</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.002342</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.006628</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.000439</td>\n",
       "      <td>0.004610</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.003694</td>\n",
       "      <td>0.007166</td>\n",
       "      <td>0.000846</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.048415</td>\n",
       "      <td>0.005257</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.003756</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.003770</td>\n",
       "      <td>0.013462</td>\n",
       "      <td>0.007280</td>\n",
       "      <td>0.011230</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.023284</td>\n",
       "      <td>0.001801</td>\n",
       "      <td>0.005585</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.000464</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.002579</td>\n",
       "      <td>0.021474</td>\n",
       "      <td>0.021410</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.001968</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.007545</td>\n",
       "      <td>0.024480</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.006838</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.007557</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.005580</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.000779</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.002967</td>\n",
       "      <td>0.002203</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.005826</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.004561</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.000633</td>\n",
       "      <td>0.006477</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.002024</td>\n",
       "      <td>0.004833</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.004893</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.007152</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.001405</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.002700</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.017764</td>\n",
       "      <td>0.002714</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001391</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.010842</td>\n",
       "      <td>0.001684</td>\n",
       "      <td>0.002669</td>\n",
       "      <td>0.017354</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.005105</td>\n",
       "      <td>0.009152</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.004401</td>\n",
       "      <td>0.001089</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000940</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.010523</td>\n",
       "      <td>0.004820</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>0.003681</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.024195</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.001469</td>\n",
       "      <td>0.000347</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.001216</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.004060</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.003016</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000815</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001937</td>\n",
       "      <td>0.017152</td>\n",
       "      <td>0.001353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.000982</td>\n",
       "      <td>0.002453</td>\n",
       "      <td>0.003012</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.003638</td>\n",
       "      <td>0.007281</td>\n",
       "      <td>0.007204</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.005166</td>\n",
       "      <td>0.002789</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.005871</td>\n",
       "      <td>0.000945</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.000869</td>\n",
       "      <td>0.001490</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.003112</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000728</td>\n",
       "      <td>0.001441</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000661</td>\n",
       "      <td>0.001959</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000936</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.003800</td>\n",
       "      <td>0.003445</td>\n",
       "      <td>0.002121</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.000411</td>\n",
       "      <td>0.002364</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.004954</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.004989</td>\n",
       "      <td>0.015709</td>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.004497</td>\n",
       "      <td>0.004327</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.003282</td>\n",
       "      <td>0.000923</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.000964</td>\n",
       "      <td>0.004178</td>\n",
       "      <td>0.001686</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.003149</td>\n",
       "      <td>0.000794</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.005624</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000630</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.000890</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.000901</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.003633</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.006085</td>\n",
       "      <td>0.000545</td>\n",
       "      <td>0.002604</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.003658</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.001836</td>\n",
       "      <td>0.003731</td>\n",
       "      <td>0.002719</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.001170</td>\n",
       "      <td>0.005067</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.004870</td>\n",
       "      <td>0.003908</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.002022</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.003728</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.006655</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.003568</td>\n",
       "      <td>0.034316</td>\n",
       "      <td>0.007652</td>\n",
       "      <td>0.005062</td>\n",
       "      <td>0.001664</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.000753</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.001725</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.001381</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.001639</td>\n",
       "      <td>0.001186</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.000645</td>\n",
       "      <td>0.000609</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.001357</td>\n",
       "      <td>0.000784</td>\n",
       "      <td>0.003850</td>\n",
       "      <td>0.005033</td>\n",
       "      <td>0.002633</td>\n",
       "      <td>0.001905</td>\n",
       "      <td>0.019721</td>\n",
       "      <td>0.000686</td>\n",
       "      <td>0.012107</td>\n",
       "      <td>0.003760</td>\n",
       "      <td>0.002891</td>\n",
       "      <td>0.001715</td>\n",
       "      <td>0.062141</td>\n",
       "      <td>0.015617</td>\n",
       "      <td>0.001812</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>0.001514</td>\n",
       "      <td>0.002731</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.002592</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.000332</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.031636</td>\n",
       "      <td>0.001611</td>\n",
       "      <td>0.002826</td>\n",
       "      <td>0.007226</td>\n",
       "      <td>0.000977</td>\n",
       "      <td>0.000540</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.003350</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>0.011445</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.002800</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.000670</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.001097</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.008366</td>\n",
       "      <td>0.001368</td>\n",
       "      <td>0.001116</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>0.000292</td>\n",
       "      <td>0.007264</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.002213</td>\n",
       "      <td>0.003306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.001407</td>\n",
       "      <td>0.002114</td>\n",
       "      <td>0.012372</td>\n",
       "      <td>0.016655</td>\n",
       "      <td>0.004947</td>\n",
       "      <td>0.004310</td>\n",
       "      <td>0.003902</td>\n",
       "      <td>0.000672</td>\n",
       "      <td>0.022006</td>\n",
       "      <td>0.038018</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.004235</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.002532</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.004966</td>\n",
       "      <td>0.004777</td>\n",
       "      <td>0.002271</td>\n",
       "      <td>0.003049</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.001313</td>\n",
       "      <td>0.003773</td>\n",
       "      <td>0.002277</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.009067</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.002663</td>\n",
       "      <td>0.002317</td>\n",
       "      <td>0.003209</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.014111</td>\n",
       "      <td>0.008289</td>\n",
       "      <td>0.007538</td>\n",
       "      <td>0.001247</td>\n",
       "      <td>0.001069</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.003542</td>\n",
       "      <td>0.002335</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.001524</td>\n",
       "      <td>0.001763</td>\n",
       "      <td>0.036773</td>\n",
       "      <td>0.002300</td>\n",
       "      <td>0.005779</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002852</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.002065</td>\n",
       "      <td>0.007399</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.002893</td>\n",
       "      <td>0.000843</td>\n",
       "      <td>0.005719</td>\n",
       "      <td>0.004632</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.001281</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.013453</td>\n",
       "      <td>0.004872</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.003872</td>\n",
       "      <td>0.020781</td>\n",
       "      <td>0.014512</td>\n",
       "      <td>0.050883</td>\n",
       "      <td>0.009712</td>\n",
       "      <td>0.001099</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.005044</td>\n",
       "      <td>0.003914</td>\n",
       "      <td>0.001630</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.002478</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.001692</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.000718</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>0.013400</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.001200</td>\n",
       "      <td>0.003953</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.035045</td>\n",
       "      <td>0.001751</td>\n",
       "      <td>0.002229</td>\n",
       "      <td>0.006921</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.001992</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.002777</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001221</td>\n",
       "      <td>0.001413</td>\n",
       "      <td>0.002288</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.002730</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.005411</td>\n",
       "      <td>0.003185</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.008032</td>\n",
       "      <td>0.001218</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.003470</td>\n",
       "      <td>0.004270</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.002983</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.001526</td>\n",
       "      <td>0.000808</td>\n",
       "      <td>0.006882</td>\n",
       "      <td>0.009011</td>\n",
       "      <td>0.003900</td>\n",
       "      <td>0.006757</td>\n",
       "      <td>0.001477</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001197</td>\n",
       "      <td>0.006758</td>\n",
       "      <td>0.002499</td>\n",
       "      <td>0.005329</td>\n",
       "      <td>0.002835</td>\n",
       "      <td>0.004064</td>\n",
       "      <td>0.007908</td>\n",
       "      <td>0.001148</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.002154</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.003569</td>\n",
       "      <td>0.000878</td>\n",
       "      <td>0.002523</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.008218</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.001726</td>\n",
       "      <td>0.000610</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.001721</td>\n",
       "      <td>0.001719</td>\n",
       "      <td>0.001574</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.033095</td>\n",
       "      <td>0.004247</td>\n",
       "      <td>0.003331</td>\n",
       "      <td>0.004680</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.009061</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.004551</td>\n",
       "      <td>0.001321</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>0.002680</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.001919</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.007775</td>\n",
       "      <td>0.003948</td>\n",
       "      <td>0.001090</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.003573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001982</td>\n",
       "      <td>0.001953</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.010806</td>\n",
       "      <td>0.010668</td>\n",
       "      <td>0.003265</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.008886</td>\n",
       "      <td>0.016536</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.000680</td>\n",
       "      <td>0.001442</td>\n",
       "      <td>0.001742</td>\n",
       "      <td>0.001945</td>\n",
       "      <td>0.003516</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.005007</td>\n",
       "      <td>0.005130</td>\n",
       "      <td>0.001067</td>\n",
       "      <td>0.003841</td>\n",
       "      <td>0.001354</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.007314</td>\n",
       "      <td>0.003499</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.004572</td>\n",
       "      <td>0.003360</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.008206</td>\n",
       "      <td>0.008327</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.011414</td>\n",
       "      <td>0.008019</td>\n",
       "      <td>0.008563</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.003033</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.001212</td>\n",
       "      <td>0.005894</td>\n",
       "      <td>0.002617</td>\n",
       "      <td>0.002878</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.005434</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.004264</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.005746</td>\n",
       "      <td>0.002924</td>\n",
       "      <td>0.002568</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.012123</td>\n",
       "      <td>0.006268</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.005282</td>\n",
       "      <td>0.032889</td>\n",
       "      <td>0.004241</td>\n",
       "      <td>0.004579</td>\n",
       "      <td>0.001140</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.010795</td>\n",
       "      <td>0.002381</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.001016</td>\n",
       "      <td>0.002507</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001457</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.005665</td>\n",
       "      <td>0.004663</td>\n",
       "      <td>0.000594</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.002750</td>\n",
       "      <td>0.003626</td>\n",
       "      <td>0.016677</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>0.008318</td>\n",
       "      <td>0.000681</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.004294</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.003442</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.003104</td>\n",
       "      <td>0.003309</td>\n",
       "      <td>0.004686</td>\n",
       "      <td>0.002480</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.001207</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.002842</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.006635</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.004352</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.002775</td>\n",
       "      <td>0.003327</td>\n",
       "      <td>0.001673</td>\n",
       "      <td>0.006007</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.001881</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.004755</td>\n",
       "      <td>0.003669</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001403</td>\n",
       "      <td>0.010520</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.002463</td>\n",
       "      <td>0.001705</td>\n",
       "      <td>0.006297</td>\n",
       "      <td>0.002895</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.002143</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>0.003661</td>\n",
       "      <td>0.003498</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.002330</td>\n",
       "      <td>0.001447</td>\n",
       "      <td>0.005499</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.004840</td>\n",
       "      <td>0.001983</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>0.001115</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.004353</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.008453</td>\n",
       "      <td>0.007754</td>\n",
       "      <td>0.002070</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.001388</td>\n",
       "      <td>0.002598</td>\n",
       "      <td>0.014903</td>\n",
       "      <td>0.002363</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.001743</td>\n",
       "      <td>0.001765</td>\n",
       "      <td>0.003861</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001322</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.002431</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.004136</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.003719</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.002116</td>\n",
       "      <td>0.000526</td>\n",
       "      <td>0.002297</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001293                0.001952   \n",
       "1  id_001897cda                     0.000600                0.000982   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.001430                0.001407   \n",
       "4  id_0027f1083                     0.001982                0.001953   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.003056                        0.010624   \n",
       "1        0.002453                        0.003012   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.002114                        0.012372   \n",
       "4        0.002447                        0.010806   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.019323                        0.005754   \n",
       "1                           0.001086                        0.001342   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.016655                        0.004947   \n",
       "4                           0.010668                        0.003265   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002078                       0.008484   \n",
       "1                    0.003638                       0.007281   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.004310                       0.003902   \n",
       "4                    0.005332                       0.002125   \n",
       "\n",
       "   adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                    0.000576                     0.009351   \n",
       "1                    0.007204                     0.002598   \n",
       "2                    0.000000                     0.000000   \n",
       "3                    0.000672                     0.022006   \n",
       "4                    0.001034                     0.008886   \n",
       "\n",
       "   adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                        0.010256       0.000560   \n",
       "1                        0.005166       0.002789   \n",
       "2                        0.000000       0.000000   \n",
       "3                        0.038018       0.002439   \n",
       "4                        0.016536       0.002629   \n",
       "\n",
       "   aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  analgesic  \\\n",
       "0                          0.001248       0.000601        0.001181   0.001198   \n",
       "1                          0.000592       0.005871        0.000945   0.000797   \n",
       "2                          0.000000       0.000000        0.000000   0.000000   \n",
       "3                          0.000812       0.004235        0.001726   0.002532   \n",
       "4                          0.000680       0.001442        0.001742   0.001945   \n",
       "\n",
       "   androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0                   0.002772                      0.007842   \n",
       "1                   0.000869                      0.001490   \n",
       "2                   0.000000                      0.000000   \n",
       "3                   0.002995                      0.004966   \n",
       "4                   0.003516                      0.004058   \n",
       "\n",
       "   anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0            0.008646                0.002889   \n",
       "1            0.001269                0.003112   \n",
       "2            0.000000                0.000000   \n",
       "3            0.004777                0.002271   \n",
       "4            0.002043                0.002003   \n",
       "\n",
       "   angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                         0.002346           0.004180        0.000809   \n",
       "1                         0.004863           0.002779        0.000642   \n",
       "2                         0.000000           0.000000        0.000000   \n",
       "3                         0.003049           0.001858        0.001313   \n",
       "4                         0.005007           0.005130        0.001067   \n",
       "\n",
       "   antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0    0.002408        0.000691    0.001159       0.001085      0.001526   \n",
       "1    0.001386        0.000728    0.001441       0.000911      0.000661   \n",
       "2    0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3    0.003773        0.002277    0.001571       0.001835      0.002996   \n",
       "4    0.003841        0.001354    0.001315       0.001320      0.001598   \n",
       "\n",
       "   antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0     0.003318       0.002066   0.001038             0.005662   \n",
       "1     0.001959       0.001709   0.000936             0.002872   \n",
       "2     0.000000       0.000000   0.000000             0.000000   \n",
       "3     0.009067       0.003298   0.002663             0.002317   \n",
       "4     0.007314       0.003499   0.002384             0.004572   \n",
       "\n",
       "   aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0             0.005636              0.000492   \n",
       "1             0.001208              0.001603   \n",
       "2             0.000000              0.000000   \n",
       "3             0.003209              0.001484   \n",
       "4             0.003360              0.000941   \n",
       "\n",
       "   atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                    0.000659                0.001413   \n",
       "1                                    0.000692                0.000495   \n",
       "2                                    0.000000                0.000000   \n",
       "3                                    0.000963                0.000874   \n",
       "4                                    0.000789                0.000942   \n",
       "\n",
       "   atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0          0.005596              0.000596                 0.000802   \n",
       "1          0.002719              0.003800                 0.003445   \n",
       "2          0.000000              0.000000                 0.000000   \n",
       "3          0.002707              0.001094                 0.001079   \n",
       "4          0.002200              0.001002                 0.000893   \n",
       "\n",
       "   autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0             0.000697                                   0.005372   \n",
       "1             0.002121                                   0.001352   \n",
       "2             0.000000                                   0.000000   \n",
       "3             0.001297                                   0.004623   \n",
       "4             0.001143                                   0.008206   \n",
       "\n",
       "   bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                   0.010667              0.001946   \n",
       "1                                   0.001164              0.000411   \n",
       "2                                   0.000000              0.000000   \n",
       "3                                   0.007712              0.003841   \n",
       "4                                   0.008327              0.002908   \n",
       "\n",
       "   bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                 0.006122                        0.008474   \n",
       "1                                 0.002364                        0.000914   \n",
       "2                                 0.000000                        0.000000   \n",
       "3                                 0.014111                        0.008289   \n",
       "4                                 0.011414                        0.008019   \n",
       "\n",
       "   bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                 0.007262                                0.001139   \n",
       "1                 0.002018                                0.000715   \n",
       "2                 0.000000                                0.000000   \n",
       "3                 0.007538                                0.001247   \n",
       "4                 0.008563                                0.001021   \n",
       "\n",
       "   bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0       0.003841           0.000831                         0.003551   \n",
       "1       0.001252           0.001507                         0.004954   \n",
       "2       0.000000           0.000000                         0.000000   \n",
       "3       0.001069           0.001107                         0.003542   \n",
       "4       0.003033           0.001026                         0.003758   \n",
       "\n",
       "   beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                0.001926               0.004378       0.000727   \n",
       "1                0.001650               0.004989       0.015709   \n",
       "2                0.000000               0.000000       0.000000   \n",
       "3                0.002335               0.000666       0.001524   \n",
       "4                0.002237               0.001592       0.001402   \n",
       "\n",
       "   calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0               0.000888                 0.017088   \n",
       "1               0.000555                 0.004497   \n",
       "2               0.000000                 0.000000   \n",
       "3               0.001763                 0.036773   \n",
       "4               0.001212                 0.005894   \n",
       "\n",
       "   cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                      0.001900                         0.001913   \n",
       "1                      0.004327                         0.004000   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.002300                         0.005779   \n",
       "4                      0.002617                         0.002878   \n",
       "\n",
       "   carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  caspase_activator  \\\n",
       "0                      0.002342                 0.001705           0.003083   \n",
       "1                      0.001451                 0.003282           0.000923   \n",
       "2                      0.000000                 0.000000           0.000000   \n",
       "3                      0.002486                 0.002852           0.001729   \n",
       "4                      0.002837                 0.003195           0.001650   \n",
       "\n",
       "   catechol_o_methyltransferase_inhibitor  cc_chemokine_receptor_antagonist  \\\n",
       "0                                0.001676                          0.006628   \n",
       "1                                0.000905                          0.004806   \n",
       "2                                0.000000                          0.000000   \n",
       "3                                0.002065                          0.007399   \n",
       "4                                0.001487                          0.005434   \n",
       "\n",
       "   cck_receptor_antagonist  cdk_inhibitor  chelating_agent  chk_inhibitor  \\\n",
       "0                 0.002108       0.000439         0.004610       0.000766   \n",
       "1                 0.000964       0.004178         0.001686       0.003300   \n",
       "2                 0.000000       0.000000         0.000000       0.000000   \n",
       "3                 0.001546       0.001198         0.002893       0.000843   \n",
       "4                 0.001963       0.002082         0.004264       0.000870   \n",
       "\n",
       "   chloride_channel_blocker  cholesterol_inhibitor  \\\n",
       "0                  0.002410               0.003694   \n",
       "1                  0.001144               0.003149   \n",
       "2                  0.000000               0.000000   \n",
       "3                  0.005719               0.004632   \n",
       "4                  0.005746               0.002924   \n",
       "\n",
       "   cholinergic_receptor_antagonist  coagulation_factor_inhibitor  \\\n",
       "0                         0.007166                      0.000846   \n",
       "1                         0.000794                      0.000718   \n",
       "2                         0.000000                      0.000000   \n",
       "3                         0.003475                      0.001281   \n",
       "4                         0.002568                      0.001000   \n",
       "\n",
       "   corticosteroid_agonist  cyclooxygenase_inhibitor  \\\n",
       "0                0.001487                  0.048415   \n",
       "1                0.000902                  0.005624   \n",
       "2                0.000000                  0.000000   \n",
       "3                0.000969                  0.013453   \n",
       "4                0.001230                  0.012123   \n",
       "\n",
       "   cytochrome_p450_inhibitor  dihydrofolate_reductase_inhibitor  \\\n",
       "0                   0.005257                           0.001090   \n",
       "1                   0.001985                           0.000808   \n",
       "2                   0.000000                           0.000000   \n",
       "3                   0.004872                           0.002229   \n",
       "4                   0.006268                           0.002338   \n",
       "\n",
       "   dipeptidyl_peptidase_inhibitor  diuretic  dna_alkylating_agent  \\\n",
       "0                        0.003756  0.000912              0.003770   \n",
       "1                        0.001497  0.000630              0.001146   \n",
       "2                        0.000000  0.000000              0.000000   \n",
       "3                        0.001535  0.000993              0.003872   \n",
       "4                        0.002085  0.001158              0.005282   \n",
       "\n",
       "   dna_inhibitor  dopamine_receptor_agonist  dopamine_receptor_antagonist  \\\n",
       "0       0.013462                   0.007280                      0.011230   \n",
       "1       0.001388                   0.001172                      0.000890   \n",
       "2       0.000000                   0.000000                      0.000000   \n",
       "3       0.020781                   0.014512                      0.050883   \n",
       "4       0.032889                   0.004241                      0.004579   \n",
       "\n",
       "   egfr_inhibitor  elastase_inhibitor  erbb2_inhibitor  \\\n",
       "0        0.000600            0.001294         0.000645   \n",
       "1        0.000806            0.000901         0.000775   \n",
       "2        0.000000            0.000000         0.000000   \n",
       "3        0.009712            0.001099         0.000981   \n",
       "4        0.001140            0.000837         0.000770   \n",
       "\n",
       "   estrogen_receptor_agonist  estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                   0.023284                      0.001801        0.005585   \n",
       "1                   0.003633                      0.002827        0.006085   \n",
       "2                   0.000000                      0.000000        0.000000   \n",
       "3                   0.005044                      0.003914        0.001630   \n",
       "4                   0.010795                      0.002381        0.001761   \n",
       "\n",
       "   farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  fgfr_inhibitor  \\\n",
       "0                       0.000867                     0.001499        0.000746   \n",
       "1                       0.000545                     0.002604        0.005746   \n",
       "2                       0.000000                     0.000000        0.000000   \n",
       "3                       0.001457                     0.001583        0.002478   \n",
       "4                       0.001016                     0.002507        0.001902   \n",
       "\n",
       "   flt3_inhibitor  focal_adhesion_kinase_inhibitor  free_radical_scavenger  \\\n",
       "0        0.000464                         0.001270                0.001792   \n",
       "1        0.003658                         0.000798                0.000821   \n",
       "2        0.000000                         0.000000                0.000000   \n",
       "3        0.001027                         0.000863                0.001484   \n",
       "4        0.001325                         0.000574                0.001457   \n",
       "\n",
       "   fungal_squalene_epoxidase_inhibitor  gaba_receptor_agonist  \\\n",
       "0                             0.002579               0.021474   \n",
       "1                             0.001030               0.001836   \n",
       "2                             0.000000               0.000000   \n",
       "3                             0.001692               0.004172   \n",
       "4                             0.000943               0.005665   \n",
       "\n",
       "   gaba_receptor_antagonist  gamma_secretase_inhibitor  \\\n",
       "0                  0.021410                   0.001610   \n",
       "1                  0.003731                   0.002719   \n",
       "2                  0.000000                   0.000000   \n",
       "3                  0.005392                   0.001181   \n",
       "4                  0.004663                   0.000594   \n",
       "\n",
       "   glucocorticoid_receptor_agonist  glutamate_inhibitor  \\\n",
       "0                         0.001968             0.001535   \n",
       "1                         0.001368             0.001805   \n",
       "2                         0.000000             0.000000   \n",
       "3                         0.000718             0.001798   \n",
       "4                         0.001012             0.002750   \n",
       "\n",
       "   glutamate_receptor_agonist  glutamate_receptor_antagonist  \\\n",
       "0                    0.007545                       0.024480   \n",
       "1                    0.001170                       0.005067   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.005102                       0.013400   \n",
       "4                    0.003626                       0.016677   \n",
       "\n",
       "   gonadotropin_receptor_agonist  gsk_inhibitor  hcv_inhibitor  \\\n",
       "0                       0.001743       0.000986       0.006838   \n",
       "1                       0.001279       0.004870       0.003908   \n",
       "2                       0.000000       0.000000       0.000000   \n",
       "3                       0.002115       0.001200       0.003953   \n",
       "4                       0.001957       0.000984       0.004967   \n",
       "\n",
       "   hdac_inhibitor  histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0        0.002138                    0.005685                       0.007557   \n",
       "1        0.001176                    0.000699                       0.002022   \n",
       "2        0.000000                    0.000000                       0.000000   \n",
       "3        0.001125                    0.002991                       0.035045   \n",
       "4        0.001748                    0.002432                       0.008318   \n",
       "\n",
       "   histone_lysine_demethylase_inhibitor  \\\n",
       "0                              0.000707   \n",
       "1                              0.001197   \n",
       "2                              0.000000   \n",
       "3                              0.001751   \n",
       "4                              0.000681   \n",
       "\n",
       "   histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  hmgcr_inhibitor  \\\n",
       "0                                    0.001515       0.005580         0.000673   \n",
       "1                                    0.003728       0.001324         0.000902   \n",
       "2                                    0.000000       0.000000         0.000000   \n",
       "3                                    0.002229       0.006921         0.002629   \n",
       "4                                    0.001464       0.004294         0.000853   \n",
       "\n",
       "   hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0       0.000918         0.000779       0.002087   \n",
       "1       0.000896         0.006655       0.002116   \n",
       "2       0.000000         0.000000       0.000000   \n",
       "3       0.000798         0.001992       0.001131   \n",
       "4       0.001450         0.000737       0.003442   \n",
       "\n",
       "   imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                      0.002967           0.002203              0.002474   \n",
       "1                      0.001316           0.002518              0.003568   \n",
       "2                      0.000000           0.000000              0.000000   \n",
       "3                      0.003431           0.002777              0.002216   \n",
       "4                      0.002124           0.003104              0.003309   \n",
       "\n",
       "   insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0            0.000828            0.003685       0.000802       0.000512   \n",
       "1            0.034316            0.007652       0.005062       0.001664   \n",
       "2            0.000000            0.000000       0.000000       0.000000   \n",
       "3            0.001318            0.001805       0.000620       0.001065   \n",
       "4            0.004686            0.002480       0.001210       0.001034   \n",
       "\n",
       "   laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0  0.001015               0.000964                         0.005826   \n",
       "1  0.000701               0.000663                         0.004100   \n",
       "2  0.000000               0.000000                         0.000000   \n",
       "3  0.001221               0.001413                         0.002288   \n",
       "4  0.001207               0.001191                         0.002684   \n",
       "\n",
       "   lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0          0.001423                0.004561     0.001155       0.000983   \n",
       "1          0.001557                0.003264     0.000753       0.000986   \n",
       "2          0.000000                0.000000     0.000000       0.000000   \n",
       "3          0.001419                0.002730     0.001767       0.000726   \n",
       "4          0.001371                0.002842     0.000844       0.000755   \n",
       "\n",
       "   mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0       0.000633                      0.006477   \n",
       "1       0.001821                      0.001725   \n",
       "2       0.000000                      0.000000   \n",
       "3       0.001158                      0.005411   \n",
       "4       0.000969                      0.006635   \n",
       "\n",
       "   mineralocorticoid_receptor_antagonist  monoacylglycerol_lipase_inhibitor  \\\n",
       "0                               0.002535                           0.002024   \n",
       "1                               0.001357                           0.000680   \n",
       "2                               0.000000                           0.000000   \n",
       "3                               0.003185                           0.001172   \n",
       "4                               0.001691                           0.001201   \n",
       "\n",
       "   monoamine_oxidase_inhibitor  monopolar_spindle_1_kinase_inhibitor  \\\n",
       "0                     0.004833                              0.000998   \n",
       "1                     0.001381                              0.001155   \n",
       "2                     0.000000                              0.000000   \n",
       "3                     0.008032                              0.001218   \n",
       "4                     0.004352                              0.000928   \n",
       "\n",
       "   mtor_inhibitor  mucolytic_agent  neuropeptide_receptor_antagonist  \\\n",
       "0        0.000826         0.004893                          0.000675   \n",
       "1        0.002692         0.000874                          0.001639   \n",
       "2        0.000000         0.000000                          0.000000   \n",
       "3        0.002003         0.003470                          0.004270   \n",
       "4        0.002775         0.003327                          0.001673   \n",
       "\n",
       "   nfkb_inhibitor  nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0        0.007152                    0.001236            0.001405   \n",
       "1        0.001186                    0.000669            0.000736   \n",
       "2        0.000000                    0.000000            0.000000   \n",
       "3        0.003867                    0.001093            0.002983   \n",
       "4        0.006007                    0.001044            0.003002   \n",
       "\n",
       "   nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                           0.001315                         0.001397   \n",
       "1                           0.001049                         0.000645   \n",
       "2                           0.000000                         0.000000   \n",
       "3                           0.001158                         0.002968   \n",
       "4                           0.001679                         0.001881   \n",
       "\n",
       "   norepinephrine_reuptake_inhibitor  nrf2_activator  opioid_receptor_agonist  \\\n",
       "0                           0.000820        0.001595                 0.002367   \n",
       "1                           0.000609        0.000932                 0.001069   \n",
       "2                           0.000000        0.000000                 0.000000   \n",
       "3                           0.001526        0.000808                 0.006882   \n",
       "4                           0.000966        0.001432                 0.003259   \n",
       "\n",
       "   opioid_receptor_antagonist  orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                    0.005865                    0.001601            0.000766   \n",
       "1                    0.001701                    0.001357            0.000784   \n",
       "2                    0.000000                    0.000000            0.000000   \n",
       "3                    0.009011                    0.003900            0.006757   \n",
       "4                    0.004755                    0.003669            0.001483   \n",
       "\n",
       "   p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  pdk_inhibitor  \\\n",
       "0                  0.001178        0.002700         0.000433       0.001566   \n",
       "1                  0.003850        0.005033         0.002633       0.001905   \n",
       "2                  0.000000        0.000000         0.000000       0.000000   \n",
       "3                  0.001477        0.001657         0.001065       0.001197   \n",
       "4                  0.001594        0.002534         0.001530       0.001403   \n",
       "\n",
       "   phosphodiesterase_inhibitor  phospholipase_inhibitor  pi3k_inhibitor  \\\n",
       "0                     0.017764                 0.002714        0.001373   \n",
       "1                     0.019721                 0.000686        0.012107   \n",
       "2                     0.000000                 0.000000        0.000000   \n",
       "3                     0.006758                 0.002499        0.005329   \n",
       "4                     0.010520                 0.001349        0.002463   \n",
       "\n",
       "   pkc_inhibitor  potassium_channel_activator  potassium_channel_antagonist  \\\n",
       "0       0.001391                     0.002461                      0.010842   \n",
       "1       0.003760                     0.002891                      0.001715   \n",
       "2       0.000000                     0.000000                      0.000000   \n",
       "3       0.002835                     0.004064                      0.007908   \n",
       "4       0.001705                     0.006297                      0.002895   \n",
       "\n",
       "   ppar_receptor_agonist  ppar_receptor_antagonist  \\\n",
       "0               0.001684                  0.002669   \n",
       "1               0.062141                  0.015617   \n",
       "2               0.000000                  0.000000   \n",
       "3               0.001148                  0.001728   \n",
       "4               0.009822                  0.002143   \n",
       "\n",
       "   progesterone_receptor_agonist  progesterone_receptor_antagonist  \\\n",
       "0                       0.017354                          0.002287   \n",
       "1                       0.001812                          0.020423   \n",
       "2                       0.000000                          0.000000   \n",
       "3                       0.002154                          0.001294   \n",
       "4                       0.004684                          0.002638   \n",
       "\n",
       "   prostaglandin_inhibitor  prostanoid_receptor_antagonist  \\\n",
       "0                 0.005105                        0.009152   \n",
       "1                 0.001514                        0.002731   \n",
       "2                 0.000000                        0.000000   \n",
       "3                 0.002829                        0.003569   \n",
       "4                 0.003661                        0.003498   \n",
       "\n",
       "   proteasome_inhibitor  protein_kinase_inhibitor  \\\n",
       "0              0.000901                  0.003887   \n",
       "1              0.000663                  0.002592   \n",
       "2              0.000000                  0.000000   \n",
       "3              0.000878                  0.002523   \n",
       "4              0.001336                  0.002330   \n",
       "\n",
       "   protein_phosphatase_inhibitor  protein_synthesis_inhibitor  \\\n",
       "0                       0.000647                     0.002988   \n",
       "1                       0.000953                     0.000756   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.001126                     0.008218   \n",
       "4                       0.001447                     0.005499   \n",
       "\n",
       "   protein_tyrosine_kinase_inhibitor  radiopaque_medium  raf_inhibitor  \\\n",
       "0                           0.001009           0.004401       0.001089   \n",
       "1                           0.001471           0.000798       0.000332   \n",
       "2                           0.000000           0.000000       0.000000   \n",
       "3                           0.002115           0.003892       0.001146   \n",
       "4                           0.001754           0.004840       0.001983   \n",
       "\n",
       "   ras_gtpase_inhibitor  retinoid_receptor_agonist  \\\n",
       "0              0.001049                   0.002638   \n",
       "1              0.001317                   0.001048   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.001726                   0.000610   \n",
       "4              0.001049                   0.001115   \n",
       "\n",
       "   retinoid_receptor_antagonist  rho_associated_kinase_inhibitor  \\\n",
       "0                      0.000733                         0.000940   \n",
       "1                      0.002002                         0.031636   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.001181                         0.001721   \n",
       "4                      0.001592                         0.002834   \n",
       "\n",
       "   ribonucleoside_reductase_inhibitor  rna_polymerase_inhibitor  \\\n",
       "0                            0.001040                  0.002114   \n",
       "1                            0.001611                  0.002826   \n",
       "2                            0.000000                  0.000000   \n",
       "3                            0.001719                  0.001574   \n",
       "4                            0.004353                  0.002216   \n",
       "\n",
       "   serotonin_receptor_agonist  serotonin_receptor_antagonist  \\\n",
       "0                    0.010523                       0.004820   \n",
       "1                    0.007226                       0.000977   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.016589                       0.033095   \n",
       "4                    0.008453                       0.007754   \n",
       "\n",
       "   serotonin_reuptake_inhibitor  sigma_receptor_agonist  \\\n",
       "0                      0.004235                0.003681   \n",
       "1                      0.000540                0.000993   \n",
       "2                      0.000000                0.000000   \n",
       "3                      0.004247                0.003331   \n",
       "4                      0.002070                0.001912   \n",
       "\n",
       "   sigma_receptor_antagonist  smoothened_receptor_antagonist  \\\n",
       "0                   0.001466                        0.001454   \n",
       "1                   0.001112                        0.003350   \n",
       "2                   0.000000                        0.000000   \n",
       "3                   0.004680                        0.001929   \n",
       "4                   0.001388                        0.002598   \n",
       "\n",
       "   sodium_channel_inhibitor  sphingosine_receptor_agonist  src_inhibitor  \\\n",
       "0                  0.024195                      0.003137       0.000704   \n",
       "1                  0.004546                      0.000992       0.011445   \n",
       "2                  0.000000                      0.000000       0.000000   \n",
       "3                  0.009061                      0.002179       0.002163   \n",
       "4                  0.014903                      0.002363       0.001144   \n",
       "\n",
       "    steroid  syk_inhibitor  tachykinin_antagonist  \\\n",
       "0  0.000826       0.000792               0.001469   \n",
       "1  0.000905       0.003275               0.002800   \n",
       "2  0.000000       0.000000               0.000000   \n",
       "3  0.001608       0.001651               0.004551   \n",
       "4  0.001743       0.001765               0.003861   \n",
       "\n",
       "   tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                     0.000347            0.000955   \n",
       "1                     0.002143            0.000670   \n",
       "2                     0.000000            0.000000   \n",
       "3                     0.001321            0.002336   \n",
       "4                     0.001317            0.001322   \n",
       "\n",
       "   thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  tnf_inhibitor  \\\n",
       "0                        0.001216     0.001664        0.001052       0.004060   \n",
       "1                        0.000481     0.001097        0.001606       0.002478   \n",
       "2                        0.000000     0.000000        0.000000       0.000000   \n",
       "3                        0.003241     0.002680        0.001107       0.001919   \n",
       "4                        0.003400     0.002431        0.001161       0.001767   \n",
       "\n",
       "   topoisomerase_inhibitor  transient_receptor_potential_channel_antagonist  \\\n",
       "0                 0.001427                                         0.001354   \n",
       "1                 0.008366                                         0.001368   \n",
       "2                 0.000000                                         0.000000   \n",
       "3                 0.001484                                         0.001931   \n",
       "4                 0.004136                                         0.001150   \n",
       "\n",
       "   tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                               0.001092      0.003016         0.005292   \n",
       "1                               0.001116      0.000956         0.004205   \n",
       "2                               0.000000      0.000000         0.000000   \n",
       "3                               0.001049      0.001164         0.002884   \n",
       "4                               0.001198      0.000859         0.003719   \n",
       "\n",
       "   tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0           0.001437                   0.000815   \n",
       "1           0.000292                   0.007264   \n",
       "2           0.000000                   0.000000   \n",
       "3           0.007775                   0.003948   \n",
       "4           0.001564                   0.001667   \n",
       "\n",
       "   ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                               0.001008         0.001007   0.001937   \n",
       "1                               0.000654         0.006777   0.001188   \n",
       "2                               0.000000         0.000000   0.000000   \n",
       "3                               0.001090         0.001821   0.002346   \n",
       "4                               0.001086         0.002338   0.002116   \n",
       "\n",
       "   vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0.017152       0.001353  \n",
       "1                    0.002213       0.003306  \n",
       "2                    0.000000       0.000000  \n",
       "3                    0.001199       0.003573  \n",
       "4                    0.000526       0.002297  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = sample_submission.drop(columns=target_cols)\\\n",
    ".merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    ".fillna(0.0).reset_index(drop=True)\n",
    "name_sub = 'submission.csv'\n",
    "submission.to_csv(name_sub, index=False)\n",
    "# print(name_sub)\n",
    "print(\"ANN_1dic\")\n",
    "submission_ANN_30nov = submission.copy()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.075776,
     "end_time": "2020-12-06T00:02:37.986880",
     "exception": false,
     "start_time": "2020-12-06T00:02:37.911104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.075475,
     "end_time": "2020-12-06T00:02:38.138622",
     "exception": false,
     "start_time": "2020-12-06T00:02:38.063147",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ANN del 30 de noviembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:30.009Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:38.294893Z",
     "iopub.status.busy": "2020-12-06T00:02:38.294101Z",
     "iopub.status.idle": "2020-12-06T00:02:38.297938Z",
     "shell.execute_reply": "2020-12-06T00:02:38.297431Z"
    },
    "papermill": {
     "duration": 0.083339,
     "end_time": "2020-12-06T00:02:38.298028",
     "exception": false,
     "start_time": "2020-12-06T00:02:38.214689",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directorio = '../input/annrandomnosmooth' \n",
    "# folds_cp = folds.copy()\n",
    "# feature_cols = feature_cols_ini.copy()\n",
    "# NFOLDS = CFG.num_folds\n",
    "# num_features=len(feature_cols)\n",
    "# num_targets=len(target_cols)\n",
    "\n",
    "# DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# EPOCHS = 25\n",
    "# BATCH_SIZE = 128\n",
    "# LEARNING_RATE = 1e-3\n",
    "# WEIGHT_DECAY = 1e-5\n",
    "# EARLY_STOPPING_STEPS = 10\n",
    "# EARLY_STOP = False\n",
    "# hidden_size = 256\n",
    "# res = []\n",
    "# drop1_feat = 0.05\n",
    "# drop2_feat = 0.05\n",
    "# drop3_feat = 0.25\n",
    "\n",
    "\n",
    "# # Averaging on multiple SEEDS\n",
    "# oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "# predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "# losses_list = []\n",
    "# # SEED = [[0,1],[1,1],[2,1],[3,1]]\n",
    "# SEED = np.arange(16)\n",
    "# for seed in tqdm(SEED):\n",
    "#     seed_fold = seed // 4\n",
    "#     seed_run = seed % 4\n",
    "#     predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "#     predictions += predictions_\n",
    "\n",
    "# # FINAL CV LOGLOSS\n",
    "# for col in target_cols:\n",
    "#     test_noctl[col] = 0.0\n",
    "# test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-10-25T10:54:30.012Z"
    },
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:38.456702Z",
     "iopub.status.busy": "2020-12-06T00:02:38.454824Z",
     "iopub.status.idle": "2020-12-06T00:02:38.457454Z",
     "shell.execute_reply": "2020-12-06T00:02:38.457936Z"
    },
    "papermill": {
     "duration": 0.083193,
     "end_time": "2020-12-06T00:02:38.458041",
     "exception": false,
     "start_time": "2020-12-06T00:02:38.374848",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = sample_submission.drop(columns=target_cols)\\\n",
    "# .merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    "# .fillna(0.0).reset_index(drop=True)\n",
    "# name_sub = 'submission.csv'\n",
    "# submission.to_csv(name_sub, index=False)\n",
    "# print(\"ANN_30nov\")\n",
    "# submission_ANN_30nov = submission.copy()\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.093455,
     "end_time": "2020-12-06T00:02:38.628483",
     "exception": false,
     "start_time": "2020-12-06T00:02:38.535028",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## ANN del 28 de noviembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:38.790054Z",
     "iopub.status.busy": "2020-12-06T00:02:38.789359Z",
     "iopub.status.idle": "2020-12-06T00:02:38.792994Z",
     "shell.execute_reply": "2020-12-06T00:02:38.792530Z"
    },
    "papermill": {
     "duration": 0.084508,
     "end_time": "2020-12-06T00:02:38.793080",
     "exception": false,
     "start_time": "2020-12-06T00:02:38.708572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directorio = '../input/annrandom28nov3'\n",
    "# folds_cp = folds.copy()\n",
    "# feature_cols = feature_cols_ini_nocps.copy()\n",
    "# NFOLDS = CFG.num_folds\n",
    "# num_features=len(feature_cols)\n",
    "# num_targets=len(target_cols)\n",
    "\n",
    "# DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# EPOCHS = 25\n",
    "# BATCH_SIZE = 128\n",
    "# LEARNING_RATE = 1e-3\n",
    "# WEIGHT_DECAY = 1e-5\n",
    "# EARLY_STOPPING_STEPS = 10\n",
    "# EARLY_STOP = False\n",
    "# hidden_size = 256\n",
    "# res = []\n",
    "# drop1_feat = 0.05\n",
    "# drop2_feat = 0.05\n",
    "# drop3_feat = 0.25\n",
    "\n",
    "\n",
    "# # Averaging on multiple SEEDS\n",
    "# oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "# predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "# losses_list = []\n",
    "# # SEED = [[0,1],[1,1],[2,1],[3,1]]\n",
    "# SEED = np.arange(16)\n",
    "# for seed in tqdm(SEED):\n",
    "#     seed_fold = seed // 4\n",
    "#     seed_run = seed % 4\n",
    "#     predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "#     predictions += predictions_\n",
    "\n",
    "# # FINAL CV LOGLOSS\n",
    "# for col in target_cols:\n",
    "#     test_noctl[col] = 0.0\n",
    "# test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:38.951010Z",
     "iopub.status.busy": "2020-12-06T00:02:38.950262Z",
     "iopub.status.idle": "2020-12-06T00:02:38.953259Z",
     "shell.execute_reply": "2020-12-06T00:02:38.952749Z"
    },
    "papermill": {
     "duration": 0.082524,
     "end_time": "2020-12-06T00:02:38.953352",
     "exception": false,
     "start_time": "2020-12-06T00:02:38.870828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = sample_submission.drop(columns=target_cols)\\\n",
    "# .merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    "# .fillna(0.0).reset_index(drop=True)\n",
    "# # sub.to_csv('submission.csv', index=False)\n",
    "# # name_sub = 'submission.csv'\n",
    "# # submission.to_csv(name_sub, index=False)\n",
    "# print(\"ANN_28nov\")\n",
    "# submission_ANN_28nov = submission.copy()\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.076938,
     "end_time": "2020-12-06T00:02:39.107875",
     "exception": false,
     "start_time": "2020-12-06T00:02:39.030937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.076379,
     "end_time": "2020-12-06T00:02:39.261952",
     "exception": false,
     "start_time": "2020-12-06T00:02:39.185573",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 2. RESNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:39.443468Z",
     "iopub.status.busy": "2020-12-06T00:02:39.441630Z",
     "iopub.status.idle": "2020-12-06T00:02:39.444234Z",
     "shell.execute_reply": "2020-12-06T00:02:39.444728Z"
    },
    "papermill": {
     "duration": 0.104678,
     "end_time": "2020-12-06T00:02:39.444838",
     "exception": false,
     "start_time": "2020-12-06T00:02:39.340160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "hidden_size1 = 1024\n",
    "hidden_size2 = hidden_size1 // 2\n",
    "drop_h1 = 0.2\n",
    "drop_h2 = 0.2\n",
    "drop_h3 = 0.2\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_features, num_targets):\n",
    "        super(Model, self).__init__()\n",
    "        # h1\n",
    "        self.batch_norm1_h1 = nn.BatchNorm1d(num_features)\n",
    "        self.dropout1_h1 = nn.Dropout(drop_h1) \n",
    "        self.dense1_h1 = nn.Linear(num_features, hidden_size1)\n",
    "        \n",
    "        self.batch_norm2_h1 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.dense2_h1 = nn.Linear(hidden_size1, hidden_size2)\n",
    "\n",
    "        # h2\n",
    "        self.batch_norm1_h2 = nn.BatchNorm1d(hidden_size2+num_features)\n",
    "        self.dropout1_h2 = nn.Dropout(drop_h2) \n",
    "        self.dense1_h2 = nn.Linear(hidden_size2+num_features, hidden_size1)\n",
    "        \n",
    "        self.batch_norm2_h2 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.dense2_h2 = nn.Linear(hidden_size1,hidden_size1)\n",
    "        \n",
    "        self.batch_norm3_h2 = nn.BatchNorm1d(hidden_size1)\n",
    "        self.dense3_h2 = nn.Linear(hidden_size1,hidden_size2)\n",
    "               \n",
    "        self.batch_norm4_h2 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.dense4_h2 = nn.Linear(hidden_size2,hidden_size2)\n",
    "        \n",
    "        # h3\n",
    "#         self.batch_norm1_h3 = nn.BatchNorm1d(hidden_size2*2)\n",
    "#         self.dropout1_h3 = nn.Dropout(drop_h3) \n",
    "#         self.dense1_h3 = nn.Linear(hidden_size2*2,hidden_size2)\n",
    "\n",
    "        self.batch_norm1_h3 = nn.BatchNorm1d(hidden_size2)\n",
    "        self.dropout1_h3 = nn.Dropout(drop_h3) \n",
    "        self.dense1_h3 = nn.Linear(hidden_size2,hidden_size2//2)\n",
    "        \n",
    "        self.batch_norm2_h3 = nn.BatchNorm1d(hidden_size2//2)\n",
    "        self.dropout2_h3 = nn.Dropout(drop_h3) \n",
    "        self.dense2_h3 = nn.Linear(hidden_size2//2,num_targets)\n",
    "        \n",
    "#         self.batch_norm3_h3 = nn.BatchNorm1d(num_targets)\n",
    "#         self.dense3_h3 = nn.Linear(num_targets,num_targets)\n",
    "        \n",
    "#     x2 = torch.log(1+x-torch.min(x))\n",
    "#         x2 = self.batch_norm1B(x2)\n",
    "#         x2 = self.dropout1B(x2)\n",
    "#         x2 = self.dense1B(x2)\n",
    "#         x2 = torch.exp(x2)\n",
    "#         x2 = F.leaky_relu(x2)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        xmult = x-torch.min(x)\n",
    "        \n",
    "        h1 = self.batch_norm1_h1(x)\n",
    "        h1 = self.dropout1_h1(h1)\n",
    "#         h1 = F.elu(self.dense1_h1(h1))\n",
    "        h1 = F.leaky_relu(self.dense1_h1(h1))\n",
    "        h1 = self.batch_norm2_h1(h1)\n",
    "#         h1 = F.elu(self.dense2_h1(h1))\n",
    "        h1 = F.leaky_relu(self.dense2_h1(h1))\n",
    "    \n",
    "        combined = torch.cat((x.view(x.size(0), -1), h1.view(h1.size(0), -1)), dim=1)\n",
    "        h2 = self.batch_norm1_h2(combined)\n",
    "        h2 = self.dropout1_h2(h2)\n",
    "#         h2 = F.relu(self.dense1_h2(h2))\n",
    "        h2 = F.leaky_relu(self.dense1_h2(h2))\n",
    "        h2 = self.batch_norm2_h2(h2)\n",
    "#         h2 = F.elu(self.dense2_h2(h2))\n",
    "        h2 = F.leaky_relu(self.dense2_h2(h2))\n",
    "        h2 = self.batch_norm3_h2(h2)\n",
    "#         h2 = F.relu(self.dense3_h2(h2))\n",
    "        h2 = F.leaky_relu(self.dense3_h2(h2))\n",
    "        h2 = self.batch_norm4_h2(h2)\n",
    "#         h2 = F.elu(self.dense4_h2(h2))\n",
    "        h2 = F.leaky_relu(self.dense4_h2(h2))\n",
    "    \n",
    "#         # Xmult\n",
    "#         xmult = x-torch.min(x)\n",
    "#         h1b = torch.log1p(xmult)\n",
    "#         h1b = self.batch_norm1_h1(h1b)\n",
    "#         h1b = self.dropout1_h1(h1b)\n",
    "#         h1b = self.dense1_h1(h1b)\n",
    "#         h1b = torch.expm1(h1b)\n",
    "#         h1b = F.leaky_relu(h1b)\n",
    "        \n",
    "#         h1b = self.batch_norm2_h1(h1b)\n",
    "# #         h1 = F.elu(self.dense2_h1(h1))\n",
    "#         h1b = F.leaky_relu(self.dense2_h1(h1b))\n",
    "    \n",
    "#         combined_mult = torch.cat((x.view(x.size(0), -1), h1b.view(h1b.size(0), -1)), dim=1)\n",
    "#         h2b = combined_mult-torch.min(combined_mult)\n",
    "#         h2b = torch.log1p(h2b)\n",
    "#         h2b = self.batch_norm1_h2(h2b)\n",
    "#         h2b = self.dropout1_h2(h2b)\n",
    "#         h2b = self.dense1_h2(h2b)\n",
    "#         h2b = torch.expm1(h2b)\n",
    "#         h2b = F.leaky_relu(h2b)\n",
    "\n",
    "#         h2b = self.batch_norm2_h2(h2b)\n",
    "# #         h2 = F.elu(self.dense2_h2(h2))\n",
    "#         h2b = F.leaky_relu(self.dense2_h2(h2b))\n",
    "#         h2b = self.batch_norm3_h2(h2b)\n",
    "# #         h2 = F.relu(self.dense3_h2(h2))\n",
    "#         h2b = F.leaky_relu(self.dense3_h2(h2b))\n",
    "#         h2b = self.batch_norm4_h2(h2b)\n",
    "# #         h2 = F.elu(self.dense4_h2(h2))\n",
    "#         h2b = F.leaky_relu(self.dense4_h2(h2b))\n",
    "    \n",
    "    \n",
    "#         combined_h1s = torch.cat((h1.view(h1.size(0), -1), h1b.view(h1b.size(0), -1)), dim=1)\n",
    "#         combined_h2s = torch.cat((h2.view(h2.size(0), -1), h2b.view(h2b.size(0), -1)), dim=1)\n",
    "    \n",
    "#         average2heads= (combined_h1s+combined_h2s)/2.0\n",
    "        average2heads = (h1+h2)/2.0\n",
    "        h3 = self.batch_norm1_h3(average2heads)\n",
    "#         h3 = self.dropout1_h3(h3)\n",
    "        h3 = self.dense1_h3(h3)\n",
    "        \n",
    "        output = self.batch_norm2_h3(h3)\n",
    "#         output = self.dropout2_h3(output)\n",
    "        output = self.dense2_h3(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:39.609442Z",
     "iopub.status.busy": "2020-12-06T00:02:39.607681Z",
     "iopub.status.idle": "2020-12-06T00:02:39.610183Z",
     "shell.execute_reply": "2020-12-06T00:02:39.610666Z"
    },
    "papermill": {
     "duration": 0.08876,
     "end_time": "2020-12-06T00:02:39.610776",
     "exception": false,
     "start_time": "2020-12-06T00:02:39.522016",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_RESNET(fold, seedfold, seedrun, display=2):\n",
    "    seed_everything(seedrun)\n",
    "    test_ = test_noctl.copy()\n",
    "    \n",
    "    #--------------------- PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    testdataset = TestDataset(x_test)\n",
    "    testloader = torch.utils.data.DataLoader(testdataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    model = Model(\n",
    "        num_features=num_features,\n",
    "        num_targets=num_targets,\n",
    "#         hidden_size=hidden_size,\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load(f'{directorio}/ann_model_seedfold{seedfold}_seedrun{seedrun}_fold__{fold}.pth'))\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    predictions_tst = np.zeros((len(test_), target.iloc[:, 1:].shape[1]))\n",
    "    predictions_tst = inference_fn(model, testloader, DEVICE)\n",
    "    \n",
    "    return predictions_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:39.770567Z",
     "iopub.status.busy": "2020-12-06T00:02:39.769885Z",
     "iopub.status.idle": "2020-12-06T00:02:39.773293Z",
     "shell.execute_reply": "2020-12-06T00:02:39.773729Z"
    },
    "papermill": {
     "duration": 0.085849,
     "end_time": "2020-12-06T00:02:39.773841",
     "exception": false,
     "start_time": "2020-12-06T00:02:39.687992",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed_fold, seed_run, display=2):\n",
    "    predictions_kfold = np.zeros((len(test_noctl), len(target_cols)))\n",
    "    for fold in range(NFOLDS):\n",
    "        pred_ = run_inference_RESNET(fold, seed_fold, seed_run, display)\n",
    "        predictions_kfold += pred_ / NFOLDS\n",
    "    return predictions_kfold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.077677,
     "end_time": "2020-12-06T00:02:39.928721",
     "exception": false,
     "start_time": "2020-12-06T00:02:39.851044",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resnet del 30 de noviembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:02:40.094694Z",
     "iopub.status.busy": "2020-12-06T00:02:40.093812Z",
     "iopub.status.idle": "2020-12-06T00:03:58.525002Z",
     "shell.execute_reply": "2020-12-06T00:03:58.523993Z"
    },
    "papermill": {
     "duration": 78.519117,
     "end_time": "2020-12-06T00:03:58.525129",
     "exception": false,
     "start_time": "2020-12-06T00:02:40.006012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7903dfefa48948e28aa27fe505eb03b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "directorio = '../input/resnetrandom' #'../input/resnetstrat' #'../input/resnetrandomnostrat' #'../input/resnetstratnosmooth' #'../input/resnetrandomnostrat'\n",
    "folds_cp = folds.copy()\n",
    "feature_cols = feature_cols_ini.copy()\n",
    "NFOLDS = CFG.num_folds\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "\n",
    "DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "EPOCHS = 25\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 1e-5\n",
    "EARLY_STOPPING_STEPS = 10\n",
    "EARLY_STOP = False\n",
    "\n",
    "res = []\n",
    "# Averaging on multiple SEEDS\n",
    "oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "losses_list = []\n",
    "# SEED = [[0,3],[1,3],[2,2],[3,0]]\n",
    "# for seed_fold, seed_run in tqdm(SEED):\n",
    "SEED = np.arange(16)\n",
    "for seed in tqdm(SEED):\n",
    "    seed_fold = seed // 4\n",
    "    seed_run = seed % 4\n",
    "    predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "    predictions += predictions_\n",
    "\n",
    "# FINAL CV LOGLOSS\n",
    "for col in target_cols:\n",
    "    test_noctl[col] = 0.0\n",
    "test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:03:58.701723Z",
     "iopub.status.busy": "2020-12-06T00:03:58.700380Z",
     "iopub.status.idle": "2020-12-06T00:04:00.641120Z",
     "shell.execute_reply": "2020-12-06T00:04:00.641624Z"
    },
    "papermill": {
     "duration": 2.030478,
     "end_time": "2020-12-06T00:04:00.641785",
     "exception": false,
     "start_time": "2020-12-06T00:03:58.611307",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESNET 30nov\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.001340</td>\n",
       "      <td>0.003699</td>\n",
       "      <td>0.010391</td>\n",
       "      <td>0.014362</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.002167</td>\n",
       "      <td>0.006994</td>\n",
       "      <td>0.000471</td>\n",
       "      <td>0.008558</td>\n",
       "      <td>0.011338</td>\n",
       "      <td>0.000703</td>\n",
       "      <td>0.001393</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.001308</td>\n",
       "      <td>0.002041</td>\n",
       "      <td>0.005685</td>\n",
       "      <td>0.006751</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.002864</td>\n",
       "      <td>0.004606</td>\n",
       "      <td>0.000822</td>\n",
       "      <td>0.002810</td>\n",
       "      <td>0.000802</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.001996</td>\n",
       "      <td>0.003463</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.005534</td>\n",
       "      <td>0.004967</td>\n",
       "      <td>0.000542</td>\n",
       "      <td>0.000720</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.006512</td>\n",
       "      <td>0.000532</td>\n",
       "      <td>0.000987</td>\n",
       "      <td>0.000667</td>\n",
       "      <td>0.004276</td>\n",
       "      <td>0.010717</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.007091</td>\n",
       "      <td>0.007154</td>\n",
       "      <td>0.005985</td>\n",
       "      <td>0.001434</td>\n",
       "      <td>0.003479</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.002146</td>\n",
       "      <td>0.003741</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.018831</td>\n",
       "      <td>0.001698</td>\n",
       "      <td>0.001932</td>\n",
       "      <td>0.002163</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.002490</td>\n",
       "      <td>0.001722</td>\n",
       "      <td>0.005543</td>\n",
       "      <td>0.001892</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.000563</td>\n",
       "      <td>0.002747</td>\n",
       "      <td>0.005081</td>\n",
       "      <td>0.005517</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.055824</td>\n",
       "      <td>0.006146</td>\n",
       "      <td>0.001246</td>\n",
       "      <td>0.002956</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.005736</td>\n",
       "      <td>0.017876</td>\n",
       "      <td>0.008564</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>0.000449</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.000673</td>\n",
       "      <td>0.017718</td>\n",
       "      <td>0.001815</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.000565</td>\n",
       "      <td>0.000339</td>\n",
       "      <td>0.001530</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.002181</td>\n",
       "      <td>0.022570</td>\n",
       "      <td>0.021721</td>\n",
       "      <td>0.002108</td>\n",
       "      <td>0.001585</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.008050</td>\n",
       "      <td>0.023278</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.009450</td>\n",
       "      <td>0.002073</td>\n",
       "      <td>0.005162</td>\n",
       "      <td>0.007772</td>\n",
       "      <td>0.000768</td>\n",
       "      <td>0.001257</td>\n",
       "      <td>0.004546</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001151</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.001527</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000302</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.000954</td>\n",
       "      <td>0.005075</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.003400</td>\n",
       "      <td>0.001488</td>\n",
       "      <td>0.000834</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.008768</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.004123</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.000795</td>\n",
       "      <td>0.005056</td>\n",
       "      <td>0.000778</td>\n",
       "      <td>0.005332</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>0.000524</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.002616</td>\n",
       "      <td>0.000247</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.010169</td>\n",
       "      <td>0.002934</td>\n",
       "      <td>0.003232</td>\n",
       "      <td>0.010857</td>\n",
       "      <td>0.002476</td>\n",
       "      <td>0.006286</td>\n",
       "      <td>0.008062</td>\n",
       "      <td>0.000679</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.000757</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.003469</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.002779</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.000891</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.002197</td>\n",
       "      <td>0.009240</td>\n",
       "      <td>0.004762</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>0.003893</td>\n",
       "      <td>0.001235</td>\n",
       "      <td>0.001594</td>\n",
       "      <td>0.021606</td>\n",
       "      <td>0.003533</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.005084</td>\n",
       "      <td>0.000719</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.001293</td>\n",
       "      <td>0.003830</td>\n",
       "      <td>0.004720</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.000797</td>\n",
       "      <td>0.001933</td>\n",
       "      <td>0.022528</td>\n",
       "      <td>0.001695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.000601</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.000495</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.006281</td>\n",
       "      <td>0.008441</td>\n",
       "      <td>0.003315</td>\n",
       "      <td>0.004406</td>\n",
       "      <td>0.003003</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.013655</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.003597</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.001100</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.000384</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.000840</td>\n",
       "      <td>0.001671</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.000288</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.002973</td>\n",
       "      <td>0.003937</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000436</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>0.000590</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.003602</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.005441</td>\n",
       "      <td>0.016899</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.002724</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.002578</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.000556</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.000683</td>\n",
       "      <td>0.006828</td>\n",
       "      <td>0.001535</td>\n",
       "      <td>0.003461</td>\n",
       "      <td>0.000620</td>\n",
       "      <td>0.004905</td>\n",
       "      <td>0.000463</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.000746</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.000379</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.000548</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.000496</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.001402</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000772</td>\n",
       "      <td>0.001613</td>\n",
       "      <td>0.001947</td>\n",
       "      <td>0.006391</td>\n",
       "      <td>0.000414</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>0.012295</td>\n",
       "      <td>0.004076</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000529</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.003137</td>\n",
       "      <td>0.001459</td>\n",
       "      <td>0.000905</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.000607</td>\n",
       "      <td>0.003128</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.004560</td>\n",
       "      <td>0.002481</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.000451</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.002153</td>\n",
       "      <td>0.004140</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.001327</td>\n",
       "      <td>0.000966</td>\n",
       "      <td>0.007523</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.004194</td>\n",
       "      <td>0.044226</td>\n",
       "      <td>0.007635</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.002743</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.003319</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.002228</td>\n",
       "      <td>0.000739</td>\n",
       "      <td>0.000593</td>\n",
       "      <td>0.002256</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.000836</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>0.001101</td>\n",
       "      <td>0.000697</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>0.000541</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.000626</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000514</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.000467</td>\n",
       "      <td>0.000710</td>\n",
       "      <td>0.000481</td>\n",
       "      <td>0.000658</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.005373</td>\n",
       "      <td>0.003838</td>\n",
       "      <td>0.004854</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.014334</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.017689</td>\n",
       "      <td>0.003486</td>\n",
       "      <td>0.002171</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>0.044096</td>\n",
       "      <td>0.016302</td>\n",
       "      <td>0.000512</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.001644</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.000472</td>\n",
       "      <td>0.001676</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.002244</td>\n",
       "      <td>0.001683</td>\n",
       "      <td>0.029939</td>\n",
       "      <td>0.000776</td>\n",
       "      <td>0.002489</td>\n",
       "      <td>0.006110</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.000474</td>\n",
       "      <td>0.000777</td>\n",
       "      <td>0.000705</td>\n",
       "      <td>0.002467</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.017514</td>\n",
       "      <td>0.000804</td>\n",
       "      <td>0.003687</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>0.000981</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.001386</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.000493</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.000151</td>\n",
       "      <td>0.007047</td>\n",
       "      <td>0.000454</td>\n",
       "      <td>0.010448</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001162</td>\n",
       "      <td>0.002062</td>\n",
       "      <td>0.013520</td>\n",
       "      <td>0.020921</td>\n",
       "      <td>0.004904</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>0.004053</td>\n",
       "      <td>0.000358</td>\n",
       "      <td>0.009724</td>\n",
       "      <td>0.029755</td>\n",
       "      <td>0.001438</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.001590</td>\n",
       "      <td>0.001839</td>\n",
       "      <td>0.003046</td>\n",
       "      <td>0.004659</td>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.003322</td>\n",
       "      <td>0.001398</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.002666</td>\n",
       "      <td>0.005919</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.002285</td>\n",
       "      <td>0.001912</td>\n",
       "      <td>0.002486</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.000688</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.001160</td>\n",
       "      <td>0.003974</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.014078</td>\n",
       "      <td>0.005252</td>\n",
       "      <td>0.005643</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.000755</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.003622</td>\n",
       "      <td>0.002113</td>\n",
       "      <td>0.000615</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.029783</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.004903</td>\n",
       "      <td>0.001895</td>\n",
       "      <td>0.002465</td>\n",
       "      <td>0.001269</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000937</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.000528</td>\n",
       "      <td>0.004899</td>\n",
       "      <td>0.002660</td>\n",
       "      <td>0.003750</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.017596</td>\n",
       "      <td>0.004040</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.001194</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.002332</td>\n",
       "      <td>0.008574</td>\n",
       "      <td>0.011224</td>\n",
       "      <td>0.050859</td>\n",
       "      <td>0.009088</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.006908</td>\n",
       "      <td>0.004541</td>\n",
       "      <td>0.001587</td>\n",
       "      <td>0.000801</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.001258</td>\n",
       "      <td>0.003887</td>\n",
       "      <td>0.004566</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.000747</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.003743</td>\n",
       "      <td>0.013898</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.000712</td>\n",
       "      <td>0.003692</td>\n",
       "      <td>0.003298</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.028316</td>\n",
       "      <td>0.001850</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>0.003707</td>\n",
       "      <td>0.003930</td>\n",
       "      <td>0.000459</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.000844</td>\n",
       "      <td>0.002643</td>\n",
       "      <td>0.002487</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.001068</td>\n",
       "      <td>0.001709</td>\n",
       "      <td>0.000828</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.001811</td>\n",
       "      <td>0.001415</td>\n",
       "      <td>0.000637</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.004296</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.001130</td>\n",
       "      <td>0.006485</td>\n",
       "      <td>0.001302</td>\n",
       "      <td>0.001997</td>\n",
       "      <td>0.003545</td>\n",
       "      <td>0.004693</td>\n",
       "      <td>0.005342</td>\n",
       "      <td>0.000875</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.002527</td>\n",
       "      <td>0.001139</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.006602</td>\n",
       "      <td>0.005396</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.004955</td>\n",
       "      <td>0.001323</td>\n",
       "      <td>0.001267</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.006163</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.004112</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.000871</td>\n",
       "      <td>0.002596</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.002834</td>\n",
       "      <td>0.000825</td>\n",
       "      <td>0.002859</td>\n",
       "      <td>0.001657</td>\n",
       "      <td>0.003142</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>0.001577</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.001753</td>\n",
       "      <td>0.001070</td>\n",
       "      <td>0.014396</td>\n",
       "      <td>0.066897</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.002519</td>\n",
       "      <td>0.004595</td>\n",
       "      <td>0.002637</td>\n",
       "      <td>0.006834</td>\n",
       "      <td>0.002389</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.001254</td>\n",
       "      <td>0.000676</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.000662</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.002141</td>\n",
       "      <td>0.002729</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.000578</td>\n",
       "      <td>0.001704</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.008780</td>\n",
       "      <td>0.003745</td>\n",
       "      <td>0.000889</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.002433</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.002304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.002083</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.017240</td>\n",
       "      <td>0.004190</td>\n",
       "      <td>0.004420</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.008443</td>\n",
       "      <td>0.016500</td>\n",
       "      <td>0.002361</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.000941</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.001805</td>\n",
       "      <td>0.004191</td>\n",
       "      <td>0.004730</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.004108</td>\n",
       "      <td>0.004911</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.003474</td>\n",
       "      <td>0.001374</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.001298</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.006666</td>\n",
       "      <td>0.003671</td>\n",
       "      <td>0.002214</td>\n",
       "      <td>0.004057</td>\n",
       "      <td>0.003204</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.000816</td>\n",
       "      <td>0.002106</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>0.001103</td>\n",
       "      <td>0.007115</td>\n",
       "      <td>0.006929</td>\n",
       "      <td>0.002565</td>\n",
       "      <td>0.010310</td>\n",
       "      <td>0.007714</td>\n",
       "      <td>0.012284</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.002306</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.004259</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.001942</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.001210</td>\n",
       "      <td>0.007174</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.002798</td>\n",
       "      <td>0.001519</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.005524</td>\n",
       "      <td>0.002158</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.000725</td>\n",
       "      <td>0.005279</td>\n",
       "      <td>0.002540</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.022745</td>\n",
       "      <td>0.005488</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.002441</td>\n",
       "      <td>0.001187</td>\n",
       "      <td>0.004740</td>\n",
       "      <td>0.038158</td>\n",
       "      <td>0.004245</td>\n",
       "      <td>0.007525</td>\n",
       "      <td>0.001571</td>\n",
       "      <td>0.000957</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.015376</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.000544</td>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>0.005865</td>\n",
       "      <td>0.006326</td>\n",
       "      <td>0.000647</td>\n",
       "      <td>0.001458</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>0.019463</td>\n",
       "      <td>0.001958</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.001301</td>\n",
       "      <td>0.003085</td>\n",
       "      <td>0.007978</td>\n",
       "      <td>0.000583</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.001315</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>0.002308</td>\n",
       "      <td>0.002411</td>\n",
       "      <td>0.003079</td>\n",
       "      <td>0.002502</td>\n",
       "      <td>0.002827</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.000726</td>\n",
       "      <td>0.001230</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.002517</td>\n",
       "      <td>0.001429</td>\n",
       "      <td>0.003370</td>\n",
       "      <td>0.000727</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.000560</td>\n",
       "      <td>0.005096</td>\n",
       "      <td>0.001869</td>\n",
       "      <td>0.001183</td>\n",
       "      <td>0.006197</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.003330</td>\n",
       "      <td>0.001500</td>\n",
       "      <td>0.005466</td>\n",
       "      <td>0.001178</td>\n",
       "      <td>0.002711</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001949</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.005111</td>\n",
       "      <td>0.003284</td>\n",
       "      <td>0.001735</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.002644</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.011893</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.003928</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.006283</td>\n",
       "      <td>0.001817</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.005220</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.002005</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>0.005146</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.001453</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.002705</td>\n",
       "      <td>0.002101</td>\n",
       "      <td>0.010018</td>\n",
       "      <td>0.010450</td>\n",
       "      <td>0.002550</td>\n",
       "      <td>0.002274</td>\n",
       "      <td>0.001711</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.016114</td>\n",
       "      <td>0.002086</td>\n",
       "      <td>0.000646</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.001754</td>\n",
       "      <td>0.003305</td>\n",
       "      <td>0.001053</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.003251</td>\n",
       "      <td>0.002524</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>0.001512</td>\n",
       "      <td>0.001169</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>0.001957</td>\n",
       "      <td>0.001566</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.001927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001017                0.001340   \n",
       "1  id_001897cda                     0.000407                0.000601   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.001022                0.001162   \n",
       "4  id_0027f1083                     0.002083                0.002052   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.003699                        0.010391   \n",
       "1        0.001691                        0.001471   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.002062                        0.013520   \n",
       "4        0.002470                        0.013300   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.014362                        0.004175   \n",
       "1                           0.000495                        0.000994   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.020921                        0.004904   \n",
       "4                           0.017240                        0.004190   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002167                       0.006994   \n",
       "1                    0.002743                       0.006281   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.003508                       0.004053   \n",
       "4                    0.004420                       0.002925   \n",
       "\n",
       "   adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                    0.000471                     0.008558   \n",
       "1                    0.008441                     0.003315   \n",
       "2                    0.000000                     0.000000   \n",
       "3                    0.000358                     0.009724   \n",
       "4                    0.000749                     0.008443   \n",
       "\n",
       "   adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                        0.011338       0.000703   \n",
       "1                        0.004406       0.003003   \n",
       "2                        0.000000       0.000000   \n",
       "3                        0.029755       0.001438   \n",
       "4                        0.016500       0.002361   \n",
       "\n",
       "   aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  analgesic  \\\n",
       "0                          0.001393       0.000627        0.001432   0.001308   \n",
       "1                          0.000363       0.013655        0.000710   0.000717   \n",
       "2                          0.000000       0.000000        0.000000   0.000000   \n",
       "3                          0.000756       0.001358        0.001590   0.001839   \n",
       "4                          0.000722       0.000941        0.001588   0.001805   \n",
       "\n",
       "   androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0                   0.002041                      0.005685   \n",
       "1                   0.000469                      0.001147   \n",
       "2                   0.000000                      0.000000   \n",
       "3                   0.003046                      0.004659   \n",
       "4                   0.004191                      0.004730   \n",
       "\n",
       "   anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0            0.006751                0.002637   \n",
       "1            0.001010                0.003252   \n",
       "2            0.000000                0.000000   \n",
       "3            0.003904                0.002306   \n",
       "4            0.002870                0.002344   \n",
       "\n",
       "   angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                         0.002864           0.004606        0.000822   \n",
       "1                         0.003597           0.001821        0.000511   \n",
       "2                         0.000000           0.000000        0.000000   \n",
       "3                         0.002427           0.002440        0.001117   \n",
       "4                         0.004108           0.004911        0.001102   \n",
       "\n",
       "   antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0    0.002810        0.000802    0.001233       0.001154      0.001996   \n",
       "1    0.000744        0.000663    0.001100       0.000747      0.000384   \n",
       "2    0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3    0.003322        0.001398    0.001065       0.002180      0.002666   \n",
       "4    0.003474        0.001374    0.001213       0.001298      0.001466   \n",
       "\n",
       "   antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0     0.003463       0.002157   0.001031             0.005534   \n",
       "1     0.001268       0.001352   0.001032             0.001334   \n",
       "2     0.000000       0.000000   0.000000             0.000000   \n",
       "3     0.005919       0.002444   0.002285             0.001912   \n",
       "4     0.006666       0.003671   0.002214             0.004057   \n",
       "\n",
       "   aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0             0.004967              0.000542   \n",
       "1             0.000840              0.001671   \n",
       "2             0.000000              0.000000   \n",
       "3             0.002486              0.001284   \n",
       "4             0.003204              0.000744   \n",
       "\n",
       "   atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                    0.000720                0.001592   \n",
       "1                                    0.000650                0.000288   \n",
       "2                                    0.000000                0.000000   \n",
       "3                                    0.000821                0.000513   \n",
       "4                                    0.000837                0.000816   \n",
       "\n",
       "   atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0          0.006512              0.000532                 0.000987   \n",
       "1          0.002014              0.002973                 0.003937   \n",
       "2          0.000000              0.000000                 0.000000   \n",
       "3          0.001771              0.000688                 0.000927   \n",
       "4          0.002106              0.000732                 0.000492   \n",
       "\n",
       "   autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0             0.000667                                   0.004276   \n",
       "1             0.001598                                   0.000694   \n",
       "2             0.000000                                   0.000000   \n",
       "3             0.001160                                   0.003974   \n",
       "4             0.001103                                   0.007115   \n",
       "\n",
       "   bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                   0.010717              0.001440   \n",
       "1                                   0.000436              0.000308   \n",
       "2                                   0.000000              0.000000   \n",
       "3                                   0.006602              0.002038   \n",
       "4                                   0.006929              0.002565   \n",
       "\n",
       "   bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                 0.007091                        0.007154   \n",
       "1                                 0.002123                        0.000708   \n",
       "2                                 0.000000                        0.000000   \n",
       "3                                 0.014078                        0.005252   \n",
       "4                                 0.010310                        0.007714   \n",
       "\n",
       "   bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                 0.005985                                0.001434   \n",
       "1                 0.000733                                0.000590   \n",
       "2                 0.000000                                0.000000   \n",
       "3                 0.005643                                0.000745   \n",
       "4                 0.012284                                0.000958   \n",
       "\n",
       "   bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0       0.003479           0.000593                         0.002486   \n",
       "1       0.000823           0.002519                         0.003602   \n",
       "2       0.000000           0.000000                         0.000000   \n",
       "3       0.000755           0.001155                         0.003622   \n",
       "4       0.002306           0.000699                         0.004259   \n",
       "\n",
       "   beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                0.002146               0.003741       0.000531   \n",
       "1                0.001301               0.005441       0.016899   \n",
       "2                0.000000               0.000000       0.000000   \n",
       "3                0.002113               0.000615       0.000985   \n",
       "4                0.002200               0.001942       0.001120   \n",
       "\n",
       "   calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0               0.000993                 0.018831   \n",
       "1               0.000548                 0.002724   \n",
       "2               0.000000                 0.000000   \n",
       "3               0.001452                 0.029783   \n",
       "4               0.001210                 0.007174   \n",
       "\n",
       "   cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                      0.001698                         0.001932   \n",
       "1                      0.002791                         0.002578   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.001814                         0.004903   \n",
       "4                      0.002586                         0.002785   \n",
       "\n",
       "   carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  caspase_activator  \\\n",
       "0                      0.002163                 0.001586           0.002490   \n",
       "1                      0.000939                 0.004493           0.000556   \n",
       "2                      0.000000                 0.000000           0.000000   \n",
       "3                      0.001895                 0.002465           0.001269   \n",
       "4                      0.003191                 0.002798           0.001519   \n",
       "\n",
       "   catechol_o_methyltransferase_inhibitor  cc_chemokine_receptor_antagonist  \\\n",
       "0                                0.001722                          0.005543   \n",
       "1                                0.000700                          0.002991   \n",
       "2                                0.000000                          0.000000   \n",
       "3                                0.001471                          0.011056   \n",
       "4                                0.001643                          0.005524   \n",
       "\n",
       "   cck_receptor_antagonist  cdk_inhibitor  chelating_agent  chk_inhibitor  \\\n",
       "0                 0.001892       0.000399         0.004165       0.000563   \n",
       "1                 0.000683       0.006828         0.001535       0.003461   \n",
       "2                 0.000000       0.000000         0.000000       0.000000   \n",
       "3                 0.001364       0.000937         0.002805       0.000528   \n",
       "4                 0.002158       0.002220         0.004346       0.000725   \n",
       "\n",
       "   chloride_channel_blocker  cholesterol_inhibitor  \\\n",
       "0                  0.002747               0.005081   \n",
       "1                  0.000620               0.004905   \n",
       "2                  0.000000               0.000000   \n",
       "3                  0.004899               0.002660   \n",
       "4                  0.005279               0.002540   \n",
       "\n",
       "   cholinergic_receptor_antagonist  coagulation_factor_inhibitor  \\\n",
       "0                         0.005517                      0.000827   \n",
       "1                         0.000463                      0.000678   \n",
       "2                         0.000000                      0.000000   \n",
       "3                         0.003750                      0.001031   \n",
       "4                         0.003548                      0.001109   \n",
       "\n",
       "   corticosteroid_agonist  cyclooxygenase_inhibitor  \\\n",
       "0                0.001082                  0.055824   \n",
       "1                0.000746                  0.003685   \n",
       "2                0.000000                  0.000000   \n",
       "3                0.000727                  0.017596   \n",
       "4                0.001430                  0.022745   \n",
       "\n",
       "   cytochrome_p450_inhibitor  dihydrofolate_reductase_inhibitor  \\\n",
       "0                   0.006146                           0.001246   \n",
       "1                   0.001439                           0.000379   \n",
       "2                   0.000000                           0.000000   \n",
       "3                   0.004040                           0.001647   \n",
       "4                   0.005488                           0.001814   \n",
       "\n",
       "   dipeptidyl_peptidase_inhibitor  diuretic  dna_alkylating_agent  \\\n",
       "0                        0.002956  0.000845              0.005736   \n",
       "1                        0.001138  0.000548              0.000583   \n",
       "2                        0.000000  0.000000              0.000000   \n",
       "3                        0.001194  0.000957              0.002332   \n",
       "4                        0.002441  0.001187              0.004740   \n",
       "\n",
       "   dna_inhibitor  dopamine_receptor_agonist  dopamine_receptor_antagonist  \\\n",
       "0       0.017876                   0.008564                      0.012604   \n",
       "1       0.000496                   0.000870                      0.000694   \n",
       "2       0.000000                   0.000000                      0.000000   \n",
       "3       0.008574                   0.011224                      0.050859   \n",
       "4       0.038158                   0.004245                      0.007525   \n",
       "\n",
       "   egfr_inhibitor  elastase_inhibitor  erbb2_inhibitor  \\\n",
       "0        0.000449            0.001486         0.000673   \n",
       "1        0.001402            0.000864         0.000772   \n",
       "2        0.000000            0.000000         0.000000   \n",
       "3        0.009088            0.000902         0.000872   \n",
       "4        0.001571            0.000957         0.000800   \n",
       "\n",
       "   estrogen_receptor_agonist  estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                   0.017718                      0.001815        0.007837   \n",
       "1                   0.001613                      0.001947        0.006391   \n",
       "2                   0.000000                      0.000000        0.000000   \n",
       "3                   0.006908                      0.004541        0.001587   \n",
       "4                   0.015376                      0.001957        0.001182   \n",
       "\n",
       "   farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  fgfr_inhibitor  \\\n",
       "0                       0.000986                     0.001493        0.000565   \n",
       "1                       0.000414                     0.002920        0.012295   \n",
       "2                       0.000000                     0.000000        0.000000   \n",
       "3                       0.000801                     0.001081        0.001717   \n",
       "4                       0.000967                     0.001969        0.001199   \n",
       "\n",
       "   flt3_inhibitor  focal_adhesion_kinase_inhibitor  free_radical_scavenger  \\\n",
       "0        0.000339                         0.001530                0.001627   \n",
       "1        0.004076                         0.000734                0.000694   \n",
       "2        0.000000                         0.000000                0.000000   \n",
       "3        0.001091                         0.000510                0.001486   \n",
       "4        0.000773                         0.000544                0.001634   \n",
       "\n",
       "   fungal_squalene_epoxidase_inhibitor  gaba_receptor_agonist  \\\n",
       "0                             0.002181               0.022570   \n",
       "1                             0.000529               0.000749   \n",
       "2                             0.000000               0.000000   \n",
       "3                             0.001258               0.003887   \n",
       "4                             0.001030               0.005865   \n",
       "\n",
       "   gaba_receptor_antagonist  gamma_secretase_inhibitor  \\\n",
       "0                  0.021721                   0.002108   \n",
       "1                  0.003137                   0.001459   \n",
       "2                  0.000000                   0.000000   \n",
       "3                  0.004566                   0.001084   \n",
       "4                  0.006326                   0.000647   \n",
       "\n",
       "   glucocorticoid_receptor_agonist  glutamate_inhibitor  \\\n",
       "0                         0.001585             0.001485   \n",
       "1                         0.000905             0.001077   \n",
       "2                         0.000000             0.000000   \n",
       "3                         0.000747             0.001323   \n",
       "4                         0.001458             0.002379   \n",
       "\n",
       "   glutamate_receptor_agonist  glutamate_receptor_antagonist  \\\n",
       "0                    0.008050                       0.023278   \n",
       "1                    0.000607                       0.003128   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.003743                       0.013898   \n",
       "4                    0.004645                       0.019463   \n",
       "\n",
       "   gonadotropin_receptor_agonist  gsk_inhibitor  hcv_inhibitor  \\\n",
       "0                       0.002046       0.001040       0.009450   \n",
       "1                       0.001188       0.004560       0.002481   \n",
       "2                       0.000000       0.000000       0.000000   \n",
       "3                       0.002092       0.000712       0.003692   \n",
       "4                       0.001958       0.001104       0.003836   \n",
       "\n",
       "   hdac_inhibitor  histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0        0.002073                    0.005162                       0.007772   \n",
       "1        0.001363                    0.000451                       0.001431   \n",
       "2        0.000000                    0.000000                       0.000000   \n",
       "3        0.003298                    0.003862                       0.028316   \n",
       "4        0.001301                    0.003085                       0.007978   \n",
       "\n",
       "   histone_lysine_demethylase_inhibitor  \\\n",
       "0                              0.000768   \n",
       "1                              0.002153   \n",
       "2                              0.000000   \n",
       "3                              0.001850   \n",
       "4                              0.000583   \n",
       "\n",
       "   histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  hmgcr_inhibitor  \\\n",
       "0                                    0.001257       0.004546         0.001078   \n",
       "1                                    0.004140       0.000911         0.001327   \n",
       "2                                    0.000000       0.000000         0.000000   \n",
       "3                                    0.003487       0.003707         0.003930   \n",
       "4                                    0.001352       0.003749         0.000769   \n",
       "\n",
       "   hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0       0.001151         0.000856       0.001545   \n",
       "1       0.000966         0.007523       0.001119   \n",
       "2       0.000000         0.000000       0.000000   \n",
       "3       0.000459         0.001497       0.000844   \n",
       "4       0.001315         0.000510       0.002308   \n",
       "\n",
       "   imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                      0.002979           0.002125              0.003197   \n",
       "1                      0.001489           0.001252              0.004194   \n",
       "2                      0.000000           0.000000              0.000000   \n",
       "3                      0.002643           0.002487              0.001735   \n",
       "4                      0.002411           0.003079              0.002502   \n",
       "\n",
       "   insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0            0.001527            0.004500       0.001415       0.000302   \n",
       "1            0.044226            0.007635       0.004510       0.002743   \n",
       "2            0.000000            0.000000       0.000000       0.000000   \n",
       "3            0.001068            0.001709       0.000828       0.001188   \n",
       "4            0.002827            0.001736       0.000826       0.000726   \n",
       "\n",
       "   laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0  0.000996               0.000954                         0.005075   \n",
       "1  0.000737               0.000692                         0.003319   \n",
       "2  0.000000               0.000000                         0.000000   \n",
       "3  0.000990               0.001027                         0.001603   \n",
       "4  0.001230               0.001189                         0.002517   \n",
       "\n",
       "   lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0          0.001878                0.003400     0.001488       0.000834   \n",
       "1          0.001079                0.002228     0.000739       0.000593   \n",
       "2          0.000000                0.000000     0.000000       0.000000   \n",
       "3          0.001426                0.001811     0.001415       0.000637   \n",
       "4          0.001429                0.003370     0.000727       0.000947   \n",
       "\n",
       "   mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0       0.000773                      0.008768   \n",
       "1       0.002256                      0.001408   \n",
       "2       0.000000                      0.000000   \n",
       "3       0.001169                      0.004296   \n",
       "4       0.000560                      0.005096   \n",
       "\n",
       "   mineralocorticoid_receptor_antagonist  monoacylglycerol_lipase_inhibitor  \\\n",
       "0                               0.002825                           0.002147   \n",
       "1                               0.000836                           0.000627   \n",
       "2                               0.000000                           0.000000   \n",
       "3                               0.003246                           0.001130   \n",
       "4                               0.001869                           0.001183   \n",
       "\n",
       "   monoamine_oxidase_inhibitor  monopolar_spindle_1_kinase_inhibitor  \\\n",
       "0                     0.004123                              0.000906   \n",
       "1                     0.001101                              0.000697   \n",
       "2                     0.000000                              0.000000   \n",
       "3                     0.006485                              0.001302   \n",
       "4                     0.006197                              0.001001   \n",
       "\n",
       "   mtor_inhibitor  mucolytic_agent  neuropeptide_receptor_antagonist  \\\n",
       "0        0.000795         0.005056                          0.000778   \n",
       "1        0.003892         0.000541                          0.000932   \n",
       "2        0.000000         0.000000                          0.000000   \n",
       "3        0.001997         0.003545                          0.004693   \n",
       "4        0.002191         0.003330                          0.001500   \n",
       "\n",
       "   nfkb_inhibitor  nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0        0.005332                    0.001117            0.001277   \n",
       "1        0.000626                    0.000636            0.000514   \n",
       "2        0.000000                    0.000000            0.000000   \n",
       "3        0.005342                    0.000875            0.002535   \n",
       "4        0.005466                    0.001178            0.002711   \n",
       "\n",
       "   nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                           0.001157                         0.001245   \n",
       "1                           0.000842                         0.000467   \n",
       "2                           0.000000                         0.000000   \n",
       "3                           0.000826                         0.002527   \n",
       "4                           0.001454                         0.001949   \n",
       "\n",
       "   norepinephrine_reuptake_inhibitor  nrf2_activator  opioid_receptor_agonist  \\\n",
       "0                           0.000785        0.001269                 0.002011   \n",
       "1                           0.000710        0.000481                 0.000658   \n",
       "2                           0.000000        0.000000                 0.000000   \n",
       "3                           0.001139        0.000582                 0.006602   \n",
       "4                           0.001121        0.001226                 0.003458   \n",
       "\n",
       "   opioid_receptor_antagonist  orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                    0.005175                    0.001579            0.000524   \n",
       "1                    0.001284                    0.001163            0.001139   \n",
       "2                    0.000000                    0.000000            0.000000   \n",
       "3                    0.005396                    0.003000            0.004955   \n",
       "4                    0.005111                    0.003284            0.001735   \n",
       "\n",
       "   p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  pdk_inhibitor  \\\n",
       "0                  0.001119        0.002616         0.000247       0.001464   \n",
       "1                  0.005373        0.003838         0.004854       0.001700   \n",
       "2                  0.000000        0.000000         0.000000       0.000000   \n",
       "3                  0.001323        0.001267         0.000998       0.000910   \n",
       "4                  0.001390        0.002644         0.000963       0.001332   \n",
       "\n",
       "   phosphodiesterase_inhibitor  phospholipase_inhibitor  pi3k_inhibitor  \\\n",
       "0                     0.015000                 0.002421        0.002055   \n",
       "1                     0.014334                 0.000578        0.017689   \n",
       "2                     0.000000                 0.000000        0.000000   \n",
       "3                     0.006163                 0.002450        0.004112   \n",
       "4                     0.011893                 0.001842        0.002887   \n",
       "\n",
       "   pkc_inhibitor  potassium_channel_activator  potassium_channel_antagonist  \\\n",
       "0       0.001201                     0.003048                      0.010169   \n",
       "1       0.003486                     0.002171                      0.001593   \n",
       "2       0.000000                     0.000000                      0.000000   \n",
       "3       0.001219                     0.004898                      0.007492   \n",
       "4       0.001566                     0.005145                      0.003928   \n",
       "\n",
       "   ppar_receptor_agonist  ppar_receptor_antagonist  \\\n",
       "0               0.002934                  0.003232   \n",
       "1               0.044096                  0.016302   \n",
       "2               0.000000                  0.000000   \n",
       "3               0.001373                  0.001408   \n",
       "4               0.005571                  0.001901   \n",
       "\n",
       "   progesterone_receptor_agonist  progesterone_receptor_antagonist  \\\n",
       "0                       0.010857                          0.002476   \n",
       "1                       0.000512                          0.016848   \n",
       "2                       0.000000                          0.000000   \n",
       "3                       0.001883                          0.000871   \n",
       "4                       0.006283                          0.001817   \n",
       "\n",
       "   prostaglandin_inhibitor  prostanoid_receptor_antagonist  \\\n",
       "0                 0.006286                        0.008062   \n",
       "1                 0.000974                        0.001644   \n",
       "2                 0.000000                        0.000000   \n",
       "3                 0.002596                        0.004307   \n",
       "4                 0.003387                        0.005220   \n",
       "\n",
       "   proteasome_inhibitor  protein_kinase_inhibitor  \\\n",
       "0              0.000679                  0.005327   \n",
       "1              0.001124                  0.001922   \n",
       "2              0.000000                  0.000000   \n",
       "3              0.000921                  0.002834   \n",
       "4              0.001077                  0.002005   \n",
       "\n",
       "   protein_phosphatase_inhibitor  protein_synthesis_inhibitor  \\\n",
       "0                       0.000757                     0.003248   \n",
       "1                       0.000691                     0.000472   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.000825                     0.002859   \n",
       "4                       0.001177                     0.004902   \n",
       "\n",
       "   protein_tyrosine_kinase_inhibitor  radiopaque_medium  raf_inhibitor  \\\n",
       "0                           0.000908           0.003469       0.000726   \n",
       "1                           0.001676           0.000520       0.000308   \n",
       "2                           0.000000           0.000000       0.000000   \n",
       "3                           0.001657           0.003142       0.001943   \n",
       "4                           0.001572           0.005146       0.002506   \n",
       "\n",
       "   ras_gtpase_inhibitor  retinoid_receptor_agonist  \\\n",
       "0              0.001328                   0.002779   \n",
       "1              0.001339                   0.002244   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.001577                   0.000531   \n",
       "4              0.000817                   0.000997   \n",
       "\n",
       "   retinoid_receptor_antagonist  rho_associated_kinase_inhibitor  \\\n",
       "0                      0.000871                         0.000891   \n",
       "1                      0.001683                         0.029939   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.001117                         0.000838   \n",
       "4                      0.001453                         0.002477   \n",
       "\n",
       "   ribonucleoside_reductase_inhibitor  rna_polymerase_inhibitor  \\\n",
       "0                            0.001412                  0.002197   \n",
       "1                            0.000776                  0.002489   \n",
       "2                            0.000000                  0.000000   \n",
       "3                            0.001753                  0.001070   \n",
       "4                            0.002705                  0.002101   \n",
       "\n",
       "   serotonin_receptor_agonist  serotonin_receptor_antagonist  \\\n",
       "0                    0.009240                       0.004762   \n",
       "1                    0.006110                       0.000666   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.014396                       0.066897   \n",
       "4                    0.010018                       0.010450   \n",
       "\n",
       "   serotonin_reuptake_inhibitor  sigma_receptor_agonist  \\\n",
       "0                      0.004286                0.003893   \n",
       "1                      0.000474                0.000777   \n",
       "2                      0.000000                0.000000   \n",
       "3                      0.002809                0.002519   \n",
       "4                      0.002550                0.002274   \n",
       "\n",
       "   sigma_receptor_antagonist  smoothened_receptor_antagonist  \\\n",
       "0                   0.001235                        0.001594   \n",
       "1                   0.000705                        0.002467   \n",
       "2                   0.000000                        0.000000   \n",
       "3                   0.004595                        0.002637   \n",
       "4                   0.001711                        0.001821   \n",
       "\n",
       "   sodium_channel_inhibitor  sphingosine_receptor_agonist  src_inhibitor  \\\n",
       "0                  0.021606                      0.003533       0.000407   \n",
       "1                  0.003256                      0.000513       0.017514   \n",
       "2                  0.000000                      0.000000       0.000000   \n",
       "3                  0.006834                      0.002389       0.001972   \n",
       "4                  0.016114                      0.002086       0.000646   \n",
       "\n",
       "    steroid  syk_inhibitor  tachykinin_antagonist  \\\n",
       "0  0.001003       0.000695               0.001402   \n",
       "1  0.000804       0.003687               0.002645   \n",
       "2  0.000000       0.000000               0.000000   \n",
       "3  0.001254       0.000676               0.004087   \n",
       "4  0.001416       0.001754               0.003305   \n",
       "\n",
       "   tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                     0.000272            0.000825   \n",
       "1                     0.005267            0.000568   \n",
       "2                     0.000000            0.000000   \n",
       "3                     0.000662            0.001963   \n",
       "4                     0.001053            0.001394   \n",
       "\n",
       "   thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  tnf_inhibitor  \\\n",
       "0                        0.001651     0.001649        0.001297       0.005084   \n",
       "1                        0.000190     0.000981        0.001510       0.002423   \n",
       "2                        0.000000     0.000000        0.000000       0.000000   \n",
       "3                        0.002141     0.002729        0.001006       0.001301   \n",
       "4                        0.003251     0.002524        0.001195       0.001596   \n",
       "\n",
       "   topoisomerase_inhibitor  transient_receptor_potential_channel_antagonist  \\\n",
       "0                 0.000719                                         0.001336   \n",
       "1                 0.003256                                         0.001386   \n",
       "2                 0.000000                                         0.000000   \n",
       "3                 0.000578                                         0.001704   \n",
       "4                 0.002692                                         0.001512   \n",
       "\n",
       "   tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                               0.001293      0.003830         0.004720   \n",
       "1                               0.000968      0.000493         0.004298   \n",
       "2                               0.000000      0.000000         0.000000   \n",
       "3                               0.001005      0.001203         0.002461   \n",
       "4                               0.001169      0.001012         0.004528   \n",
       "\n",
       "   tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0           0.001045                   0.000551   \n",
       "1           0.000151                   0.007047   \n",
       "2           0.000000                   0.000000   \n",
       "3           0.008780                   0.003745   \n",
       "4           0.001957                   0.001566   \n",
       "\n",
       "   ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                               0.001057         0.000797   0.001933   \n",
       "1                               0.000454         0.010448   0.001022   \n",
       "2                               0.000000         0.000000   0.000000   \n",
       "3                               0.000889         0.001287   0.002433   \n",
       "4                               0.001249         0.001583   0.002423   \n",
       "\n",
       "   vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0.022528       0.001695  \n",
       "1                    0.001647       0.002949  \n",
       "2                    0.000000       0.000000  \n",
       "3                    0.000727       0.002304  \n",
       "4                    0.000587       0.001927  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = sample_submission.drop(columns=target_cols)\\\n",
    ".merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    ".fillna(0.0).reset_index(drop=True)\n",
    "name_sub = 'submission.csv'\n",
    "submission.to_csv(name_sub, index=False)\n",
    "print(\"RESNET 30nov\")\n",
    "submission_RESNET_30nov = submission.copy()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081927,
     "end_time": "2020-12-06T00:04:00.806089",
     "exception": false,
     "start_time": "2020-12-06T00:04:00.724162",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Resnet del 28 de noviembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:00.975181Z",
     "iopub.status.busy": "2020-12-06T00:04:00.974611Z",
     "iopub.status.idle": "2020-12-06T00:04:00.978774Z",
     "shell.execute_reply": "2020-12-06T00:04:00.978316Z"
    },
    "papermill": {
     "duration": 0.090257,
     "end_time": "2020-12-06T00:04:00.978869",
     "exception": false,
     "start_time": "2020-12-06T00:04:00.888612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directorio = '../input/resnetrandom28nov'\n",
    "# folds_cp = folds.copy()\n",
    "# feature_cols = feature_cols_ini_nocps.copy()\n",
    "# NFOLDS = CFG.num_folds\n",
    "# num_features=len(feature_cols)\n",
    "# num_targets=len(target_cols)\n",
    "\n",
    "# DEVICE = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# EPOCHS = 25\n",
    "# BATCH_SIZE = 128\n",
    "# LEARNING_RATE = 1e-3\n",
    "# WEIGHT_DECAY = 1e-5\n",
    "# EARLY_STOPPING_STEPS = 10\n",
    "# EARLY_STOP = False\n",
    "\n",
    "# res = []\n",
    "# # Averaging on multiple SEEDS\n",
    "# oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "# predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "# losses_list = []\n",
    "# # SEED = [[0,3],[1,3],[2,2],[3,0]]\n",
    "# # for seed_fold, seed_run in tqdm(SEED):\n",
    "# SEED = np.arange(16)\n",
    "# for seed in tqdm(SEED):\n",
    "#     seed_fold = seed // 4\n",
    "#     seed_run = seed % 4\n",
    "#     predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "#     predictions += predictions_\n",
    "\n",
    "# # FINAL CV LOGLOSS\n",
    "# for col in target_cols:\n",
    "#     test_noctl[col] = 0.0\n",
    "# test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:01.147898Z",
     "iopub.status.busy": "2020-12-06T00:04:01.146418Z",
     "iopub.status.idle": "2020-12-06T00:04:01.148839Z",
     "shell.execute_reply": "2020-12-06T00:04:01.149440Z"
    },
    "papermill": {
     "duration": 0.089206,
     "end_time": "2020-12-06T00:04:01.149551",
     "exception": false,
     "start_time": "2020-12-06T00:04:01.060345",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = sample_submission.drop(columns=target_cols)\\\n",
    "# .merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    "# .fillna(0.0).reset_index(drop=True)\n",
    "# # sub.to_csv('submission.csv', index=False)\n",
    "# # name_sub = 'submission.csv'\n",
    "# # submission.to_csv(name_sub, index=False)\n",
    "# # print(name_sub)\n",
    "# print(\"RESNET 28nov\")\n",
    "# submission_RESNET_28nov = submission.copy()\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.081349,
     "end_time": "2020-12-06T00:04:01.313029",
     "exception": false,
     "start_time": "2020-12-06T00:04:01.231680",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.081451,
     "end_time": "2020-12-06T00:04:01.476158",
     "exception": false,
     "start_time": "2020-12-06T00:04:01.394707",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. TABNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:01.643743Z",
     "iopub.status.busy": "2020-12-06T00:04:01.642990Z",
     "iopub.status.idle": "2020-12-06T00:04:01.645976Z",
     "shell.execute_reply": "2020-12-06T00:04:01.645526Z"
    },
    "papermill": {
     "duration": 0.088299,
     "end_time": "2020-12-06T00:04:01.646083",
     "exception": false,
     "start_time": "2020-12-06T00:04:01.557784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys.path.insert(0, \"../input/tabnetdevelop/tabnet-develop\")\n",
    "seed_everything(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:01.816974Z",
     "iopub.status.busy": "2020-12-06T00:04:01.816249Z",
     "iopub.status.idle": "2020-12-06T00:04:01.819208Z",
     "shell.execute_reply": "2020-12-06T00:04:01.818728Z"
    },
    "papermill": {
     "duration": 0.090637,
     "end_time": "2020-12-06T00:04:01.819319",
     "exception": false,
     "start_time": "2020-12-06T00:04:01.728682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def process_data(data):\n",
    "    data = pd.get_dummies(data, columns=['cp_time','cp_dose'])\n",
    "    return data\n",
    "\n",
    "def process_data_tabnet(data):\n",
    "    data.loc[:, 'cp_time'] = data.loc[:, 'cp_time'].map({24: 0, 48: 1, 72: 2})\n",
    "    data.loc[:, 'cp_dose'] = data.loc[:, 'cp_dose'].map({'D1': 0, 'D2': 1})\n",
    "    return data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:01.991347Z",
     "iopub.status.busy": "2020-12-06T00:04:01.990480Z",
     "iopub.status.idle": "2020-12-06T00:04:01.992722Z",
     "shell.execute_reply": "2020-12-06T00:04:01.993175Z"
    },
    "papermill": {
     "duration": 0.091065,
     "end_time": "2020-12-06T00:04:01.993309",
     "exception": false,
     "start_time": "2020-12-06T00:04:01.902244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# folds = process_data_tabnet(folds_novar_enc)\n",
    "# test_noctl = process_data_tabnet(test_noctl_novar_enc)\n",
    "# print(folds.shape, test_noctl.shape)\n",
    "\n",
    "# del folds_novar_enc\n",
    "# del test_noctl_novar_enc\n",
    "# gc.collect()\n",
    "\n",
    "# feature_cols = [c for c in folds.columns if c not in target_cols]\n",
    "# if not CFG.original_feats: feature_cols = [c for c in feature_cols if c not in GENES+CELLS]\n",
    "# feature_cols_ini = [c for c in feature_cols if c not in ['kfold','sig_id','cp_time','cp_dose']] #, 'cp_time_24', 'cp_time_48', 'cp_time_72', 'cp_dose_D1', 'cp_dose_D2']] #,'cp_dose','cp_time']]\n",
    "# len(feature_cols_ini)\n",
    "# categorical_feats = ['cp_time','cp_dose']\n",
    "# numeric_feats = [c for c in feature_cols_ini if c not in ['cp_time','cp_dose']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:02.163983Z",
     "iopub.status.busy": "2020-12-06T00:04:02.163116Z",
     "iopub.status.idle": "2020-12-06T00:04:02.166285Z",
     "shell.execute_reply": "2020-12-06T00:04:02.165784Z"
    },
    "papermill": {
     "duration": 0.089291,
     "end_time": "2020-12-06T00:04:02.166385",
     "exception": false,
     "start_time": "2020-12-06T00:04:02.077094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Standarize Numerical Features\n",
    "# for colname in numeric_feats:\n",
    "#     valor_fold = folds[colname].values\n",
    "#     valor_tst = test_noctl[colname].values\n",
    "#     mean_v = np.mean(valor_fold)\n",
    "#     std_v = np.std(valor_fold)\n",
    "#     if std_v==0:\n",
    "#         std_v=1e-5\n",
    "#     folds[colname] = (valor_fold-mean_v)/std_v\n",
    "#     test_noctl[colname] = (valor_tst-mean_v)/std_v\n",
    "#     print(colname, mean_v, std_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:02.342820Z",
     "iopub.status.busy": "2020-12-06T00:04:02.342101Z",
     "iopub.status.idle": "2020-12-06T00:04:02.396447Z",
     "shell.execute_reply": "2020-12-06T00:04:02.395800Z"
    },
    "papermill": {
     "duration": 0.146499,
     "end_time": "2020-12-06T00:04:02.396548",
     "exception": false,
     "start_time": "2020-12-06T00:04:02.250049",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "## TABNET https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-training\n",
    "#from pytorch_tabnet.tab_model import TabModel\n",
    "from scipy.sparse import csc_matrix\n",
    "import time\n",
    "from abc import abstractmethod\n",
    "from pytorch_tabnet import tab_network\n",
    "from pytorch_tabnet.multiclass_utils import unique_labels\n",
    "from sklearn.metrics import roc_auc_score, mean_squared_error, accuracy_score\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from pytorch_tabnet.utils import (PredictDataset,\n",
    "                                  create_dataloaders,\n",
    "                                  create_explain_matrix)\n",
    "from sklearn.base import BaseEstimator\n",
    "from torch.utils.data import DataLoader\n",
    "from copy import deepcopy\n",
    "import io\n",
    "import json\n",
    "import shutil\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:02.569018Z",
     "iopub.status.busy": "2020-12-06T00:04:02.568243Z",
     "iopub.status.idle": "2020-12-06T00:04:02.571184Z",
     "shell.execute_reply": "2020-12-06T00:04:02.570706Z"
    },
    "papermill": {
     "duration": 0.092615,
     "end_time": "2020-12-06T00:04:02.571298",
     "exception": false,
     "start_time": "2020-12-06T00:04:02.478683",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def evals(model, X, y, verbose=True):\n",
    "#     with torch.no_grad():\n",
    "#         y_preds = model.predict(X)\n",
    "#         y_preds = torch.clamp(y_preds, 0.0, 1.0).detach().numpy()\n",
    "#     score = log_loss_multi(y, y_preds)\n",
    "#     #print(\"Logloss = \", score)\n",
    "#     return y_preds, score\n",
    "\n",
    "\n",
    "# def inference_fn(model, X ,verbose=True):\n",
    "#     with torch.no_grad():\n",
    "#         y_preds = model.predict( X )\n",
    "#         y_preds = torch.sigmoid(torch.as_tensor(y_preds)).numpy()\n",
    "#     return y_preds\n",
    "\n",
    "\n",
    "# def log_loss_score(actual, predicted,  eps=1e-15):\n",
    "\n",
    "#         \"\"\"\n",
    "#         :param predicted:   The predicted probabilities as floats between 0-1\n",
    "#         :param actual:      The binary labels. Either 0 or 1.\n",
    "#         :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n",
    "#         :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n",
    "#         \"\"\"\n",
    "\n",
    "        \n",
    "#         p1 = actual * np.log(predicted+eps)\n",
    "#         p0 = (1-actual) * np.log(1-predicted+eps)\n",
    "#         loss = p0 + p1\n",
    "\n",
    "#         return -loss.mean()\n",
    "    \n",
    "# def log_loss_multi(y_true, y_pred):\n",
    "#     M = y_true.shape[1]\n",
    "#     results = np.zeros(M)\n",
    "#     for i in range(M):\n",
    "#         results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n",
    "#     return results.mean()\n",
    "\n",
    "def check_targets(targets):\n",
    "    ### check if targets are all binary in training set\n",
    "    \n",
    "    for i in range(targets.shape[1]):\n",
    "        if len(np.unique(targets[:,i])) != 2:\n",
    "            return False\n",
    "    return True\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:02.772830Z",
     "iopub.status.busy": "2020-12-06T00:04:02.756893Z",
     "iopub.status.idle": "2020-12-06T00:04:02.856969Z",
     "shell.execute_reply": "2020-12-06T00:04:02.856477Z"
    },
    "papermill": {
     "duration": 0.203151,
     "end_time": "2020-12-06T00:04:02.857070",
     "exception": false,
     "start_time": "2020-12-06T00:04:02.653919",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TabModel(BaseEstimator):\n",
    "    def __init__(self, n_d=8, n_a=8, n_steps=3, gamma=1.3, cat_idxs=[], cat_dims=[], cat_emb_dim=1,\n",
    "                 n_independent=2, n_shared=2, epsilon=1e-15,  momentum=0.02,\n",
    "                 lambda_sparse=1e-3, seed=0,\n",
    "                 clip_value=1, verbose=1,\n",
    "                 optimizer_fn=torch.optim.Adam,\n",
    "                 optimizer_params=dict(lr=2e-2),\n",
    "                 scheduler_params=None, scheduler_fn=None,\n",
    "                 mask_type=\"sparsemax\",\n",
    "                 input_dim=None, output_dim=None,\n",
    "                 device_name='auto'):\n",
    "        \"\"\" Class for TabNet model\n",
    "        Parameters\n",
    "        ----------\n",
    "            device_name: str\n",
    "                'cuda' if running on GPU, 'cpu' if not, 'auto' to autodetect\n",
    "        \"\"\"\n",
    "\n",
    "        self.n_d = n_d\n",
    "        self.n_a = n_a\n",
    "        self.n_steps = n_steps\n",
    "        self.gamma = gamma\n",
    "        self.cat_idxs = cat_idxs\n",
    "        self.cat_dims = cat_dims\n",
    "        self.cat_emb_dim = cat_emb_dim\n",
    "        self.n_independent = n_independent\n",
    "        self.n_shared = n_shared\n",
    "        self.epsilon = epsilon\n",
    "        self.momentum = momentum\n",
    "        self.lambda_sparse = lambda_sparse\n",
    "        self.clip_value = clip_value\n",
    "        self.verbose = verbose\n",
    "        self.optimizer_fn = optimizer_fn\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.device_name = device_name\n",
    "        self.scheduler_params = scheduler_params\n",
    "        self.scheduler_fn = scheduler_fn\n",
    "        self.mask_type = mask_type\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        self.batch_size = 1024\n",
    "\n",
    "        self.seed = seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        # Defining device\n",
    "        if device_name == 'auto':\n",
    "            if torch.cuda.is_available():\n",
    "                device_name = 'cuda'\n",
    "            else:\n",
    "                device_name = 'cpu'\n",
    "        self.device = torch.device(device_name)\n",
    "        print(f\"Device used : {self.device}\")\n",
    "\n",
    "    @abstractmethod\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid,\n",
    "                          weights, batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define construct_loaders to use this base class')\n",
    "\n",
    "    def init_network(\n",
    "                     self,\n",
    "                     input_dim,\n",
    "                     output_dim,\n",
    "                     n_d,\n",
    "                     n_a,\n",
    "                     n_steps,\n",
    "                     gamma,\n",
    "                     cat_idxs,\n",
    "                     cat_dims,\n",
    "                     cat_emb_dim,\n",
    "                     n_independent,\n",
    "                     n_shared,\n",
    "                     epsilon,\n",
    "                     virtual_batch_size,\n",
    "                     momentum,\n",
    "                     device_name,\n",
    "                     mask_type,\n",
    "                     ):\n",
    "        self.network = tab_network.TabNet(\n",
    "            input_dim,\n",
    "            output_dim,\n",
    "            n_d=n_d,\n",
    "            n_a=n_a,\n",
    "            n_steps=n_steps,\n",
    "            gamma=gamma,\n",
    "            cat_idxs=cat_idxs,\n",
    "            cat_dims=cat_dims,\n",
    "            cat_emb_dim=cat_emb_dim,\n",
    "            n_independent=n_independent,\n",
    "            n_shared=n_shared,\n",
    "            epsilon=epsilon,\n",
    "            virtual_batch_size=virtual_batch_size,\n",
    "            momentum=momentum,\n",
    "            device_name=device_name,\n",
    "            mask_type=mask_type).to(self.device)\n",
    "\n",
    "        self.reducing_matrix = create_explain_matrix(\n",
    "            self.network.input_dim,\n",
    "            self.network.cat_emb_dim,\n",
    "            self.network.cat_idxs,\n",
    "            self.network.post_embed_dim)\n",
    "\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, loss_fn=None,\n",
    "            weights=0, max_epochs=100, patience=10, batch_size=1024,\n",
    "            virtual_batch_size=128, num_workers=0, drop_last=False):\n",
    "        \"\"\"Train a neural network stored in self.network\n",
    "        Using train_dataloader for training data and\n",
    "        valid_dataloader for validation.\n",
    "        Parameters\n",
    "        ----------\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            X_train: np.ndarray\n",
    "                Train set\n",
    "            y_train : np.array\n",
    "                Train targets\n",
    "            weights : bool or dictionnary\n",
    "                0 for no balancing\n",
    "                1 for automated balancing\n",
    "                dict for custom weights per class\n",
    "            max_epochs : int\n",
    "                Maximum number of epochs during training\n",
    "            patience : int\n",
    "                Number of consecutive non improving epoch before early stopping\n",
    "            batch_size : int\n",
    "                Training batch size\n",
    "            virtual_batch_size : int\n",
    "                Batch size for Ghost Batch Normalization (virtual_batch_size < batch_size)\n",
    "            num_workers : int\n",
    "                Number of workers used in torch.utils.data.DataLoader\n",
    "            drop_last : bool\n",
    "                Whether to drop last batch during training\n",
    "        \"\"\"\n",
    "        # update model name\n",
    "\n",
    "        self.update_fit_params(X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                               weights, max_epochs, patience, batch_size,\n",
    "                               virtual_batch_size, num_workers, drop_last)\n",
    "\n",
    "        train_dataloader, valid_dataloader = self.construct_loaders(X_train,\n",
    "                                                                    y_train,\n",
    "                                                                    X_valid,\n",
    "                                                                    y_valid,\n",
    "                                                                    self.updated_weights,\n",
    "                                                                    self.batch_size,\n",
    "                                                                    self.num_workers,\n",
    "                                                                    self.drop_last)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=self.virtual_batch_size,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "\n",
    "        self.optimizer = self.optimizer_fn(self.network.parameters(),\n",
    "                                           **self.optimizer_params)\n",
    "\n",
    "        if self.scheduler_fn:\n",
    "            self.scheduler = self.scheduler_fn(self.optimizer, **self.scheduler_params)\n",
    "        else:\n",
    "            self.scheduler = None\n",
    "\n",
    "        self.losses_train = []\n",
    "        self.losses_valid = []\n",
    "        self.learning_rates = []\n",
    "        self.metrics_train = []\n",
    "        self.metrics_valid = []\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            print(\"Will train until validation stopping metric\",\n",
    "                  f\"hasn't improved in {self.patience} rounds.\")\n",
    "            msg_epoch = f'| EPOCH |  train  |   valid  | total time (s)'\n",
    "            print('---------------------------------------')\n",
    "            print(msg_epoch)\n",
    "\n",
    "        total_time = 0\n",
    "        while (self.epoch < self.max_epochs and\n",
    "               self.patience_counter < self.patience):\n",
    "            starting_time = time.time()\n",
    "            # updates learning rate history\n",
    "            self.learning_rates.append(self.optimizer.param_groups[-1][\"lr\"])\n",
    "\n",
    "            fit_metrics = self.fit_epoch(train_dataloader, valid_dataloader)\n",
    "\n",
    "            # leaving it here, may be used for callbacks later\n",
    "            self.losses_train.append(fit_metrics['train']['loss_avg'])\n",
    "            self.losses_valid.append(fit_metrics['valid']['total_loss'])\n",
    "            self.metrics_train.append(fit_metrics['train']['stopping_loss'])\n",
    "            self.metrics_valid.append(fit_metrics['valid']['stopping_loss'])\n",
    "\n",
    "            stopping_loss = fit_metrics['valid']['stopping_loss']\n",
    "            if stopping_loss < self.best_cost:\n",
    "                self.best_cost = stopping_loss\n",
    "                self.patience_counter = 0\n",
    "                # Saving model\n",
    "                self.best_network = deepcopy(self.network)\n",
    "                has_improved = True\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "                has_improved=False\n",
    "            self.epoch += 1\n",
    "            total_time += time.time() - starting_time\n",
    "            if self.verbose > 0:\n",
    "                if self.epoch % self.verbose == 0:\n",
    "                    separator = \"|\"\n",
    "                    msg_epoch = f\"| {self.epoch:<5} | \"\n",
    "                    msg_epoch += f\" {fit_metrics['train']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {fit_metrics['valid']['stopping_loss']:.5f}\"\n",
    "                    msg_epoch += f' {separator:<2} '\n",
    "                    msg_epoch += f\" {np.round(total_time, 1):<10}\"\n",
    "                    msg_epoch += f\" {has_improved}\"\n",
    "                    print(msg_epoch)\n",
    "\n",
    "        if self.verbose > 0:\n",
    "            if self.patience_counter == self.patience:\n",
    "                print(f\"Early stopping occured at epoch {self.epoch}\")\n",
    "            print(f\"Training done in {total_time:.3f} seconds.\")\n",
    "            print('---------------------------------------')\n",
    "\n",
    "        self.history = {\"train\": {\"loss\": self.losses_train,\n",
    "                                  \"metric\": self.metrics_train,\n",
    "                                  \"lr\": self.learning_rates},\n",
    "                        \"valid\": {\"loss\": self.losses_valid,\n",
    "                                  \"metric\": self.metrics_valid}}\n",
    "        # load best models post training\n",
    "        self.load_best_model()\n",
    "\n",
    "        # compute feature importance once the best model is defined\n",
    "        self._compute_feature_importances(train_dataloader)\n",
    "\n",
    "    def save_model(self, path):\n",
    "        \"\"\"\n",
    "        Saving model with two distinct files.\n",
    "        \"\"\"\n",
    "        saved_params = {}\n",
    "        for key, val in self.get_params().items():\n",
    "            if isinstance(val, type):\n",
    "                # Don't save torch specific params\n",
    "                continue\n",
    "            else:\n",
    "                saved_params[key] = val\n",
    "\n",
    "        # Create folder\n",
    "        Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Save models params\n",
    "        with open(Path(path).joinpath(\"model_params.json\"), \"w\", encoding=\"utf8\") as f:\n",
    "            json.dump(saved_params, f)\n",
    "\n",
    "        # Save state_dict\n",
    "        torch.save(self.network.state_dict(), Path(path).joinpath(\"network.pt\"))\n",
    "        shutil.make_archive(path, 'zip', path)\n",
    "        shutil.rmtree(path)\n",
    "        print(f\"Successfully saved model at {path}.zip\")\n",
    "        return f\"{path}.zip\"\n",
    "\n",
    "    def load_model(self, filepath):\n",
    "\n",
    "        try:\n",
    "            with open(filepath + '/model_params.json') as f:\n",
    "                loaded_params = json.load(f)\n",
    "            saved_state_dict = torch.load(filepath + '/network.pt')\n",
    "            \n",
    "#             with zipfile.ZipFile(filepath) as z:\n",
    "#                 with z.open(\"model_params.json\") as f:\n",
    "#                     loaded_params = json.load(f)\n",
    "#                 with z.open(\"network.pt\") as f:\n",
    "#                     try:\n",
    "#                         saved_state_dict = torch.load(f)\n",
    "#                     except io.UnsupportedOperation:\n",
    "#                         # In Python <3.7, the returned file object is not seekable (which at least\n",
    "#                         # some versions of PyTorch require) - so we'll try buffering it in to a\n",
    "#                         # BytesIO instead:\n",
    "#                         saved_state_dict = torch.load(io.BytesIO(f.read()))\n",
    "        except KeyError:\n",
    "            raise KeyError(\"Your zip file is missing at least one component\")\n",
    "\n",
    "        self.__init__(**loaded_params)\n",
    "\n",
    "        self.init_network(\n",
    "            input_dim=self.input_dim,\n",
    "            output_dim=self.output_dim,\n",
    "            n_d=self.n_d,\n",
    "            n_a=self.n_a,\n",
    "            n_steps=self.n_steps,\n",
    "            gamma=self.gamma,\n",
    "            cat_idxs=self.cat_idxs,\n",
    "            cat_dims=self.cat_dims,\n",
    "            cat_emb_dim=self.cat_emb_dim,\n",
    "            n_independent=self.n_independent,\n",
    "            n_shared=self.n_shared,\n",
    "            epsilon=self.epsilon,\n",
    "            virtual_batch_size=1024,\n",
    "            momentum=self.momentum,\n",
    "            device_name=self.device_name,\n",
    "            mask_type=self.mask_type\n",
    "        )\n",
    "        self.network.load_state_dict(saved_state_dict)\n",
    "        self.network.eval()\n",
    "        return\n",
    "\n",
    "    def fit_epoch(self, train_dataloader, valid_dataloader):\n",
    "        \"\"\"\n",
    "        Evaluates and updates network for one epoch.\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "            valid_dataloader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with valid set\n",
    "        \"\"\"\n",
    "        train_metrics = self.train_epoch(train_dataloader)\n",
    "        valid_metrics = self.predict_epoch(valid_dataloader)\n",
    "\n",
    "        fit_metrics = {'train': train_metrics,\n",
    "                       'valid': valid_metrics}\n",
    "\n",
    "        return fit_metrics\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define train_batch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_epoch to use this base class')\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict_batch to use this base class')\n",
    "\n",
    "    def load_best_model(self):\n",
    "        if self.best_network is not None:\n",
    "            self.network = self.best_network\n",
    "\n",
    "    @abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem or the last class\n",
    "        \"\"\"\n",
    "        raise NotImplementedError('users must define predict to use this base class')\n",
    "\n",
    "    def explain(self, X):\n",
    "        \"\"\"\n",
    "        Return local explanation\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            M_explain: matrix\n",
    "                Importance per sample, per columns.\n",
    "            masks: matrix\n",
    "                Sparse matrix showing attention masks used by network.\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            for key, value in masks.items():\n",
    "                masks[key] = csc_matrix.dot(value.cpu().detach().numpy(),\n",
    "                                            self.reducing_matrix)\n",
    "\n",
    "            if batch_nb == 0:\n",
    "                res_explain = csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                             self.reducing_matrix)\n",
    "                res_masks = masks\n",
    "            else:\n",
    "                res_explain = np.vstack([res_explain,\n",
    "                                         csc_matrix.dot(M_explain.cpu().detach().numpy(),\n",
    "                                                        self.reducing_matrix)])\n",
    "                for key, value in masks.items():\n",
    "                    res_masks[key] = np.vstack([res_masks[key], value])\n",
    "        return res_explain, res_masks\n",
    "\n",
    "    def _compute_feature_importances(self, loader):\n",
    "        self.network.eval()\n",
    "        feature_importances_ = np.zeros((self.network.post_embed_dim))\n",
    "        for data, targets in loader:\n",
    "            data = data.to(self.device).float()\n",
    "            M_explain, masks = self.network.forward_masks(data)\n",
    "            feature_importances_ += M_explain.sum(dim=0).cpu().detach().numpy()\n",
    "\n",
    "        feature_importances_ = csc_matrix.dot(feature_importances_,\n",
    "                                              self.reducing_matrix)\n",
    "        self.feature_importances_ = feature_importances_ / np.sum(feature_importances_)\n",
    "        \n",
    "        \n",
    "class TabNetRegressor(TabModel):\n",
    "\n",
    "    def construct_loaders(self, X_train, y_train, X_valid, y_valid, weights,\n",
    "                          batch_size, num_workers, drop_last):\n",
    "        \"\"\"\n",
    "        Returns\n",
    "        -------\n",
    "        train_dataloader, valid_dataloader : torch.DataLoader, torch.DataLoader\n",
    "            Training and validation dataloaders\n",
    "        -------\n",
    "        \"\"\"\n",
    "        if isinstance(weights, int):\n",
    "            if weights == 1:\n",
    "                raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "        if isinstance(weights, dict):\n",
    "            raise ValueError(\"Please provide a list of weights for regression.\")\n",
    "\n",
    "        train_dataloader, valid_dataloader = create_dataloaders(X_train,\n",
    "                                                                y_train,\n",
    "                                                                X_valid,\n",
    "                                                                y_valid,\n",
    "                                                                weights,\n",
    "                                                                batch_size,\n",
    "                                                                num_workers,\n",
    "                                                                drop_last)\n",
    "        return train_dataloader, valid_dataloader\n",
    "\n",
    "    def update_fit_params(self, X_train, y_train, X_valid, y_valid, loss_fn,\n",
    "                          weights, max_epochs, patience,\n",
    "                          batch_size, virtual_batch_size, num_workers, drop_last):\n",
    "\n",
    "        if loss_fn is None:\n",
    "            self.loss_fn = torch.nn.functional.mse_loss\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "\n",
    "        assert X_train.shape[1] == X_valid.shape[1], \"Dimension mismatch X_train X_valid\"\n",
    "        self.input_dim = X_train.shape[1]\n",
    "\n",
    "        if len(y_train.shape) == 1:\n",
    "            raise ValueError(\"\"\"Please apply reshape(-1, 1) to your targets\n",
    "                                if doing single regression.\"\"\")\n",
    "        assert y_train.shape[1] == y_valid.shape[1], \"Dimension mismatch y_train y_valid\"\n",
    "        self.output_dim = y_train.shape[1]\n",
    "\n",
    "        self.updated_weights = weights\n",
    "\n",
    "        self.max_epochs = max_epochs\n",
    "        self.patience = patience\n",
    "        self.batch_size = batch_size\n",
    "        self.virtual_batch_size = virtual_batch_size\n",
    "        # Initialize counters and histories.\n",
    "        self.patience_counter = 0\n",
    "        self.epoch = 0\n",
    "        self.best_cost = np.inf\n",
    "        self.num_workers = num_workers\n",
    "        self.drop_last = drop_last\n",
    "\n",
    "    def train_epoch(self, train_loader):\n",
    "        \"\"\"\n",
    "        Trains one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            train_loader: a :class: `torch.utils.data.Dataloader`\n",
    "                DataLoader with train set\n",
    "        \"\"\"\n",
    "\n",
    "        self.network.train()\n",
    "        total_loss = 0\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for data, targets in train_loader:\n",
    "            batch_outs = self.train_batch(data, targets)\n",
    "            y_pred.append(batch_outs[\"y_preds\"].sigmoid().cpu().detach().numpy())\n",
    "            y_true.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.concatenate(y_true)\n",
    "\n",
    "        #stopping_loss = mean_squared_error(y_true=ys, y_pred=y_preds)\n",
    "        stopping_loss = log_loss_multi(y_true, y_pred)\n",
    "        total_loss = total_loss / len(train_loader)\n",
    "        epoch_metrics = {'loss_avg': total_loss,\n",
    "                         'stopping_loss': total_loss,\n",
    "                         }\n",
    "\n",
    "        if self.scheduler is not None:\n",
    "            self.scheduler.step()\n",
    "        return epoch_metrics\n",
    "\n",
    "    def train_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Trains one batch of data\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.tensor`\n",
    "                Target data\n",
    "        \"\"\"\n",
    "        self.network.train()\n",
    "        data = data.to(self.device).float()\n",
    "\n",
    "        targets = targets.to(self.device).float()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "\n",
    "        loss = loss_tr(output, targets) #self.loss_fn(output, targets)\n",
    "        \n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "\n",
    "        loss.backward()\n",
    "        if self.clip_value:\n",
    "            clip_grad_norm_(self.network.parameters(), self.clip_value)\n",
    "        self.optimizer.step()\n",
    "\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict_epoch(self, loader):\n",
    "        \"\"\"\n",
    "        Validates one epoch of the network in self.network\n",
    "        Parameters\n",
    "        ----------\n",
    "            loader: a :class: `torch.utils.data.Dataloader`\n",
    "                    DataLoader with validation set\n",
    "        \"\"\"\n",
    "        \n",
    "        self.network.eval()\n",
    "        total_loss = 0\n",
    "\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for data, targets in loader:\n",
    "            batch_outs = self.predict_batch(data, targets)\n",
    "            total_loss += batch_outs[\"loss\"]\n",
    "            y_pred.append(batch_outs[\"y_preds\"].sigmoid().cpu().detach().numpy())\n",
    "            y_true.append(batch_outs[\"y\"].cpu().detach().numpy())\n",
    "\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        y_true = np.concatenate(y_true)\n",
    "\n",
    "        stopping_loss = log_loss_multi(y_true, y_pred)\n",
    "\n",
    "        total_loss = total_loss / len(loader)\n",
    "        epoch_metrics = {'total_loss': total_loss,\n",
    "                         'stopping_loss': stopping_loss}\n",
    "\n",
    "        return epoch_metrics\n",
    "\n",
    "    def predict_batch(self, data, targets):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            batch_outs: dict\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        data = data.to(self.device).float()\n",
    "        targets = targets.to(self.device).float()\n",
    "\n",
    "        output, M_loss = self.network(data)\n",
    "        \n",
    "        loss = loss_val(output, targets) #self.loss_fn(output, targets)\n",
    "        #print(self.loss_fn, loss)\n",
    "        loss -= self.lambda_sparse*M_loss\n",
    "        #print(loss)\n",
    "        loss_value = loss.item()\n",
    "        batch_outs = {'loss': loss_value,\n",
    "                      'y_preds': output,\n",
    "                      'y': targets}\n",
    "        return batch_outs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions on a batch (valid)\n",
    "        Parameters\n",
    "        ----------\n",
    "            data: a :tensor: `torch.Tensor`\n",
    "                Input data\n",
    "            target: a :tensor: `torch.Tensor`\n",
    "                Target data\n",
    "        Returns\n",
    "        -------\n",
    "            predictions: np.array\n",
    "                Predictions of the regression problem\n",
    "        \"\"\"\n",
    "        self.network.eval()\n",
    "        dataloader = DataLoader(PredictDataset(X),\n",
    "                                batch_size=self.batch_size, shuffle=False)\n",
    "\n",
    "        y_pred = []\n",
    "        for batch_nb, data in enumerate(dataloader):\n",
    "            data = data.to(self.device).float()\n",
    "\n",
    "            output, M_loss = self.network(data)\n",
    "            predictions = output.sigmoid().cpu().detach().numpy()\n",
    "            y_pred.append(predictions)\n",
    "        y_pred = np.concatenate(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:03.035686Z",
     "iopub.status.busy": "2020-12-06T00:04:03.034965Z",
     "iopub.status.idle": "2020-12-06T00:04:03.038120Z",
     "shell.execute_reply": "2020-12-06T00:04:03.038679Z"
    },
    "papermill": {
     "duration": 0.097785,
     "end_time": "2020-12-06T00:04:03.038788",
     "exception": false,
     "start_time": "2020-12-06T00:04:02.941003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.modules.loss import _WeightedLoss\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SmoothBCEwLogits(_WeightedLoss):\n",
    "    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n",
    "        super().__init__(weight=weight, reduction=reduction)\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "        self.reduction = reduction\n",
    "\n",
    "    @staticmethod\n",
    "    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n",
    "        assert 0 <= smoothing < 1\n",
    "        with torch.no_grad():\n",
    "            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n",
    "        return targets\n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n",
    "            self.smoothing)\n",
    "        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n",
    "\n",
    "        if  self.reduction == 'sum':\n",
    "            loss = loss.sum()\n",
    "        elif  self.reduction == 'mean':\n",
    "            loss = loss.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:03.209786Z",
     "iopub.status.busy": "2020-12-06T00:04:03.209013Z",
     "iopub.status.idle": "2020-12-06T00:04:03.211965Z",
     "shell.execute_reply": "2020-12-06T00:04:03.211472Z"
    },
    "papermill": {
     "duration": 0.09053,
     "end_time": "2020-12-06T00:04:03.212059",
     "exception": false,
     "start_time": "2020-12-06T00:04:03.121529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "loss_tr = SmoothBCEwLogits(smoothing =0.001)\n",
    "loss_val = torch.nn.functional.binary_cross_entropy_with_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:03.388065Z",
     "iopub.status.busy": "2020-12-06T00:04:03.387363Z",
     "iopub.status.idle": "2020-12-06T00:04:03.390435Z",
     "shell.execute_reply": "2020-12-06T00:04:03.389816Z"
    },
    "papermill": {
     "duration": 0.094978,
     "end_time": "2020-12-06T00:04:03.390532",
     "exception": false,
     "start_time": "2020-12-06T00:04:03.295554",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_TABNET(fold, seed_fold, seed_run, display=2):\n",
    "    seed_everything(seed_run)\n",
    "    test_ = test_noctl.copy()\n",
    "\n",
    "#     where_categorical = [i for i, feat in enumerate(feature_cols) if feat in categorical_feats]\n",
    "    model = TabNetRegressor(n_d=tabnet_parameters['n_d'], n_a=tabnet_parameters['n_a'], \n",
    "                        n_steps=tabnet_parameters['n_steps'], gamma=tabnet_parameters['gamma'], \n",
    "                        lambda_sparse=tabnet_parameters['lambda_sparse'],mask_type=tabnet_parameters['mask_type'],\n",
    "#                         cat_dims=[3,2], cat_emb_dim=[1,1], cat_idxs=where_categorical, \n",
    "                        optimizer_fn=torch.optim.Adam,\n",
    "                        optimizer_params=dict(lr=tabnet_parameters['lr'], weight_decay=tabnet_parameters['weight_decay']),\n",
    "                        device_name='cuda', scheduler_params=dict(milestones=[ 100,150], gamma=0.9), \n",
    "                        scheduler_fn=torch.optim.lr_scheduler.MultiStepLR,\n",
    "                        seed=int(seed_run), verbose=1)\n",
    "    model.load_model(f'{directorio}/tabnet_model_seedfold{seed_fold}_seedrun{seed_run}_fold__{fold}')\n",
    "    #--------------------- TEST PREDICTION---------------------\n",
    "    x_test = test_[feature_cols].values\n",
    "    x_test = torch.as_tensor(x_test)\n",
    "    predictions_tst = model.predict(x_test)\n",
    "    return predictions_tst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:03.609523Z",
     "iopub.status.busy": "2020-12-06T00:04:03.608609Z",
     "iopub.status.idle": "2020-12-06T00:04:03.622602Z",
     "shell.execute_reply": "2020-12-06T00:04:03.623505Z"
    },
    "papermill": {
     "duration": 0.137564,
     "end_time": "2020-12-06T00:04:03.623732",
     "exception": false,
     "start_time": "2020-12-06T00:04:03.486168",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_k_fold(NFOLDS, seed_fold, seed_run, display=2):\n",
    "    predictions_kfold = np.zeros((len(test_noctl), len(target_cols)))\n",
    "    for fold in range(NFOLDS):\n",
    "        pred_ = run_inference_TABNET(fold, seed_fold, seed_run, display)\n",
    "        predictions_kfold += pred_ / NFOLDS\n",
    "    return predictions_kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:03.911426Z",
     "iopub.status.busy": "2020-12-06T00:04:03.910486Z",
     "iopub.status.idle": "2020-12-06T00:04:03.914199Z",
     "shell.execute_reply": "2020-12-06T00:04:03.912191Z"
    },
    "papermill": {
     "duration": 0.146368,
     "end_time": "2020-12-06T00:04:03.914352",
     "exception": false,
     "start_time": "2020-12-06T00:04:03.767984",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "974"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_cols_ini)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.122712,
     "end_time": "2020-12-06T00:04:04.159110",
     "exception": false,
     "start_time": "2020-12-06T00:04:04.036398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TABNET del 30 de noviembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:04.372805Z",
     "iopub.status.busy": "2020-12-06T00:04:04.371572Z",
     "iopub.status.idle": "2020-12-06T00:04:31.695153Z",
     "shell.execute_reply": "2020-12-06T00:04:31.694485Z"
    },
    "papermill": {
     "duration": 27.431997,
     "end_time": "2020-12-06T00:04:31.695315",
     "exception": false,
     "start_time": "2020-12-06T00:04:04.263318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffe622ad72e34ea5b51fb6c644335336",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "Device used : cuda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directorio = '../input/tabnetrandommean7' #'../input/tabnetstratkmean7' #'../input/tabnetrandomnosmooth' #'../input/tabnetstratnosmooth' #'../input/tabnetrandomnosmooth' #'../input/tabnetstratnosmooth' #'../input/tabnetrandommean7'\n",
    "tabnet_parameters = {'n_d':35, 'n_a':30, 'n_steps':1, 'gamma':1.3, 'lambda_sparse':0, 'mask_type':'entmax',\n",
    "                    'max_epochs':200, 'batch_size':1024, 'lr':2e-2, 'weight_decay':1e-5, 'patience':50}\n",
    "\n",
    "\n",
    "feature_cols = feature_cols_ini.copy()\n",
    "\n",
    "folds_cp = folds.copy()\n",
    "feature_cols = feature_cols_ini.copy()\n",
    "NFOLDS = CFG.num_folds\n",
    "num_features=len(feature_cols)\n",
    "num_targets=len(target_cols)\n",
    "\n",
    "# Averaging on multiple SEEDS\n",
    "SEED = np.arange(16)\n",
    "oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "losses_list = []\n",
    "# SEED = [[0,3],[1,1],[2,1],[3,2]]\n",
    "# for seed_fold, seed_run in tqdm(SEED):\n",
    "SEED = np.arange(16)\n",
    "for seed in tqdm(SEED):\n",
    "    seed_fold = seed // 4\n",
    "    seed_run = seed % 4\n",
    "\n",
    "    predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "    predictions += predictions_\n",
    "\n",
    "# FINAL CV LOGLOSS\n",
    "for col in target_cols:\n",
    "    test_noctl[col] = 0.0\n",
    "test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:31.950006Z",
     "iopub.status.busy": "2020-12-06T00:04:31.948591Z",
     "iopub.status.idle": "2020-12-06T00:04:33.887723Z",
     "shell.execute_reply": "2020-12-06T00:04:33.888185Z"
    },
    "papermill": {
     "duration": 2.071627,
     "end_time": "2020-12-06T00:04:33.888366",
     "exception": false,
     "start_time": "2020-12-06T00:04:31.816739",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TABNET 30nov\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001706</td>\n",
       "      <td>0.001714</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.011531</td>\n",
       "      <td>0.017788</td>\n",
       "      <td>0.004932</td>\n",
       "      <td>0.002702</td>\n",
       "      <td>0.006784</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>0.008753</td>\n",
       "      <td>0.014869</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.003852</td>\n",
       "      <td>0.008377</td>\n",
       "      <td>0.006118</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.003672</td>\n",
       "      <td>0.004650</td>\n",
       "      <td>0.000876</td>\n",
       "      <td>0.002961</td>\n",
       "      <td>0.001027</td>\n",
       "      <td>0.001203</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.001797</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.004274</td>\n",
       "      <td>0.004625</td>\n",
       "      <td>0.000704</td>\n",
       "      <td>0.000741</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.004346</td>\n",
       "      <td>0.000663</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.005232</td>\n",
       "      <td>0.008558</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.008608</td>\n",
       "      <td>0.008052</td>\n",
       "      <td>0.006253</td>\n",
       "      <td>0.001028</td>\n",
       "      <td>0.003018</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>0.001006</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.017827</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.002784</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>0.001913</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>0.005626</td>\n",
       "      <td>0.001922</td>\n",
       "      <td>0.000914</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000592</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.003836</td>\n",
       "      <td>0.005447</td>\n",
       "      <td>0.000912</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.042555</td>\n",
       "      <td>0.005787</td>\n",
       "      <td>0.001383</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.019245</td>\n",
       "      <td>0.007351</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.022218</td>\n",
       "      <td>0.002678</td>\n",
       "      <td>0.004266</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.002698</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.001020</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.001534</td>\n",
       "      <td>0.002032</td>\n",
       "      <td>0.012952</td>\n",
       "      <td>0.014834</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.002896</td>\n",
       "      <td>0.001633</td>\n",
       "      <td>0.006145</td>\n",
       "      <td>0.022790</td>\n",
       "      <td>0.001717</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.006313</td>\n",
       "      <td>0.001731</td>\n",
       "      <td>0.003955</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.000863</td>\n",
       "      <td>0.001419</td>\n",
       "      <td>0.008027</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.002534</td>\n",
       "      <td>0.003022</td>\n",
       "      <td>0.002586</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.001185</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.000993</td>\n",
       "      <td>0.004539</td>\n",
       "      <td>0.001509</td>\n",
       "      <td>0.003328</td>\n",
       "      <td>0.001074</td>\n",
       "      <td>0.000860</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.006121</td>\n",
       "      <td>0.002180</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.004501</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.001206</td>\n",
       "      <td>0.001583</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.002772</td>\n",
       "      <td>0.005161</td>\n",
       "      <td>0.002169</td>\n",
       "      <td>0.000906</td>\n",
       "      <td>0.001498</td>\n",
       "      <td>0.001998</td>\n",
       "      <td>0.001188</td>\n",
       "      <td>0.001120</td>\n",
       "      <td>0.017125</td>\n",
       "      <td>0.002093</td>\n",
       "      <td>0.002434</td>\n",
       "      <td>0.001154</td>\n",
       "      <td>0.003475</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.005664</td>\n",
       "      <td>0.003264</td>\n",
       "      <td>0.022573</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.006964</td>\n",
       "      <td>0.000650</td>\n",
       "      <td>0.003388</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>0.004942</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.004086</td>\n",
       "      <td>0.001555</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.004442</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001350</td>\n",
       "      <td>0.001646</td>\n",
       "      <td>0.001700</td>\n",
       "      <td>0.010093</td>\n",
       "      <td>0.009727</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.002788</td>\n",
       "      <td>0.001775</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.017888</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.000885</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.000915</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.000564</td>\n",
       "      <td>0.001310</td>\n",
       "      <td>0.001828</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.003020</td>\n",
       "      <td>0.002147</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.001707</td>\n",
       "      <td>0.003963</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.002150</td>\n",
       "      <td>0.010352</td>\n",
       "      <td>0.002043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>0.001344</td>\n",
       "      <td>0.002457</td>\n",
       "      <td>0.002232</td>\n",
       "      <td>0.001109</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>0.002518</td>\n",
       "      <td>0.008517</td>\n",
       "      <td>0.004229</td>\n",
       "      <td>0.006849</td>\n",
       "      <td>0.004218</td>\n",
       "      <td>0.008136</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.009425</td>\n",
       "      <td>0.001217</td>\n",
       "      <td>0.001102</td>\n",
       "      <td>0.000916</td>\n",
       "      <td>0.002066</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.003581</td>\n",
       "      <td>0.004545</td>\n",
       "      <td>0.002718</td>\n",
       "      <td>0.000886</td>\n",
       "      <td>0.001502</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.001616</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.004025</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>0.001284</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.003356</td>\n",
       "      <td>0.003017</td>\n",
       "      <td>0.006667</td>\n",
       "      <td>0.001464</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.002753</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.001710</td>\n",
       "      <td>0.001256</td>\n",
       "      <td>0.002745</td>\n",
       "      <td>0.003927</td>\n",
       "      <td>0.004619</td>\n",
       "      <td>0.001777</td>\n",
       "      <td>0.024023</td>\n",
       "      <td>0.010079</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.004375</td>\n",
       "      <td>0.003437</td>\n",
       "      <td>0.003075</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.004005</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.001131</td>\n",
       "      <td>0.003459</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.009616</td>\n",
       "      <td>0.002234</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.004976</td>\n",
       "      <td>0.001439</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.001427</td>\n",
       "      <td>0.009598</td>\n",
       "      <td>0.002514</td>\n",
       "      <td>0.000743</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.001543</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.001082</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.007238</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.003717</td>\n",
       "      <td>0.008738</td>\n",
       "      <td>0.010785</td>\n",
       "      <td>0.002721</td>\n",
       "      <td>0.001252</td>\n",
       "      <td>0.001336</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.006597</td>\n",
       "      <td>0.005552</td>\n",
       "      <td>0.002038</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.001701</td>\n",
       "      <td>0.005470</td>\n",
       "      <td>0.001486</td>\n",
       "      <td>0.004499</td>\n",
       "      <td>0.004839</td>\n",
       "      <td>0.004090</td>\n",
       "      <td>0.001033</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.003433</td>\n",
       "      <td>0.003586</td>\n",
       "      <td>0.001928</td>\n",
       "      <td>0.001792</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>0.007819</td>\n",
       "      <td>0.004380</td>\n",
       "      <td>0.001896</td>\n",
       "      <td>0.002157</td>\n",
       "      <td>0.004898</td>\n",
       "      <td>0.060602</td>\n",
       "      <td>0.006089</td>\n",
       "      <td>0.011179</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.005489</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>0.001245</td>\n",
       "      <td>0.002030</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>0.002311</td>\n",
       "      <td>0.001516</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001738</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.005705</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.000888</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.000967</td>\n",
       "      <td>0.001093</td>\n",
       "      <td>0.001668</td>\n",
       "      <td>0.001227</td>\n",
       "      <td>0.002824</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>0.003840</td>\n",
       "      <td>0.005718</td>\n",
       "      <td>0.009035</td>\n",
       "      <td>0.002368</td>\n",
       "      <td>0.016546</td>\n",
       "      <td>0.001066</td>\n",
       "      <td>0.016402</td>\n",
       "      <td>0.003888</td>\n",
       "      <td>0.002082</td>\n",
       "      <td>0.002462</td>\n",
       "      <td>0.086188</td>\n",
       "      <td>0.016163</td>\n",
       "      <td>0.001728</td>\n",
       "      <td>0.012698</td>\n",
       "      <td>0.001886</td>\n",
       "      <td>0.003181</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.001399</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.001182</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.004242</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.025321</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.003226</td>\n",
       "      <td>0.004458</td>\n",
       "      <td>0.001143</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.001412</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.005860</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>0.011716</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.003244</td>\n",
       "      <td>0.002190</td>\n",
       "      <td>0.006242</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.000574</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.002104</td>\n",
       "      <td>0.003999</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.001744</td>\n",
       "      <td>0.001219</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.004489</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.006695</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.017373</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.003092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.001314</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.011295</td>\n",
       "      <td>0.016534</td>\n",
       "      <td>0.004831</td>\n",
       "      <td>0.003385</td>\n",
       "      <td>0.005403</td>\n",
       "      <td>0.000603</td>\n",
       "      <td>0.009967</td>\n",
       "      <td>0.025317</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.002865</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.004623</td>\n",
       "      <td>0.002350</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>0.002887</td>\n",
       "      <td>0.001153</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.001351</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.001586</td>\n",
       "      <td>0.001926</td>\n",
       "      <td>0.005050</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.002046</td>\n",
       "      <td>0.002636</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.000853</td>\n",
       "      <td>0.003539</td>\n",
       "      <td>0.000968</td>\n",
       "      <td>0.001072</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.003248</td>\n",
       "      <td>0.004451</td>\n",
       "      <td>0.002450</td>\n",
       "      <td>0.011263</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.005899</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.001108</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.003923</td>\n",
       "      <td>0.001774</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.001598</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.023150</td>\n",
       "      <td>0.002535</td>\n",
       "      <td>0.003980</td>\n",
       "      <td>0.002131</td>\n",
       "      <td>0.002354</td>\n",
       "      <td>0.001304</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>0.006871</td>\n",
       "      <td>0.001450</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.000639</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.002694</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.001077</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.013707</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.001612</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.002299</td>\n",
       "      <td>0.010749</td>\n",
       "      <td>0.009946</td>\n",
       "      <td>0.035604</td>\n",
       "      <td>0.007875</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.004029</td>\n",
       "      <td>0.001859</td>\n",
       "      <td>0.001002</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001174</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001840</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.006776</td>\n",
       "      <td>0.001771</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.001083</td>\n",
       "      <td>0.003878</td>\n",
       "      <td>0.016570</td>\n",
       "      <td>0.001670</td>\n",
       "      <td>0.001008</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.001972</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.020695</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.002912</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.001046</td>\n",
       "      <td>0.002085</td>\n",
       "      <td>0.001024</td>\n",
       "      <td>0.000767</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.002653</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.002410</td>\n",
       "      <td>0.001146</td>\n",
       "      <td>0.000617</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.002183</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.005633</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.003154</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.004672</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.002209</td>\n",
       "      <td>0.000879</td>\n",
       "      <td>0.002316</td>\n",
       "      <td>0.001158</td>\n",
       "      <td>0.000861</td>\n",
       "      <td>0.005485</td>\n",
       "      <td>0.005248</td>\n",
       "      <td>0.002844</td>\n",
       "      <td>0.003117</td>\n",
       "      <td>0.001537</td>\n",
       "      <td>0.001838</td>\n",
       "      <td>0.000711</td>\n",
       "      <td>0.001127</td>\n",
       "      <td>0.009850</td>\n",
       "      <td>0.002338</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>0.001532</td>\n",
       "      <td>0.003588</td>\n",
       "      <td>0.006406</td>\n",
       "      <td>0.001732</td>\n",
       "      <td>0.001416</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.002155</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.002628</td>\n",
       "      <td>0.000842</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.001371</td>\n",
       "      <td>0.003083</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.001588</td>\n",
       "      <td>0.000653</td>\n",
       "      <td>0.000976</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>0.001481</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.017397</td>\n",
       "      <td>0.036545</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>0.003792</td>\n",
       "      <td>0.002164</td>\n",
       "      <td>0.010258</td>\n",
       "      <td>0.001818</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.001019</td>\n",
       "      <td>0.000841</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>0.000969</td>\n",
       "      <td>0.002148</td>\n",
       "      <td>0.001936</td>\n",
       "      <td>0.002340</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.001877</td>\n",
       "      <td>0.000654</td>\n",
       "      <td>0.001627</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.001538</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.010556</td>\n",
       "      <td>0.003511</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.001645</td>\n",
       "      <td>0.002191</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.002247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001748</td>\n",
       "      <td>0.001716</td>\n",
       "      <td>0.001970</td>\n",
       "      <td>0.012103</td>\n",
       "      <td>0.014137</td>\n",
       "      <td>0.004475</td>\n",
       "      <td>0.003584</td>\n",
       "      <td>0.003497</td>\n",
       "      <td>0.000870</td>\n",
       "      <td>0.009722</td>\n",
       "      <td>0.016013</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001499</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>0.003101</td>\n",
       "      <td>0.004313</td>\n",
       "      <td>0.003543</td>\n",
       "      <td>0.002505</td>\n",
       "      <td>0.002975</td>\n",
       "      <td>0.004049</td>\n",
       "      <td>0.001079</td>\n",
       "      <td>0.003195</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001311</td>\n",
       "      <td>0.001312</td>\n",
       "      <td>0.001387</td>\n",
       "      <td>0.005118</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>0.002105</td>\n",
       "      <td>0.003027</td>\n",
       "      <td>0.003206</td>\n",
       "      <td>0.000833</td>\n",
       "      <td>0.000845</td>\n",
       "      <td>0.000896</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.000900</td>\n",
       "      <td>0.000709</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.004009</td>\n",
       "      <td>0.005433</td>\n",
       "      <td>0.002454</td>\n",
       "      <td>0.009086</td>\n",
       "      <td>0.005322</td>\n",
       "      <td>0.007837</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.000925</td>\n",
       "      <td>0.004027</td>\n",
       "      <td>0.001893</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.001380</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.008188</td>\n",
       "      <td>0.002818</td>\n",
       "      <td>0.002620</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.001599</td>\n",
       "      <td>0.001342</td>\n",
       "      <td>0.004952</td>\n",
       "      <td>0.001835</td>\n",
       "      <td>0.002601</td>\n",
       "      <td>0.003942</td>\n",
       "      <td>0.000745</td>\n",
       "      <td>0.003009</td>\n",
       "      <td>0.002723</td>\n",
       "      <td>0.003632</td>\n",
       "      <td>0.001037</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.018507</td>\n",
       "      <td>0.005936</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.002071</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.004085</td>\n",
       "      <td>0.036410</td>\n",
       "      <td>0.004874</td>\n",
       "      <td>0.008497</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.001095</td>\n",
       "      <td>0.000827</td>\n",
       "      <td>0.008630</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.002111</td>\n",
       "      <td>0.001125</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.002684</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.001621</td>\n",
       "      <td>0.001316</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>0.007135</td>\n",
       "      <td>0.001231</td>\n",
       "      <td>0.001791</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.004433</td>\n",
       "      <td>0.019416</td>\n",
       "      <td>0.001860</td>\n",
       "      <td>0.001451</td>\n",
       "      <td>0.003854</td>\n",
       "      <td>0.004271</td>\n",
       "      <td>0.003550</td>\n",
       "      <td>0.009497</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.001589</td>\n",
       "      <td>0.003428</td>\n",
       "      <td>0.001343</td>\n",
       "      <td>0.002060</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.001870</td>\n",
       "      <td>0.002395</td>\n",
       "      <td>0.003455</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>0.001022</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001087</td>\n",
       "      <td>0.003624</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001052</td>\n",
       "      <td>0.000903</td>\n",
       "      <td>0.004909</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.005153</td>\n",
       "      <td>0.001297</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>0.003246</td>\n",
       "      <td>0.001761</td>\n",
       "      <td>0.006137</td>\n",
       "      <td>0.001161</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.002134</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001456</td>\n",
       "      <td>0.003091</td>\n",
       "      <td>0.005493</td>\n",
       "      <td>0.002703</td>\n",
       "      <td>0.001181</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.003599</td>\n",
       "      <td>0.002387</td>\n",
       "      <td>0.001411</td>\n",
       "      <td>0.011848</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.003140</td>\n",
       "      <td>0.001885</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.004248</td>\n",
       "      <td>0.001331</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.001271</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.001329</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.004201</td>\n",
       "      <td>0.001862</td>\n",
       "      <td>0.001081</td>\n",
       "      <td>0.001152</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.003432</td>\n",
       "      <td>0.002272</td>\n",
       "      <td>0.010038</td>\n",
       "      <td>0.010832</td>\n",
       "      <td>0.002553</td>\n",
       "      <td>0.002307</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.016798</td>\n",
       "      <td>0.001979</td>\n",
       "      <td>0.001237</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.001362</td>\n",
       "      <td>0.003095</td>\n",
       "      <td>0.000740</td>\n",
       "      <td>0.001569</td>\n",
       "      <td>0.003964</td>\n",
       "      <td>0.002220</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.002444</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.001136</td>\n",
       "      <td>0.001366</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.002589</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.002336</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.002327</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001706                0.001714   \n",
       "1  id_001897cda                     0.000926                0.001344   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.001329                0.001314   \n",
       "4  id_0027f1083                     0.001748                0.001716   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.002338                        0.011531   \n",
       "1        0.002457                        0.002232   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.001712                        0.011295   \n",
       "4        0.001970                        0.012103   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.017788                        0.004932   \n",
       "1                           0.001109                        0.002004   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.016534                        0.004831   \n",
       "4                           0.014137                        0.004475   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002702                       0.006784   \n",
       "1                    0.002518                       0.008517   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.003385                       0.005403   \n",
       "4                    0.003584                       0.003497   \n",
       "\n",
       "   adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                    0.000621                     0.008753   \n",
       "1                    0.004229                     0.006849   \n",
       "2                    0.000000                     0.000000   \n",
       "3                    0.000603                     0.009967   \n",
       "4                    0.000870                     0.009722   \n",
       "\n",
       "   adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                        0.014869       0.001108   \n",
       "1                        0.004218       0.008136   \n",
       "2                        0.000000       0.000000   \n",
       "3                        0.025317       0.001324   \n",
       "4                        0.016013       0.001897   \n",
       "\n",
       "   aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  analgesic  \\\n",
       "0                          0.000942       0.000819        0.001456   0.001378   \n",
       "1                          0.001000       0.009425        0.001217   0.001102   \n",
       "2                          0.000000       0.000000        0.000000   0.000000   \n",
       "3                          0.000908       0.002054        0.001346   0.001468   \n",
       "4                          0.000904       0.001124        0.001499   0.001549   \n",
       "\n",
       "   androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0                   0.003852                      0.008377   \n",
       "1                   0.000916                      0.002066   \n",
       "2                   0.000000                      0.000000   \n",
       "3                   0.002865                      0.004874   \n",
       "4                   0.003101                      0.004313   \n",
       "\n",
       "   anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0            0.006118                0.002551   \n",
       "1            0.002089                0.003581   \n",
       "2            0.000000                0.000000   \n",
       "3            0.004623                0.002350   \n",
       "4            0.003543                0.002505   \n",
       "\n",
       "   angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                         0.003672           0.004650        0.000876   \n",
       "1                         0.004545           0.002718        0.000886   \n",
       "2                         0.000000           0.000000        0.000000   \n",
       "3                         0.002109           0.002887        0.001153   \n",
       "4                         0.002975           0.004049        0.001079   \n",
       "\n",
       "   antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0    0.002961        0.001027    0.001203       0.001143      0.001797   \n",
       "1    0.001502        0.001014    0.001474       0.001078      0.000877   \n",
       "2    0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3    0.002640        0.001351    0.001270       0.001586      0.001926   \n",
       "4    0.003195        0.001382    0.001311       0.001312      0.001387   \n",
       "\n",
       "   antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0     0.004038       0.002474   0.001517             0.004274   \n",
       "1     0.001616       0.001596   0.001333             0.004025   \n",
       "2     0.000000       0.000000   0.000000             0.000000   \n",
       "3     0.005050       0.002258   0.002079             0.002046   \n",
       "4     0.005118       0.002783   0.002105             0.003027   \n",
       "\n",
       "   aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0             0.004625              0.000704   \n",
       "1             0.001727              0.001284   \n",
       "2             0.000000              0.000000   \n",
       "3             0.002636              0.001233   \n",
       "4             0.003206              0.000833   \n",
       "\n",
       "   atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                    0.000741                0.001123   \n",
       "1                                    0.000978                0.000976   \n",
       "2                                    0.000000                0.000000   \n",
       "3                                    0.000803                0.000853   \n",
       "4                                    0.000845                0.000896   \n",
       "\n",
       "   atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0          0.004346              0.000663                 0.000707   \n",
       "1          0.003356              0.003017                 0.006667   \n",
       "2          0.000000              0.000000                 0.000000   \n",
       "3          0.003539              0.000968                 0.001072   \n",
       "4          0.003002              0.000900                 0.000709   \n",
       "\n",
       "   autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0             0.000929                                   0.005232   \n",
       "1             0.001464                                   0.001923   \n",
       "2             0.000000                                   0.000000   \n",
       "3             0.001121                                   0.003248   \n",
       "4             0.001091                                   0.004009   \n",
       "\n",
       "   bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                   0.008558              0.002252   \n",
       "1                                   0.001365              0.000806   \n",
       "2                                   0.000000              0.000000   \n",
       "3                                   0.004451              0.002450   \n",
       "4                                   0.005433              0.002454   \n",
       "\n",
       "   bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                 0.008608                        0.008052   \n",
       "1                                 0.002753                        0.001334   \n",
       "2                                 0.000000                        0.000000   \n",
       "3                                 0.011263                        0.004364   \n",
       "4                                 0.009086                        0.005322   \n",
       "\n",
       "   bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                 0.006253                                0.001028   \n",
       "1                 0.001710                                0.001256   \n",
       "2                 0.000000                                0.000000   \n",
       "3                 0.005899                                0.000845   \n",
       "4                 0.007837                                0.001022   \n",
       "\n",
       "   bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0       0.003018           0.000910                         0.003267   \n",
       "1       0.002745           0.003927                         0.004619   \n",
       "2       0.000000           0.000000                         0.000000   \n",
       "3       0.001108           0.001155                         0.003923   \n",
       "4       0.002294           0.000925                         0.004027   \n",
       "\n",
       "   beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                0.002182               0.003895       0.001006   \n",
       "1                0.001777               0.024023       0.010079   \n",
       "2                0.000000               0.000000       0.000000   \n",
       "3                0.001774               0.001271       0.001598   \n",
       "4                0.001893               0.002583       0.001380   \n",
       "\n",
       "   calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0               0.000973                 0.017827   \n",
       "1               0.000903                 0.004375   \n",
       "2               0.000000                 0.000000   \n",
       "3               0.001121                 0.023150   \n",
       "4               0.001172                 0.008188   \n",
       "\n",
       "   cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                      0.002684                         0.002784   \n",
       "1                      0.003437                         0.003075   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.002535                         0.003980   \n",
       "4                      0.002818                         0.002620   \n",
       "\n",
       "   carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  caspase_activator  \\\n",
       "0                      0.002785                 0.001913           0.002653   \n",
       "1                      0.001929                 0.004005           0.001263   \n",
       "2                      0.000000                 0.000000           0.000000   \n",
       "3                      0.002131                 0.002354           0.001304   \n",
       "4                      0.002645                 0.002397           0.001599   \n",
       "\n",
       "   catechol_o_methyltransferase_inhibitor  cc_chemokine_receptor_antagonist  \\\n",
       "0                                0.001361                          0.005626   \n",
       "1                                0.001131                          0.003459   \n",
       "2                                0.000000                          0.000000   \n",
       "3                                0.001263                          0.006871   \n",
       "4                                0.001342                          0.004952   \n",
       "\n",
       "   cck_receptor_antagonist  cdk_inhibitor  chelating_agent  chk_inhibitor  \\\n",
       "0                 0.001922       0.000914         0.003252       0.000592   \n",
       "1                 0.001222       0.009616         0.002234       0.002987   \n",
       "2                 0.000000       0.000000         0.000000       0.000000   \n",
       "3                 0.001450       0.001038         0.002968       0.000639   \n",
       "4                 0.001835       0.002601         0.003942       0.000745   \n",
       "\n",
       "   chloride_channel_blocker  cholesterol_inhibitor  \\\n",
       "0                  0.003955               0.003836   \n",
       "1                  0.001168               0.004976   \n",
       "2                  0.000000               0.000000   \n",
       "3                  0.002696               0.002694   \n",
       "4                  0.003009               0.002723   \n",
       "\n",
       "   cholinergic_receptor_antagonist  coagulation_factor_inhibitor  \\\n",
       "0                         0.005447                      0.000912   \n",
       "1                         0.001439                      0.001022   \n",
       "2                         0.000000                      0.000000   \n",
       "3                         0.003052                      0.001077   \n",
       "4                         0.003632                      0.001037   \n",
       "\n",
       "   corticosteroid_agonist  cyclooxygenase_inhibitor  \\\n",
       "0                0.001558                  0.042555   \n",
       "1                0.001427                  0.009598   \n",
       "2                0.000000                  0.000000   \n",
       "3                0.001150                  0.013707   \n",
       "4                0.001346                  0.018507   \n",
       "\n",
       "   cytochrome_p450_inhibitor  dihydrofolate_reductase_inhibitor  \\\n",
       "0                   0.005787                           0.001383   \n",
       "1                   0.002514                           0.000743   \n",
       "2                   0.000000                           0.000000   \n",
       "3                   0.004542                           0.001612   \n",
       "4                   0.005936                           0.002416   \n",
       "\n",
       "   dipeptidyl_peptidase_inhibitor  diuretic  dna_alkylating_agent  \\\n",
       "0                        0.002720  0.000971              0.003433   \n",
       "1                        0.002134  0.001003              0.001543   \n",
       "2                        0.000000  0.000000              0.000000   \n",
       "3                        0.001564  0.000975              0.002299   \n",
       "4                        0.002071  0.001142              0.004085   \n",
       "\n",
       "   dna_inhibitor  dopamine_receptor_agonist  dopamine_receptor_antagonist  \\\n",
       "0       0.019245                   0.007351                      0.014535   \n",
       "1       0.002868                   0.002224                      0.001324   \n",
       "2       0.000000                   0.000000                      0.000000   \n",
       "3       0.010749                   0.009946                      0.035604   \n",
       "4       0.036410                   0.004874                      0.008497   \n",
       "\n",
       "   egfr_inhibitor  elastase_inhibitor  erbb2_inhibitor  \\\n",
       "0        0.000881            0.001167         0.000691   \n",
       "1        0.003678            0.001297         0.001082   \n",
       "2        0.000000            0.000000         0.000000   \n",
       "3        0.007875            0.000960         0.000827   \n",
       "4        0.001823            0.001095         0.000827   \n",
       "\n",
       "   estrogen_receptor_agonist  estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                   0.022218                      0.002678        0.004266   \n",
       "1                   0.003245                      0.002164        0.007238   \n",
       "2                   0.000000                      0.000000        0.000000   \n",
       "3                   0.005666                      0.004029        0.001859   \n",
       "4                   0.008630                      0.001878        0.002111   \n",
       "\n",
       "   farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  fgfr_inhibitor  \\\n",
       "0                       0.000882                     0.002698        0.000983   \n",
       "1                       0.001521                     0.003717        0.008738   \n",
       "2                       0.000000                     0.000000        0.000000   \n",
       "3                       0.001002                     0.001307        0.001174   \n",
       "4                       0.001125                     0.001740        0.000956   \n",
       "\n",
       "   flt3_inhibitor  focal_adhesion_kinase_inhibitor  free_radical_scavenger  \\\n",
       "0        0.001020                         0.000800                0.001534   \n",
       "1        0.010785                         0.002721                0.001252   \n",
       "2        0.000000                         0.000000                0.000000   \n",
       "3        0.000911                         0.000715                0.001667   \n",
       "4        0.002684                         0.000897                0.001621   \n",
       "\n",
       "   fungal_squalene_epoxidase_inhibitor  gaba_receptor_agonist  \\\n",
       "0                             0.002032               0.012952   \n",
       "1                             0.001336               0.002887   \n",
       "2                             0.000000               0.000000   \n",
       "3                             0.001840               0.004330   \n",
       "4                             0.001316               0.006020   \n",
       "\n",
       "   gaba_receptor_antagonist  gamma_secretase_inhibitor  \\\n",
       "0                  0.014834                   0.001728   \n",
       "1                  0.006597                   0.005552   \n",
       "2                  0.000000                   0.000000   \n",
       "3                  0.006776                   0.001771   \n",
       "4                  0.007135                   0.001231   \n",
       "\n",
       "   glucocorticoid_receptor_agonist  glutamate_inhibitor  \\\n",
       "0                         0.002896             0.001633   \n",
       "1                         0.002038             0.001601   \n",
       "2                         0.000000             0.000000   \n",
       "3                         0.001370             0.001083   \n",
       "4                         0.001791             0.001432   \n",
       "\n",
       "   glutamate_receptor_agonist  glutamate_receptor_antagonist  \\\n",
       "0                    0.006145                       0.022790   \n",
       "1                    0.001701                       0.005470   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.003878                       0.016570   \n",
       "4                    0.004433                       0.019416   \n",
       "\n",
       "   gonadotropin_receptor_agonist  gsk_inhibitor  hcv_inhibitor  \\\n",
       "0                       0.001717       0.001012       0.006313   \n",
       "1                       0.001486       0.004499       0.004839   \n",
       "2                       0.000000       0.000000       0.000000   \n",
       "3                       0.001670       0.001008       0.003642   \n",
       "4                       0.001860       0.001451       0.003854   \n",
       "\n",
       "   hdac_inhibitor  histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0        0.001731                    0.003955                       0.010037   \n",
       "1        0.004090                    0.001033                       0.001935   \n",
       "2        0.000000                    0.000000                       0.000000   \n",
       "3        0.001972                    0.003992                       0.020695   \n",
       "4        0.004271                    0.003550                       0.009497   \n",
       "\n",
       "   histone_lysine_demethylase_inhibitor  \\\n",
       "0                              0.000863   \n",
       "1                              0.003433   \n",
       "2                              0.000000   \n",
       "3                              0.002071   \n",
       "4                              0.000783   \n",
       "\n",
       "   histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  hmgcr_inhibitor  \\\n",
       "0                                    0.001419       0.008027         0.001548   \n",
       "1                                    0.003586       0.001928         0.001792   \n",
       "2                                    0.000000       0.000000         0.000000   \n",
       "3                                    0.002912       0.003776         0.002222   \n",
       "4                                    0.001589       0.003428         0.001343   \n",
       "\n",
       "   hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0       0.001412         0.000799       0.001691   \n",
       "1       0.002815         0.007819       0.004380   \n",
       "2       0.000000         0.000000       0.000000   \n",
       "3       0.000833         0.001910       0.001029   \n",
       "4       0.002060         0.001003       0.001870   \n",
       "\n",
       "   imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                      0.002548           0.002534              0.003022   \n",
       "1                      0.001896           0.002157              0.004898   \n",
       "2                      0.000000           0.000000              0.000000   \n",
       "3                      0.002346           0.003168              0.001667   \n",
       "4                      0.002395           0.003455              0.002416   \n",
       "\n",
       "   insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0            0.002586            0.002887       0.000806       0.001185   \n",
       "1            0.060602            0.006089       0.011179       0.005738   \n",
       "2            0.000000            0.000000       0.000000       0.000000   \n",
       "3            0.001046            0.002085       0.001024       0.000767   \n",
       "4            0.002149            0.002416       0.001022       0.002160   \n",
       "\n",
       "   laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0  0.001112               0.000993                         0.004539   \n",
       "1  0.001176               0.001040                         0.005489   \n",
       "2  0.000000               0.000000                         0.000000   \n",
       "3  0.000950               0.001042                         0.002653   \n",
       "4  0.001124               0.001087                         0.003624   \n",
       "\n",
       "   lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0          0.001509                0.003328     0.001074       0.000860   \n",
       "1          0.001591                0.003766     0.001245       0.002030   \n",
       "2          0.000000                0.000000     0.000000       0.000000   \n",
       "3          0.001320                0.002410     0.001146       0.000617   \n",
       "4          0.001432                0.003720     0.000934       0.001052   \n",
       "\n",
       "   mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0       0.000842                      0.006121   \n",
       "1       0.001934                      0.002311   \n",
       "2       0.000000                      0.000000   \n",
       "3       0.001507                      0.003696   \n",
       "4       0.000903                      0.004909   \n",
       "\n",
       "   mineralocorticoid_receptor_antagonist  monoacylglycerol_lipase_inhibitor  \\\n",
       "0                               0.002180                           0.001787   \n",
       "1                               0.001516                           0.001351   \n",
       "2                               0.000000                           0.000000   \n",
       "3                               0.002183                           0.001032   \n",
       "4                               0.001843                           0.001232   \n",
       "\n",
       "   monoamine_oxidase_inhibitor  monopolar_spindle_1_kinase_inhibitor  \\\n",
       "0                     0.004510                              0.000991   \n",
       "1                     0.001738                              0.001370   \n",
       "2                     0.000000                              0.000000   \n",
       "3                     0.005633                              0.001414   \n",
       "4                     0.005153                              0.001297   \n",
       "\n",
       "   mtor_inhibitor  mucolytic_agent  neuropeptide_receptor_antagonist  \\\n",
       "0        0.001195         0.003514                          0.001588   \n",
       "1        0.005705         0.001176                          0.001126   \n",
       "2        0.000000         0.000000                          0.000000   \n",
       "3        0.001596         0.003154                          0.004175   \n",
       "4        0.002050         0.003246                          0.001761   \n",
       "\n",
       "   nfkb_inhibitor  nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0        0.004501                    0.001094            0.001842   \n",
       "1        0.002477                    0.001080            0.000888   \n",
       "2        0.000000                    0.000000            0.000000   \n",
       "3        0.004672                    0.000958            0.002209   \n",
       "4        0.006137                    0.001161            0.002304   \n",
       "\n",
       "   nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                           0.001206                         0.001583   \n",
       "1                           0.002111                         0.000967   \n",
       "2                           0.000000                         0.000000   \n",
       "3                           0.000879                         0.002316   \n",
       "4                           0.001411                         0.002134   \n",
       "\n",
       "   norepinephrine_reuptake_inhibitor  nrf2_activator  opioid_receptor_agonist  \\\n",
       "0                           0.000984        0.001227                 0.002772   \n",
       "1                           0.001093        0.001668                 0.001227   \n",
       "2                           0.000000        0.000000                 0.000000   \n",
       "3                           0.001158        0.000861                 0.005485   \n",
       "4                           0.001080        0.001456                 0.003091   \n",
       "\n",
       "   opioid_receptor_antagonist  orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                    0.005161                    0.002169            0.000906   \n",
       "1                    0.002824                    0.001767            0.002159   \n",
       "2                    0.000000                    0.000000            0.000000   \n",
       "3                    0.005248                    0.002844            0.003117   \n",
       "4                    0.005493                    0.002703            0.001181   \n",
       "\n",
       "   p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  pdk_inhibitor  \\\n",
       "0                  0.001498        0.001998         0.001188       0.001120   \n",
       "1                  0.003840        0.005718         0.009035       0.002368   \n",
       "2                  0.000000        0.000000         0.000000       0.000000   \n",
       "3                  0.001537        0.001838         0.000711       0.001127   \n",
       "4                  0.001497        0.003599         0.002387       0.001411   \n",
       "\n",
       "   phosphodiesterase_inhibitor  phospholipase_inhibitor  pi3k_inhibitor  \\\n",
       "0                     0.017125                 0.002093        0.002434   \n",
       "1                     0.016546                 0.001066        0.016402   \n",
       "2                     0.000000                 0.000000        0.000000   \n",
       "3                     0.009850                 0.002338        0.002976   \n",
       "4                     0.011848                 0.002007        0.003140   \n",
       "\n",
       "   pkc_inhibitor  potassium_channel_activator  potassium_channel_antagonist  \\\n",
       "0       0.001154                     0.003475                      0.006979   \n",
       "1       0.003888                     0.002082                      0.002462   \n",
       "2       0.000000                     0.000000                      0.000000   \n",
       "3       0.001532                     0.003588                      0.006406   \n",
       "4       0.001885                     0.004039                      0.004165   \n",
       "\n",
       "   ppar_receptor_agonist  ppar_receptor_antagonist  \\\n",
       "0               0.005664                  0.003264   \n",
       "1               0.086188                  0.016163   \n",
       "2               0.000000                  0.000000   \n",
       "3               0.001732                  0.001416   \n",
       "4               0.004556                  0.002092   \n",
       "\n",
       "   progesterone_receptor_agonist  progesterone_receptor_antagonist  \\\n",
       "0                       0.022573                          0.003234   \n",
       "1                       0.001728                          0.012698   \n",
       "2                       0.000000                          0.000000   \n",
       "3                       0.002394                          0.000960   \n",
       "4                       0.004248                          0.001331   \n",
       "\n",
       "   prostaglandin_inhibitor  prostanoid_receptor_antagonist  \\\n",
       "0                 0.003752                        0.006964   \n",
       "1                 0.001886                        0.003181   \n",
       "2                 0.000000                        0.000000   \n",
       "3                 0.002155                        0.003427   \n",
       "4                 0.003015                        0.004119   \n",
       "\n",
       "   proteasome_inhibitor  protein_kinase_inhibitor  \\\n",
       "0              0.000650                  0.003388   \n",
       "1              0.001781                  0.002755   \n",
       "2              0.000000                  0.000000   \n",
       "3              0.000817                  0.002628   \n",
       "4              0.001271                  0.002250   \n",
       "\n",
       "   protein_phosphatase_inhibitor  protein_synthesis_inhibitor  \\\n",
       "0                       0.000838                     0.004942   \n",
       "1                       0.001046                     0.001399   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.000842                     0.003541   \n",
       "4                       0.001329                     0.005443   \n",
       "\n",
       "   protein_tyrosine_kinase_inhibitor  radiopaque_medium  raf_inhibitor  \\\n",
       "0                           0.001276           0.004086       0.001555   \n",
       "1                           0.002407           0.001182       0.001581   \n",
       "2                           0.000000           0.000000       0.000000   \n",
       "3                           0.001371           0.003083       0.000805   \n",
       "4                           0.001382           0.004201       0.001862   \n",
       "\n",
       "   ras_gtpase_inhibitor  retinoid_receptor_agonist  \\\n",
       "0              0.001144                   0.004442   \n",
       "1              0.001528                   0.004242   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.001588                   0.000653   \n",
       "4              0.001081                   0.001152   \n",
       "\n",
       "   retinoid_receptor_antagonist  rho_associated_kinase_inhibitor  \\\n",
       "0                      0.001015                         0.001350   \n",
       "1                      0.001788                         0.025321   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.000976                         0.000988   \n",
       "4                      0.001193                         0.001558   \n",
       "\n",
       "   ribonucleoside_reductase_inhibitor  rna_polymerase_inhibitor  \\\n",
       "0                            0.001646                  0.001700   \n",
       "1                            0.001177                  0.003226   \n",
       "2                            0.000000                  0.000000   \n",
       "3                            0.001481                  0.001180   \n",
       "4                            0.003432                  0.002272   \n",
       "\n",
       "   serotonin_receptor_agonist  serotonin_receptor_antagonist  \\\n",
       "0                    0.010093                       0.009727   \n",
       "1                    0.004458                       0.001143   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.017397                       0.036545   \n",
       "4                    0.010038                       0.010832   \n",
       "\n",
       "   serotonin_reuptake_inhibitor  sigma_receptor_agonist  \\\n",
       "0                      0.003255                0.002788   \n",
       "1                      0.001161                0.001412   \n",
       "2                      0.000000                0.000000   \n",
       "3                      0.002791                0.002474   \n",
       "4                      0.002553                0.002307   \n",
       "\n",
       "   sigma_receptor_antagonist  smoothened_receptor_antagonist  \\\n",
       "0                   0.001775                        0.001712   \n",
       "1                   0.001150                        0.002620   \n",
       "2                   0.000000                        0.000000   \n",
       "3                   0.003792                        0.002164   \n",
       "4                   0.001772                        0.001876   \n",
       "\n",
       "   sodium_channel_inhibitor  sphingosine_receptor_agonist  src_inhibitor  \\\n",
       "0                  0.017888                      0.002704       0.000885   \n",
       "1                  0.005860                      0.001358       0.011716   \n",
       "2                  0.000000                      0.000000       0.000000   \n",
       "3                  0.010258                      0.001818       0.002267   \n",
       "4                  0.016798                      0.001979       0.001237   \n",
       "\n",
       "    steroid  syk_inhibitor  tachykinin_antagonist  \\\n",
       "0  0.001092       0.000915               0.002323   \n",
       "1  0.001094       0.003244               0.002190   \n",
       "2  0.000000       0.000000               0.000000   \n",
       "3  0.001019       0.000841               0.004318   \n",
       "4  0.001228       0.001362               0.003095   \n",
       "\n",
       "   tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                     0.000564            0.001310   \n",
       "1                     0.006242            0.000907   \n",
       "2                     0.000000            0.000000   \n",
       "3                     0.000969            0.002148   \n",
       "4                     0.000740            0.001569   \n",
       "\n",
       "   thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  tnf_inhibitor  \\\n",
       "0                        0.001828     0.002079        0.001236       0.003020   \n",
       "1                        0.000574     0.001499        0.002104       0.003999   \n",
       "2                        0.000000     0.000000        0.000000       0.000000   \n",
       "3                        0.001936     0.002340        0.000925       0.001877   \n",
       "4                        0.003964     0.002220        0.001179       0.002444   \n",
       "\n",
       "   topoisomerase_inhibitor  transient_receptor_potential_channel_antagonist  \\\n",
       "0                 0.002147                                         0.001598   \n",
       "1                 0.004119                                         0.001744   \n",
       "2                 0.000000                                         0.000000   \n",
       "3                 0.000654                                         0.001627   \n",
       "4                 0.002367                                         0.001484   \n",
       "\n",
       "   tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                               0.001091      0.001707         0.003963   \n",
       "1                               0.001219      0.001632         0.004489   \n",
       "2                               0.000000      0.000000         0.000000   \n",
       "3                               0.000960      0.001538         0.002440   \n",
       "4                               0.001136      0.001366         0.003077   \n",
       "\n",
       "   tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0           0.001065                   0.001072   \n",
       "1           0.001204                   0.006695   \n",
       "2           0.000000                   0.000000   \n",
       "3           0.010556                   0.003511   \n",
       "4           0.002589                   0.001882   \n",
       "\n",
       "   ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                               0.001073         0.001696   0.002150   \n",
       "1                               0.001012         0.017373   0.001627   \n",
       "2                               0.000000         0.000000   0.000000   \n",
       "3                               0.000930         0.001645   0.002191   \n",
       "4                               0.001141         0.001426   0.002336   \n",
       "\n",
       "   vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0.010352       0.002043  \n",
       "1                    0.008225       0.003092  \n",
       "2                    0.000000       0.000000  \n",
       "3                    0.000675       0.002247  \n",
       "4                    0.000820       0.002327  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = sample_submission.drop(columns=target_cols)\\\n",
    ".merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    ".fillna(0.0).reset_index(drop=True)\n",
    "name_sub = 'submission.csv'\n",
    "submission.to_csv(name_sub, index=False)\n",
    "print('TABNET 30nov')\n",
    "submission_TABNET_30nov = submission.copy()\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.122767,
     "end_time": "2020-12-06T00:04:34.135421",
     "exception": false,
     "start_time": "2020-12-06T00:04:34.012654",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## TABNET del 28 de noviembre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:34.387665Z",
     "iopub.status.busy": "2020-12-06T00:04:34.386874Z",
     "iopub.status.idle": "2020-12-06T00:04:34.389481Z",
     "shell.execute_reply": "2020-12-06T00:04:34.390067Z"
    },
    "papermill": {
     "duration": 0.130721,
     "end_time": "2020-12-06T00:04:34.390195",
     "exception": false,
     "start_time": "2020-12-06T00:04:34.259474",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# directorio = '../input/tabnetrandom28nov2'\n",
    "# tabnet_parameters = {'n_d':35, 'n_a':30, 'n_steps':1, 'gamma':1.3, 'lambda_sparse':0, 'mask_type':'entmax',\n",
    "#                     'max_epochs':200, 'batch_size':1024, 'lr':2e-2, 'weight_decay':1e-5, 'patience':50}\n",
    "\n",
    "# folds_cp = folds.copy()\n",
    "# feature_cols = feature_cols_ini_nocps.copy()\n",
    "# NFOLDS = CFG.num_folds\n",
    "# num_features=len(feature_cols)\n",
    "# num_targets=len(target_cols)\n",
    "\n",
    "# # Averaging on multiple SEEDS\n",
    "# SEED = np.arange(16)\n",
    "# oof_seed = np.zeros((len(folds), len(target_cols)))\n",
    "# predictions = np.zeros((len(test_noctl), len(target_cols)))\n",
    "# losses_list = []\n",
    "# # SEED = [[0,3],[1,1],[2,1],[3,2]]\n",
    "# # for seed_fold, seed_run in tqdm(SEED):\n",
    "# SEED = np.arange(16)\n",
    "# for seed in tqdm(SEED):\n",
    "#     seed_fold = seed // 4\n",
    "#     seed_run = seed % 4\n",
    "\n",
    "#     predictions_ = run_k_fold(NFOLDS, seed_fold, seed_run, display=2)\n",
    "#     predictions += predictions_\n",
    "\n",
    "# # FINAL CV LOGLOSS\n",
    "# for col in target_cols:\n",
    "#     test_noctl[col] = 0.0\n",
    "# test_noctl[target_cols] = predictions / len(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:34.640295Z",
     "iopub.status.busy": "2020-12-06T00:04:34.639523Z",
     "iopub.status.idle": "2020-12-06T00:04:34.642449Z",
     "shell.execute_reply": "2020-12-06T00:04:34.641964Z"
    },
    "papermill": {
     "duration": 0.129419,
     "end_time": "2020-12-06T00:04:34.642551",
     "exception": false,
     "start_time": "2020-12-06T00:04:34.513132",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# submission = sample_submission.drop(columns=target_cols)\\\n",
    "# .merge(test_noctl[['sig_id']+target_cols], on='sig_id', how='left')\\\n",
    "# .fillna(0.0).reset_index(drop=True)\n",
    "# # sub.to_csv('submission.csv', index=False)\n",
    "# # name_sub = 'submission.csv'\n",
    "# # submission.to_csv(name_sub, index=False)\n",
    "# # print(name_sub)\n",
    "# print('TABNET 28nov')\n",
    "# submission_TABNET_28nov = submission.copy()\n",
    "# submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.128831,
     "end_time": "2020-12-06T00:04:34.895165",
     "exception": false,
     "start_time": "2020-12-06T00:04:34.766334",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# BLENDING FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:35.146757Z",
     "iopub.status.busy": "2020-12-06T00:04:35.145962Z",
     "iopub.status.idle": "2020-12-06T00:04:35.148596Z",
     "shell.execute_reply": "2020-12-06T00:04:35.149031Z"
    },
    "papermill": {
     "duration": 0.12954,
     "end_time": "2020-12-06T00:04:35.149150",
     "exception": false,
     "start_time": "2020-12-06T00:04:35.019610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# assert all(submission_TABNET_28nov.sig_id==submission_TABNET_30nov.sig_id)\n",
    "# assert all(submission_TABNET_28nov.sig_id==submission_RESNET_30nov.sig_id)\n",
    "# assert all(submission_TABNET_28nov.sig_id==submission_RESNET_28nov.sig_id)\n",
    "# assert all(submission_TABNET_28nov.sig_id==submission_ANN_28nov.sig_id)\n",
    "# assert all(submission_TABNET_28nov.sig_id==submission_ANN_30nov.sig_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:35.407463Z",
     "iopub.status.busy": "2020-12-06T00:04:35.404576Z",
     "iopub.status.idle": "2020-12-06T00:04:38.071627Z",
     "shell.execute_reply": "2020-12-06T00:04:38.072950Z"
    },
    "papermill": {
     "duration": 2.800109,
     "end_time": "2020-12-06T00:04:38.073145",
     "exception": false,
     "start_time": "2020-12-06T00:04:35.273036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 229_blending_26nov\n",
    "# names_models = ['../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_73002_model_TabNet_21nov_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics_resnet_poly/']\n",
    "\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.016602\n",
    "#          Iterations: 100\n",
    "#          Function evaluations: 205\n",
    "# 0.016602136014725388 [0.38940193 0.32797063 0.2861724 ]\n",
    "# blending_models([0.389,0.328,0.286])\n",
    "# 0.016602141087921556\n",
    "\n",
    "\n",
    "\n",
    "# names_models = ['../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_73002_model_TabNet_21nov_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics_resnet_poly/',\n",
    "#                '../results/_FINAL_ANN_CPS_STRAT/',\n",
    "#                '../results/_FINAL_TABNET_CPS_STRAT/']\n",
    "#0.01659238709151143\n",
    "#[0.1075, 0.2098, 0.2107, 0.3352, 0.1414]\n",
    "\n",
    "\n",
    "\n",
    "# w = [0.1075, 0.2098, 0.2107, 0.3352, 0.1414]\n",
    "\n",
    "\n",
    "\n",
    "# names_models = ['../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_73002_model_TabNet_21nov_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics_resnet_poly/',\n",
    "#                    '../results/_FINAL_ANN_CPS_STRAT/',\n",
    "#                    '../results/_FINAL_TABNET_CPS_STRAT/',\n",
    "#                    '../results/_FINAL_RESNET_CPS_STRAT_v2/']\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.016591\n",
    "#          Iterations: 373\n",
    "#          Function evaluations: 620\n",
    "# 0.016590586038811306 [ 0.11163874  0.2195801   0.35596777  0.34734814  0.1496238  -0.17911374]\n",
    "\n",
    "# w = [ 0.11163874,  0.2195801,   0.35596777,  0.34734814,  0.1496238,  -0.17911374] #[0.112, 0.220, 0.356, 0.347, 0.150, -0.179]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FINAL\n",
    "\n",
    "# names_models = [\n",
    "#                 '../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_73002_model_TabNet_21nov_4por4_newcalcfold_metrics/',\n",
    "#                 '../results/_NEW_83005_20nov_difffolds_1E3_4por4_newcalcfold_metrics_resnet_poly/',\n",
    "                \n",
    "#                '../results/_FINAL_ANN_CPS_STRAT_KMEANS7/',\n",
    "#                '../results/_FINAL_TABNET_CPS_STRAT_KMEANS7/',\n",
    "#                '../results/_FINAL_RESNET_CPS_STRAT_KMEANS7/']\n",
    "\n",
    "# STRATIFIED BLENDING!!!!\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.016590\n",
    "#          Iterations: 402\n",
    "#          Function evaluations: 674\n",
    "# 0.01659034997712169 [0.0914673  0.18184113 0.14183788 0.3465606  0.15979273 0.08306089]\n",
    "\n",
    "# w = [0.0914673, 0.18184113, 0.14183788, 0.3465606, 0.15979273, 0.08306089]\n",
    "\n",
    "# submission = submission_ANN_28nov.copy()\n",
    "# blending = (w[0]*submission_ANN_28nov.values[:,1:]) + \\\n",
    "#             (w[1]*submission_TABNET_28nov.values[:,1:]) + \\\n",
    "#             (w[2]*submission_RESNET_28nov.values[:,1:]) + \\\n",
    "#             (w[3]*submission_ANN_30nov.values[:,1:]) + \\\n",
    "#             (w[4]*submission_TABNET_30nov.values[:,1:]) + \\\n",
    "#             (w[5]*submission_RESNET_30nov.values[:,1:])\n",
    "\n",
    "\n",
    "# STRATIFIED BLENDING!!!!\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.016476\n",
    "#          Iterations: 113\n",
    "#          Function evaluations: 228\n",
    "# 0.016476294101737145 [0.37846299 0.37115626 0.25789995]\n",
    "\n",
    "#                '../results/_FINAL_ANN_CPS_STRAT_KMEANS7_SMOOTH_NO/',\n",
    "#                '../results/_FINAL_TABNET_CPS_STRAT_KMEANS7_SMOOTH_NO/',\n",
    "#                '../results/_FINAL_RESNET_CPS_STRAT_KMEANS7_SMOOTH_NO/']\n",
    "            \n",
    "# w = [0.37846299, 0.37115626, 0.25789995]\n",
    "\n",
    "# w = [1.0/3, 1.0/3, 1.0/3]\n",
    "# w = [ 0.48450186, -0.01045436,  0.52783731]\n",
    "\n",
    "\n",
    "\n",
    "# STRATIFIED BLENDING!!!!\n",
    "# Optimization terminated successfully.\n",
    "#          Current function value: 0.016596\n",
    "#          Iterations: 91\n",
    "#          Function evaluations: 183\n",
    "# 0.01659632784193557 [0.45209143 0.31521697 0.23694659]\n",
    "\n",
    "w =  [0.45209143, 0.31521697, 0.23694659]\n",
    "\n",
    "submission = submission_ANN_30nov.copy()\n",
    "blending = (w[0]*submission_ANN_30nov.values[:,1:]) + \\\n",
    "            (w[1]*submission_TABNET_30nov.values[:,1:]) + \\\n",
    "            (w[2]*submission_RESNET_30nov.values[:,1:])\n",
    "\n",
    "\n",
    "\n",
    "blending = blending.clip(0.0, 1.0)\n",
    "submission.iloc[:,1:] = blending\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-06T00:04:38.377848Z",
     "iopub.status.busy": "2020-12-06T00:04:38.376837Z",
     "iopub.status.idle": "2020-12-06T00:04:38.511834Z",
     "shell.execute_reply": "2020-12-06T00:04:38.511332Z"
    },
    "papermill": {
     "duration": 0.308039,
     "end_time": "2020-12-06T00:04:38.511945",
     "exception": false,
     "start_time": "2020-12-06T00:04:38.203906",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sig_id</th>\n",
       "      <th>5-alpha_reductase_inhibitor</th>\n",
       "      <th>11-beta-hsd1_inhibitor</th>\n",
       "      <th>acat_inhibitor</th>\n",
       "      <th>acetylcholine_receptor_agonist</th>\n",
       "      <th>acetylcholine_receptor_antagonist</th>\n",
       "      <th>acetylcholinesterase_inhibitor</th>\n",
       "      <th>adenosine_receptor_agonist</th>\n",
       "      <th>adenosine_receptor_antagonist</th>\n",
       "      <th>adenylyl_cyclase_activator</th>\n",
       "      <th>adrenergic_receptor_agonist</th>\n",
       "      <th>adrenergic_receptor_antagonist</th>\n",
       "      <th>akt_inhibitor</th>\n",
       "      <th>aldehyde_dehydrogenase_inhibitor</th>\n",
       "      <th>alk_inhibitor</th>\n",
       "      <th>ampk_activator</th>\n",
       "      <th>analgesic</th>\n",
       "      <th>androgen_receptor_agonist</th>\n",
       "      <th>androgen_receptor_antagonist</th>\n",
       "      <th>anesthetic_-_local</th>\n",
       "      <th>angiogenesis_inhibitor</th>\n",
       "      <th>angiotensin_receptor_antagonist</th>\n",
       "      <th>anti-inflammatory</th>\n",
       "      <th>antiarrhythmic</th>\n",
       "      <th>antibiotic</th>\n",
       "      <th>anticonvulsant</th>\n",
       "      <th>antifungal</th>\n",
       "      <th>antihistamine</th>\n",
       "      <th>antimalarial</th>\n",
       "      <th>antioxidant</th>\n",
       "      <th>antiprotozoal</th>\n",
       "      <th>antiviral</th>\n",
       "      <th>apoptosis_stimulant</th>\n",
       "      <th>aromatase_inhibitor</th>\n",
       "      <th>atm_kinase_inhibitor</th>\n",
       "      <th>atp-sensitive_potassium_channel_antagonist</th>\n",
       "      <th>atp_synthase_inhibitor</th>\n",
       "      <th>atpase_inhibitor</th>\n",
       "      <th>atr_kinase_inhibitor</th>\n",
       "      <th>aurora_kinase_inhibitor</th>\n",
       "      <th>autotaxin_inhibitor</th>\n",
       "      <th>bacterial_30s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_50s_ribosomal_subunit_inhibitor</th>\n",
       "      <th>bacterial_antifolate</th>\n",
       "      <th>bacterial_cell_wall_synthesis_inhibitor</th>\n",
       "      <th>bacterial_dna_gyrase_inhibitor</th>\n",
       "      <th>bacterial_dna_inhibitor</th>\n",
       "      <th>bacterial_membrane_integrity_inhibitor</th>\n",
       "      <th>bcl_inhibitor</th>\n",
       "      <th>bcr-abl_inhibitor</th>\n",
       "      <th>benzodiazepine_receptor_agonist</th>\n",
       "      <th>beta_amyloid_inhibitor</th>\n",
       "      <th>bromodomain_inhibitor</th>\n",
       "      <th>btk_inhibitor</th>\n",
       "      <th>calcineurin_inhibitor</th>\n",
       "      <th>calcium_channel_blocker</th>\n",
       "      <th>cannabinoid_receptor_agonist</th>\n",
       "      <th>cannabinoid_receptor_antagonist</th>\n",
       "      <th>carbonic_anhydrase_inhibitor</th>\n",
       "      <th>casein_kinase_inhibitor</th>\n",
       "      <th>caspase_activator</th>\n",
       "      <th>catechol_o_methyltransferase_inhibitor</th>\n",
       "      <th>cc_chemokine_receptor_antagonist</th>\n",
       "      <th>cck_receptor_antagonist</th>\n",
       "      <th>cdk_inhibitor</th>\n",
       "      <th>chelating_agent</th>\n",
       "      <th>chk_inhibitor</th>\n",
       "      <th>chloride_channel_blocker</th>\n",
       "      <th>cholesterol_inhibitor</th>\n",
       "      <th>cholinergic_receptor_antagonist</th>\n",
       "      <th>coagulation_factor_inhibitor</th>\n",
       "      <th>corticosteroid_agonist</th>\n",
       "      <th>cyclooxygenase_inhibitor</th>\n",
       "      <th>cytochrome_p450_inhibitor</th>\n",
       "      <th>dihydrofolate_reductase_inhibitor</th>\n",
       "      <th>dipeptidyl_peptidase_inhibitor</th>\n",
       "      <th>diuretic</th>\n",
       "      <th>dna_alkylating_agent</th>\n",
       "      <th>dna_inhibitor</th>\n",
       "      <th>dopamine_receptor_agonist</th>\n",
       "      <th>dopamine_receptor_antagonist</th>\n",
       "      <th>egfr_inhibitor</th>\n",
       "      <th>elastase_inhibitor</th>\n",
       "      <th>erbb2_inhibitor</th>\n",
       "      <th>estrogen_receptor_agonist</th>\n",
       "      <th>estrogen_receptor_antagonist</th>\n",
       "      <th>faah_inhibitor</th>\n",
       "      <th>farnesyltransferase_inhibitor</th>\n",
       "      <th>fatty_acid_receptor_agonist</th>\n",
       "      <th>fgfr_inhibitor</th>\n",
       "      <th>flt3_inhibitor</th>\n",
       "      <th>focal_adhesion_kinase_inhibitor</th>\n",
       "      <th>free_radical_scavenger</th>\n",
       "      <th>fungal_squalene_epoxidase_inhibitor</th>\n",
       "      <th>gaba_receptor_agonist</th>\n",
       "      <th>gaba_receptor_antagonist</th>\n",
       "      <th>gamma_secretase_inhibitor</th>\n",
       "      <th>glucocorticoid_receptor_agonist</th>\n",
       "      <th>glutamate_inhibitor</th>\n",
       "      <th>glutamate_receptor_agonist</th>\n",
       "      <th>glutamate_receptor_antagonist</th>\n",
       "      <th>gonadotropin_receptor_agonist</th>\n",
       "      <th>gsk_inhibitor</th>\n",
       "      <th>hcv_inhibitor</th>\n",
       "      <th>hdac_inhibitor</th>\n",
       "      <th>histamine_receptor_agonist</th>\n",
       "      <th>histamine_receptor_antagonist</th>\n",
       "      <th>histone_lysine_demethylase_inhibitor</th>\n",
       "      <th>histone_lysine_methyltransferase_inhibitor</th>\n",
       "      <th>hiv_inhibitor</th>\n",
       "      <th>hmgcr_inhibitor</th>\n",
       "      <th>hsp_inhibitor</th>\n",
       "      <th>igf-1_inhibitor</th>\n",
       "      <th>ikk_inhibitor</th>\n",
       "      <th>imidazoline_receptor_agonist</th>\n",
       "      <th>immunosuppressant</th>\n",
       "      <th>insulin_secretagogue</th>\n",
       "      <th>insulin_sensitizer</th>\n",
       "      <th>integrin_inhibitor</th>\n",
       "      <th>jak_inhibitor</th>\n",
       "      <th>kit_inhibitor</th>\n",
       "      <th>laxative</th>\n",
       "      <th>leukotriene_inhibitor</th>\n",
       "      <th>leukotriene_receptor_antagonist</th>\n",
       "      <th>lipase_inhibitor</th>\n",
       "      <th>lipoxygenase_inhibitor</th>\n",
       "      <th>lxr_agonist</th>\n",
       "      <th>mdm_inhibitor</th>\n",
       "      <th>mek_inhibitor</th>\n",
       "      <th>membrane_integrity_inhibitor</th>\n",
       "      <th>mineralocorticoid_receptor_antagonist</th>\n",
       "      <th>monoacylglycerol_lipase_inhibitor</th>\n",
       "      <th>monoamine_oxidase_inhibitor</th>\n",
       "      <th>monopolar_spindle_1_kinase_inhibitor</th>\n",
       "      <th>mtor_inhibitor</th>\n",
       "      <th>mucolytic_agent</th>\n",
       "      <th>neuropeptide_receptor_antagonist</th>\n",
       "      <th>nfkb_inhibitor</th>\n",
       "      <th>nicotinic_receptor_agonist</th>\n",
       "      <th>nitric_oxide_donor</th>\n",
       "      <th>nitric_oxide_production_inhibitor</th>\n",
       "      <th>nitric_oxide_synthase_inhibitor</th>\n",
       "      <th>norepinephrine_reuptake_inhibitor</th>\n",
       "      <th>nrf2_activator</th>\n",
       "      <th>opioid_receptor_agonist</th>\n",
       "      <th>opioid_receptor_antagonist</th>\n",
       "      <th>orexin_receptor_antagonist</th>\n",
       "      <th>p38_mapk_inhibitor</th>\n",
       "      <th>p-glycoprotein_inhibitor</th>\n",
       "      <th>parp_inhibitor</th>\n",
       "      <th>pdgfr_inhibitor</th>\n",
       "      <th>pdk_inhibitor</th>\n",
       "      <th>phosphodiesterase_inhibitor</th>\n",
       "      <th>phospholipase_inhibitor</th>\n",
       "      <th>pi3k_inhibitor</th>\n",
       "      <th>pkc_inhibitor</th>\n",
       "      <th>potassium_channel_activator</th>\n",
       "      <th>potassium_channel_antagonist</th>\n",
       "      <th>ppar_receptor_agonist</th>\n",
       "      <th>ppar_receptor_antagonist</th>\n",
       "      <th>progesterone_receptor_agonist</th>\n",
       "      <th>progesterone_receptor_antagonist</th>\n",
       "      <th>prostaglandin_inhibitor</th>\n",
       "      <th>prostanoid_receptor_antagonist</th>\n",
       "      <th>proteasome_inhibitor</th>\n",
       "      <th>protein_kinase_inhibitor</th>\n",
       "      <th>protein_phosphatase_inhibitor</th>\n",
       "      <th>protein_synthesis_inhibitor</th>\n",
       "      <th>protein_tyrosine_kinase_inhibitor</th>\n",
       "      <th>radiopaque_medium</th>\n",
       "      <th>raf_inhibitor</th>\n",
       "      <th>ras_gtpase_inhibitor</th>\n",
       "      <th>retinoid_receptor_agonist</th>\n",
       "      <th>retinoid_receptor_antagonist</th>\n",
       "      <th>rho_associated_kinase_inhibitor</th>\n",
       "      <th>ribonucleoside_reductase_inhibitor</th>\n",
       "      <th>rna_polymerase_inhibitor</th>\n",
       "      <th>serotonin_receptor_agonist</th>\n",
       "      <th>serotonin_receptor_antagonist</th>\n",
       "      <th>serotonin_reuptake_inhibitor</th>\n",
       "      <th>sigma_receptor_agonist</th>\n",
       "      <th>sigma_receptor_antagonist</th>\n",
       "      <th>smoothened_receptor_antagonist</th>\n",
       "      <th>sodium_channel_inhibitor</th>\n",
       "      <th>sphingosine_receptor_agonist</th>\n",
       "      <th>src_inhibitor</th>\n",
       "      <th>steroid</th>\n",
       "      <th>syk_inhibitor</th>\n",
       "      <th>tachykinin_antagonist</th>\n",
       "      <th>tgf-beta_receptor_inhibitor</th>\n",
       "      <th>thrombin_inhibitor</th>\n",
       "      <th>thymidylate_synthase_inhibitor</th>\n",
       "      <th>tlr_agonist</th>\n",
       "      <th>tlr_antagonist</th>\n",
       "      <th>tnf_inhibitor</th>\n",
       "      <th>topoisomerase_inhibitor</th>\n",
       "      <th>transient_receptor_potential_channel_antagonist</th>\n",
       "      <th>tropomyosin_receptor_kinase_inhibitor</th>\n",
       "      <th>trpv_agonist</th>\n",
       "      <th>trpv_antagonist</th>\n",
       "      <th>tubulin_inhibitor</th>\n",
       "      <th>tyrosine_kinase_inhibitor</th>\n",
       "      <th>ubiquitin_specific_protease_inhibitor</th>\n",
       "      <th>vegfr_inhibitor</th>\n",
       "      <th>vitamin_b</th>\n",
       "      <th>vitamin_d_receptor_agonist</th>\n",
       "      <th>wnt_inhibitor</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>id_0004d9e33</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.002995</td>\n",
       "      <td>0.010900</td>\n",
       "      <td>0.017746</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.002305</td>\n",
       "      <td>0.007631</td>\n",
       "      <td>0.000568</td>\n",
       "      <td>0.009014</td>\n",
       "      <td>0.012010</td>\n",
       "      <td>0.000769</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.000678</td>\n",
       "      <td>0.001332</td>\n",
       "      <td>0.001286</td>\n",
       "      <td>0.002951</td>\n",
       "      <td>0.007533</td>\n",
       "      <td>0.007437</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.004447</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.002688</td>\n",
       "      <td>0.000826</td>\n",
       "      <td>0.001195</td>\n",
       "      <td>0.001124</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.003593</td>\n",
       "      <td>0.002225</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.005218</td>\n",
       "      <td>0.005183</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.001370</td>\n",
       "      <td>0.005443</td>\n",
       "      <td>0.000604</td>\n",
       "      <td>0.000819</td>\n",
       "      <td>0.000766</td>\n",
       "      <td>0.005091</td>\n",
       "      <td>0.010059</td>\n",
       "      <td>0.001931</td>\n",
       "      <td>0.007161</td>\n",
       "      <td>0.008064</td>\n",
       "      <td>0.006672</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.003512</td>\n",
       "      <td>0.000803</td>\n",
       "      <td>0.003224</td>\n",
       "      <td>0.002067</td>\n",
       "      <td>0.004094</td>\n",
       "      <td>0.000771</td>\n",
       "      <td>0.000943</td>\n",
       "      <td>0.017807</td>\n",
       "      <td>0.002107</td>\n",
       "      <td>0.002200</td>\n",
       "      <td>0.002449</td>\n",
       "      <td>0.001749</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.001595</td>\n",
       "      <td>0.006084</td>\n",
       "      <td>0.002007</td>\n",
       "      <td>0.000581</td>\n",
       "      <td>0.004096</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.002987</td>\n",
       "      <td>0.004083</td>\n",
       "      <td>0.006264</td>\n",
       "      <td>0.000866</td>\n",
       "      <td>0.001420</td>\n",
       "      <td>0.048529</td>\n",
       "      <td>0.005657</td>\n",
       "      <td>0.001224</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.000918</td>\n",
       "      <td>0.004145</td>\n",
       "      <td>0.016388</td>\n",
       "      <td>0.007638</td>\n",
       "      <td>0.012645</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.021728</td>\n",
       "      <td>0.002088</td>\n",
       "      <td>0.005727</td>\n",
       "      <td>0.000904</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>0.000611</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.002323</td>\n",
       "      <td>0.019139</td>\n",
       "      <td>0.019502</td>\n",
       "      <td>0.001772</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.007255</td>\n",
       "      <td>0.023766</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.007321</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>0.005040</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.000774</td>\n",
       "      <td>0.001430</td>\n",
       "      <td>0.006130</td>\n",
       "      <td>0.001048</td>\n",
       "      <td>0.001133</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.002298</td>\n",
       "      <td>0.002828</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.003642</td>\n",
       "      <td>0.000952</td>\n",
       "      <td>0.000677</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.000975</td>\n",
       "      <td>0.005267</td>\n",
       "      <td>0.001564</td>\n",
       "      <td>0.003916</td>\n",
       "      <td>0.001213</td>\n",
       "      <td>0.000913</td>\n",
       "      <td>0.000735</td>\n",
       "      <td>0.006935</td>\n",
       "      <td>0.002503</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.004584</td>\n",
       "      <td>0.000978</td>\n",
       "      <td>0.000939</td>\n",
       "      <td>0.004518</td>\n",
       "      <td>0.000990</td>\n",
       "      <td>0.005916</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.001518</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.000867</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.001782</td>\n",
       "      <td>0.000756</td>\n",
       "      <td>0.001270</td>\n",
       "      <td>0.002470</td>\n",
       "      <td>0.000629</td>\n",
       "      <td>0.001408</td>\n",
       "      <td>0.016983</td>\n",
       "      <td>0.002461</td>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.001277</td>\n",
       "      <td>0.002930</td>\n",
       "      <td>0.009511</td>\n",
       "      <td>0.003242</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.017534</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.004980</td>\n",
       "      <td>0.008243</td>\n",
       "      <td>0.000773</td>\n",
       "      <td>0.004087</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.003678</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.004100</td>\n",
       "      <td>0.001155</td>\n",
       "      <td>0.001150</td>\n",
       "      <td>0.003252</td>\n",
       "      <td>0.000858</td>\n",
       "      <td>0.001062</td>\n",
       "      <td>0.001324</td>\n",
       "      <td>0.002012</td>\n",
       "      <td>0.010128</td>\n",
       "      <td>0.006374</td>\n",
       "      <td>0.003956</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.001515</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.021697</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.000694</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.000812</td>\n",
       "      <td>0.001729</td>\n",
       "      <td>0.000399</td>\n",
       "      <td>0.001040</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.001798</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>0.003992</td>\n",
       "      <td>0.001492</td>\n",
       "      <td>0.001432</td>\n",
       "      <td>0.001144</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.004760</td>\n",
       "      <td>0.001233</td>\n",
       "      <td>0.000837</td>\n",
       "      <td>0.001044</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.002011</td>\n",
       "      <td>0.016355</td>\n",
       "      <td>0.001657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>id_001897cda</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.002284</td>\n",
       "      <td>0.002414</td>\n",
       "      <td>0.000958</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.003088</td>\n",
       "      <td>0.007465</td>\n",
       "      <td>0.006590</td>\n",
       "      <td>0.004119</td>\n",
       "      <td>0.004709</td>\n",
       "      <td>0.004537</td>\n",
       "      <td>0.000669</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.000979</td>\n",
       "      <td>0.000877</td>\n",
       "      <td>0.000792</td>\n",
       "      <td>0.001597</td>\n",
       "      <td>0.001471</td>\n",
       "      <td>0.003306</td>\n",
       "      <td>0.004484</td>\n",
       "      <td>0.002545</td>\n",
       "      <td>0.000691</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.000806</td>\n",
       "      <td>0.001377</td>\n",
       "      <td>0.000929</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.001696</td>\n",
       "      <td>0.001596</td>\n",
       "      <td>0.001088</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.001525</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.000600</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.001799</td>\n",
       "      <td>0.001382</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.000513</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.001001</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>0.000859</td>\n",
       "      <td>0.001626</td>\n",
       "      <td>0.002516</td>\n",
       "      <td>0.004549</td>\n",
       "      <td>0.001614</td>\n",
       "      <td>0.011117</td>\n",
       "      <td>0.014283</td>\n",
       "      <td>0.000666</td>\n",
       "      <td>0.004058</td>\n",
       "      <td>0.003701</td>\n",
       "      <td>0.003389</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.003811</td>\n",
       "      <td>0.000947</td>\n",
       "      <td>0.000932</td>\n",
       "      <td>0.003972</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.006538</td>\n",
       "      <td>0.001830</td>\n",
       "      <td>0.003253</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.004154</td>\n",
       "      <td>0.000922</td>\n",
       "      <td>0.000807</td>\n",
       "      <td>0.001034</td>\n",
       "      <td>0.006441</td>\n",
       "      <td>0.002031</td>\n",
       "      <td>0.000689</td>\n",
       "      <td>0.001619</td>\n",
       "      <td>0.000731</td>\n",
       "      <td>0.001142</td>\n",
       "      <td>0.001649</td>\n",
       "      <td>0.001437</td>\n",
       "      <td>0.000984</td>\n",
       "      <td>0.001856</td>\n",
       "      <td>0.001021</td>\n",
       "      <td>0.000874</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.002421</td>\n",
       "      <td>0.006547</td>\n",
       "      <td>0.000824</td>\n",
       "      <td>0.003041</td>\n",
       "      <td>0.008265</td>\n",
       "      <td>0.006019</td>\n",
       "      <td>0.001392</td>\n",
       "      <td>0.000930</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001918</td>\n",
       "      <td>0.004510</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.001475</td>\n",
       "      <td>0.001576</td>\n",
       "      <td>0.001209</td>\n",
       "      <td>0.004756</td>\n",
       "      <td>0.001328</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.002144</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.001863</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.003797</td>\n",
       "      <td>0.001422</td>\n",
       "      <td>0.001287</td>\n",
       "      <td>0.001521</td>\n",
       "      <td>0.007256</td>\n",
       "      <td>0.002602</td>\n",
       "      <td>0.001546</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.004151</td>\n",
       "      <td>0.045096</td>\n",
       "      <td>0.007188</td>\n",
       "      <td>0.006881</td>\n",
       "      <td>0.003211</td>\n",
       "      <td>0.000862</td>\n",
       "      <td>0.000791</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>0.001461</td>\n",
       "      <td>0.003191</td>\n",
       "      <td>0.000908</td>\n",
       "      <td>0.001226</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>0.001842</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.000882</td>\n",
       "      <td>0.001433</td>\n",
       "      <td>0.001119</td>\n",
       "      <td>0.003938</td>\n",
       "      <td>0.000894</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001465</td>\n",
       "      <td>0.000793</td>\n",
       "      <td>0.000734</td>\n",
       "      <td>0.001339</td>\n",
       "      <td>0.000707</td>\n",
       "      <td>0.000788</td>\n",
       "      <td>0.001061</td>\n",
       "      <td>0.001026</td>\n",
       "      <td>0.001963</td>\n",
       "      <td>0.001446</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.004224</td>\n",
       "      <td>0.004987</td>\n",
       "      <td>0.005188</td>\n",
       "      <td>0.002010</td>\n",
       "      <td>0.017528</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>0.014835</td>\n",
       "      <td>0.003752</td>\n",
       "      <td>0.002477</td>\n",
       "      <td>0.001929</td>\n",
       "      <td>0.065709</td>\n",
       "      <td>0.016018</td>\n",
       "      <td>0.001485</td>\n",
       "      <td>0.017228</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.002627</td>\n",
       "      <td>0.001128</td>\n",
       "      <td>0.002496</td>\n",
       "      <td>0.000924</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>0.001821</td>\n",
       "      <td>0.000856</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.001394</td>\n",
       "      <td>0.002343</td>\n",
       "      <td>0.001868</td>\n",
       "      <td>0.029378</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.006120</td>\n",
       "      <td>0.000960</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.002925</td>\n",
       "      <td>0.004674</td>\n",
       "      <td>0.000998</td>\n",
       "      <td>0.013017</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.003377</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.000723</td>\n",
       "      <td>0.000444</td>\n",
       "      <td>0.001201</td>\n",
       "      <td>0.001747</td>\n",
       "      <td>0.002955</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>0.001497</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.001064</td>\n",
       "      <td>0.004335</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>0.007064</td>\n",
       "      <td>0.000722</td>\n",
       "      <td>0.011016</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.003168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>id_002429b5b</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>id_00276f245</td>\n",
       "      <td>0.001307</td>\n",
       "      <td>0.001326</td>\n",
       "      <td>0.001984</td>\n",
       "      <td>0.012357</td>\n",
       "      <td>0.017698</td>\n",
       "      <td>0.004921</td>\n",
       "      <td>0.003847</td>\n",
       "      <td>0.004427</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.015394</td>\n",
       "      <td>0.032218</td>\n",
       "      <td>0.001861</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>0.002884</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.002979</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.004542</td>\n",
       "      <td>0.002314</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.001787</td>\n",
       "      <td>0.001363</td>\n",
       "      <td>0.001846</td>\n",
       "      <td>0.002593</td>\n",
       "      <td>0.007094</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.002401</td>\n",
       "      <td>0.002145</td>\n",
       "      <td>0.002870</td>\n",
       "      <td>0.001364</td>\n",
       "      <td>0.000883</td>\n",
       "      <td>0.000785</td>\n",
       "      <td>0.002759</td>\n",
       "      <td>0.000963</td>\n",
       "      <td>0.001045</td>\n",
       "      <td>0.001214</td>\n",
       "      <td>0.004056</td>\n",
       "      <td>0.006454</td>\n",
       "      <td>0.002991</td>\n",
       "      <td>0.013265</td>\n",
       "      <td>0.006367</td>\n",
       "      <td>0.006604</td>\n",
       "      <td>0.001007</td>\n",
       "      <td>0.001012</td>\n",
       "      <td>0.001138</td>\n",
       "      <td>0.003696</td>\n",
       "      <td>0.002115</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.001426</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.030979</td>\n",
       "      <td>0.002269</td>\n",
       "      <td>0.005029</td>\n",
       "      <td>0.002245</td>\n",
       "      <td>0.002615</td>\n",
       "      <td>0.001493</td>\n",
       "      <td>0.001680</td>\n",
       "      <td>0.008130</td>\n",
       "      <td>0.001479</td>\n",
       "      <td>0.001091</td>\n",
       "      <td>0.002908</td>\n",
       "      <td>0.000708</td>\n",
       "      <td>0.004596</td>\n",
       "      <td>0.003574</td>\n",
       "      <td>0.003421</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.014572</td>\n",
       "      <td>0.004592</td>\n",
       "      <td>0.001906</td>\n",
       "      <td>0.001470</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.003028</td>\n",
       "      <td>0.014815</td>\n",
       "      <td>0.012356</td>\n",
       "      <td>0.046277</td>\n",
       "      <td>0.009026</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.000911</td>\n",
       "      <td>0.005703</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.001699</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.001384</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.001010</td>\n",
       "      <td>0.000736</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>0.005656</td>\n",
       "      <td>0.001349</td>\n",
       "      <td>0.000934</td>\n",
       "      <td>0.001468</td>\n",
       "      <td>0.004416</td>\n",
       "      <td>0.014574</td>\n",
       "      <td>0.001978</td>\n",
       "      <td>0.001029</td>\n",
       "      <td>0.003810</td>\n",
       "      <td>0.001911</td>\n",
       "      <td>0.003526</td>\n",
       "      <td>0.029076</td>\n",
       "      <td>0.001883</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>0.005198</td>\n",
       "      <td>0.002820</td>\n",
       "      <td>0.000732</td>\n",
       "      <td>0.001858</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.002917</td>\n",
       "      <td>0.002843</td>\n",
       "      <td>0.001938</td>\n",
       "      <td>0.001179</td>\n",
       "      <td>0.001878</td>\n",
       "      <td>0.000799</td>\n",
       "      <td>0.001005</td>\n",
       "      <td>0.001086</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.002251</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.002423</td>\n",
       "      <td>0.001495</td>\n",
       "      <td>0.000674</td>\n",
       "      <td>0.001276</td>\n",
       "      <td>0.004629</td>\n",
       "      <td>0.002897</td>\n",
       "      <td>0.001123</td>\n",
       "      <td>0.006943</td>\n",
       "      <td>0.001305</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.003403</td>\n",
       "      <td>0.004359</td>\n",
       "      <td>0.004487</td>\n",
       "      <td>0.001003</td>\n",
       "      <td>0.002645</td>\n",
       "      <td>0.000996</td>\n",
       "      <td>0.002671</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.000775</td>\n",
       "      <td>0.006404</td>\n",
       "      <td>0.007006</td>\n",
       "      <td>0.003371</td>\n",
       "      <td>0.005211</td>\n",
       "      <td>0.001466</td>\n",
       "      <td>0.001629</td>\n",
       "      <td>0.000942</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.007621</td>\n",
       "      <td>0.002447</td>\n",
       "      <td>0.004322</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.004129</td>\n",
       "      <td>0.007369</td>\n",
       "      <td>0.001390</td>\n",
       "      <td>0.001561</td>\n",
       "      <td>0.002175</td>\n",
       "      <td>0.001094</td>\n",
       "      <td>0.002573</td>\n",
       "      <td>0.003714</td>\n",
       "      <td>0.000872</td>\n",
       "      <td>0.002640</td>\n",
       "      <td>0.000970</td>\n",
       "      <td>0.005509</td>\n",
       "      <td>0.001781</td>\n",
       "      <td>0.003476</td>\n",
       "      <td>0.001232</td>\n",
       "      <td>0.001655</td>\n",
       "      <td>0.000608</td>\n",
       "      <td>0.001106</td>\n",
       "      <td>0.001288</td>\n",
       "      <td>0.001659</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.016395</td>\n",
       "      <td>0.042332</td>\n",
       "      <td>0.003465</td>\n",
       "      <td>0.002883</td>\n",
       "      <td>0.004400</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.008949</td>\n",
       "      <td>0.002125</td>\n",
       "      <td>0.002160</td>\n",
       "      <td>0.001345</td>\n",
       "      <td>0.001172</td>\n",
       "      <td>0.004387</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002595</td>\n",
       "      <td>0.001031</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.001014</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.001296</td>\n",
       "      <td>0.002656</td>\n",
       "      <td>0.008923</td>\n",
       "      <td>0.003779</td>\n",
       "      <td>0.000997</td>\n",
       "      <td>0.001647</td>\n",
       "      <td>0.002328</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.002869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>id_0027f1083</td>\n",
       "      <td>0.001941</td>\n",
       "      <td>0.001910</td>\n",
       "      <td>0.002313</td>\n",
       "      <td>0.011852</td>\n",
       "      <td>0.013364</td>\n",
       "      <td>0.003880</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>0.002756</td>\n",
       "      <td>0.000919</td>\n",
       "      <td>0.009083</td>\n",
       "      <td>0.016433</td>\n",
       "      <td>0.002346</td>\n",
       "      <td>0.000764</td>\n",
       "      <td>0.001229</td>\n",
       "      <td>0.001636</td>\n",
       "      <td>0.001795</td>\n",
       "      <td>0.003560</td>\n",
       "      <td>0.004315</td>\n",
       "      <td>0.002720</td>\n",
       "      <td>0.002250</td>\n",
       "      <td>0.004175</td>\n",
       "      <td>0.004759</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.003566</td>\n",
       "      <td>0.001373</td>\n",
       "      <td>0.001295</td>\n",
       "      <td>0.001318</td>\n",
       "      <td>0.001507</td>\n",
       "      <td>0.006499</td>\n",
       "      <td>0.003329</td>\n",
       "      <td>0.002266</td>\n",
       "      <td>0.003983</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>0.000821</td>\n",
       "      <td>0.000902</td>\n",
       "      <td>0.002440</td>\n",
       "      <td>0.000910</td>\n",
       "      <td>0.000744</td>\n",
       "      <td>0.001122</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>0.007119</td>\n",
       "      <td>0.002696</td>\n",
       "      <td>0.010467</td>\n",
       "      <td>0.007131</td>\n",
       "      <td>0.009252</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.002641</td>\n",
       "      <td>0.000921</td>\n",
       "      <td>0.003978</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.001994</td>\n",
       "      <td>0.001334</td>\n",
       "      <td>0.001204</td>\n",
       "      <td>0.006945</td>\n",
       "      <td>0.002685</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>0.001610</td>\n",
       "      <td>0.001484</td>\n",
       "      <td>0.005327</td>\n",
       "      <td>0.001977</td>\n",
       "      <td>0.002287</td>\n",
       "      <td>0.004200</td>\n",
       "      <td>0.000800</td>\n",
       "      <td>0.004797</td>\n",
       "      <td>0.002782</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.001042</td>\n",
       "      <td>0.001319</td>\n",
       "      <td>0.016704</td>\n",
       "      <td>0.006005</td>\n",
       "      <td>0.002248</td>\n",
       "      <td>0.002174</td>\n",
       "      <td>0.001165</td>\n",
       "      <td>0.004799</td>\n",
       "      <td>0.035387</td>\n",
       "      <td>0.004459</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.001462</td>\n",
       "      <td>0.000950</td>\n",
       "      <td>0.000798</td>\n",
       "      <td>0.011244</td>\n",
       "      <td>0.002132</td>\n",
       "      <td>0.001741</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.002149</td>\n",
       "      <td>0.001445</td>\n",
       "      <td>0.001628</td>\n",
       "      <td>0.000671</td>\n",
       "      <td>0.001557</td>\n",
       "      <td>0.001085</td>\n",
       "      <td>0.005848</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.000810</td>\n",
       "      <td>0.001367</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.004137</td>\n",
       "      <td>0.018271</td>\n",
       "      <td>0.001935</td>\n",
       "      <td>0.001164</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>0.002445</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>0.000693</td>\n",
       "      <td>0.001483</td>\n",
       "      <td>0.003910</td>\n",
       "      <td>0.000991</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.000770</td>\n",
       "      <td>0.002692</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>0.002851</td>\n",
       "      <td>0.003466</td>\n",
       "      <td>0.002294</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.001191</td>\n",
       "      <td>0.001163</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>0.001410</td>\n",
       "      <td>0.003256</td>\n",
       "      <td>0.000848</td>\n",
       "      <td>0.000897</td>\n",
       "      <td>0.000855</td>\n",
       "      <td>0.005754</td>\n",
       "      <td>0.001788</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>0.005060</td>\n",
       "      <td>0.001065</td>\n",
       "      <td>0.002420</td>\n",
       "      <td>0.003316</td>\n",
       "      <td>0.001667</td>\n",
       "      <td>0.005945</td>\n",
       "      <td>0.001117</td>\n",
       "      <td>0.002725</td>\n",
       "      <td>0.001548</td>\n",
       "      <td>0.001985</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.001397</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>0.005092</td>\n",
       "      <td>0.003289</td>\n",
       "      <td>0.001454</td>\n",
       "      <td>0.001522</td>\n",
       "      <td>0.002907</td>\n",
       "      <td>0.001672</td>\n",
       "      <td>0.001395</td>\n",
       "      <td>0.011309</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>0.002787</td>\n",
       "      <td>0.001736</td>\n",
       "      <td>0.005339</td>\n",
       "      <td>0.003553</td>\n",
       "      <td>0.007197</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.004945</td>\n",
       "      <td>0.002043</td>\n",
       "      <td>0.003408</td>\n",
       "      <td>0.004117</td>\n",
       "      <td>0.001260</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>0.001352</td>\n",
       "      <td>0.005363</td>\n",
       "      <td>0.001601</td>\n",
       "      <td>0.004731</td>\n",
       "      <td>0.002077</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.001440</td>\n",
       "      <td>0.002359</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.002216</td>\n",
       "      <td>0.009359</td>\n",
       "      <td>0.009396</td>\n",
       "      <td>0.002345</td>\n",
       "      <td>0.002130</td>\n",
       "      <td>0.001592</td>\n",
       "      <td>0.002198</td>\n",
       "      <td>0.015851</td>\n",
       "      <td>0.002187</td>\n",
       "      <td>0.001060</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.001643</td>\n",
       "      <td>0.003504</td>\n",
       "      <td>0.001078</td>\n",
       "      <td>0.001423</td>\n",
       "      <td>0.003557</td>\n",
       "      <td>0.002397</td>\n",
       "      <td>0.001180</td>\n",
       "      <td>0.001948</td>\n",
       "      <td>0.003254</td>\n",
       "      <td>0.001346</td>\n",
       "      <td>0.001177</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.003724</td>\n",
       "      <td>0.001987</td>\n",
       "      <td>0.001718</td>\n",
       "      <td>0.001147</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.002267</td>\n",
       "      <td>0.000635</td>\n",
       "      <td>0.002229</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         sig_id  5-alpha_reductase_inhibitor  11-beta-hsd1_inhibitor  \\\n",
       "0  id_0004d9e33                     0.001363                0.001740   \n",
       "1  id_001897cda                     0.000660                0.001010   \n",
       "2  id_002429b5b                     0.000000                0.000000   \n",
       "3  id_00276f245                     0.001307                0.001326   \n",
       "4  id_0027f1083                     0.001941                0.001910   \n",
       "\n",
       "   acat_inhibitor  acetylcholine_receptor_agonist  \\\n",
       "0        0.002995                        0.010900   \n",
       "1        0.002284                        0.002414   \n",
       "2        0.000000                        0.000000   \n",
       "3        0.001984                        0.012357   \n",
       "4        0.002313                        0.011852   \n",
       "\n",
       "   acetylcholine_receptor_antagonist  acetylcholinesterase_inhibitor  \\\n",
       "0                           0.017746                        0.005145   \n",
       "1                           0.000958                        0.001474   \n",
       "2                           0.000000                        0.000000   \n",
       "3                           0.017698                        0.004921   \n",
       "4                           0.013364                        0.003880   \n",
       "\n",
       "   adenosine_receptor_agonist  adenosine_receptor_antagonist  \\\n",
       "0                    0.002305                       0.007631   \n",
       "1                    0.003088                       0.007465   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.003847                       0.004427   \n",
       "4                    0.004587                       0.002756   \n",
       "\n",
       "   adenylyl_cyclase_activator  adrenergic_receptor_agonist  \\\n",
       "0                    0.000568                     0.009014   \n",
       "1                    0.006590                     0.004119   \n",
       "2                    0.000000                     0.000000   \n",
       "3                    0.000579                     0.015394   \n",
       "4                    0.000919                     0.009083   \n",
       "\n",
       "   adrenergic_receptor_antagonist  akt_inhibitor  \\\n",
       "0                        0.012010       0.000769   \n",
       "1                        0.004709       0.004537   \n",
       "2                        0.000000       0.000000   \n",
       "3                        0.032218       0.001861   \n",
       "4                        0.016433       0.002346   \n",
       "\n",
       "   aldehyde_dehydrogenase_inhibitor  alk_inhibitor  ampk_activator  analgesic  \\\n",
       "0                          0.001191       0.000678        0.001332   0.001286   \n",
       "1                          0.000669       0.008861        0.000979   0.000877   \n",
       "2                          0.000000       0.000000        0.000000   0.000000   \n",
       "3                          0.000832       0.002884        0.001581   0.002043   \n",
       "4                          0.000764       0.001229        0.001636   0.001795   \n",
       "\n",
       "   androgen_receptor_agonist  androgen_receptor_antagonist  \\\n",
       "0                   0.002951                      0.007533   \n",
       "1                   0.000792                      0.001597   \n",
       "2                   0.000000                      0.000000   \n",
       "3                   0.002979                      0.004885   \n",
       "4                   0.003560                      0.004315   \n",
       "\n",
       "   anesthetic_-_local  angiogenesis_inhibitor  \\\n",
       "0            0.007437                0.002735   \n",
       "1            0.001471                0.003306   \n",
       "2            0.000000                0.000000   \n",
       "3            0.004542                0.002314   \n",
       "4            0.002720                0.002250   \n",
       "\n",
       "   angiotensin_receptor_antagonist  anti-inflammatory  antiarrhythmic  \\\n",
       "0                         0.002897           0.004447        0.000837   \n",
       "1                         0.004484           0.002545        0.000691   \n",
       "2                         0.000000           0.000000        0.000000   \n",
       "3                         0.002619           0.002328        0.001222   \n",
       "4                         0.004175           0.004759        0.001084   \n",
       "\n",
       "   antibiotic  anticonvulsant  antifungal  antihistamine  antimalarial  \\\n",
       "0    0.002688        0.000826    0.001195       0.001124      0.001729   \n",
       "1    0.001276        0.000806    0.001377       0.000929      0.000666   \n",
       "2    0.000000        0.000000    0.000000       0.000000      0.000000   \n",
       "3    0.003325        0.001787    0.001363       0.001846      0.002593   \n",
       "4    0.003566        0.001373    0.001295       0.001318      0.001507   \n",
       "\n",
       "   antioxidant  antiprotozoal  antiviral  apoptosis_stimulant  \\\n",
       "0     0.003593       0.002225   0.001192             0.005218   \n",
       "1     0.001696       0.001596   0.001088             0.002883   \n",
       "2     0.000000       0.000000   0.000000             0.000000   \n",
       "3     0.007094       0.002782   0.002401             0.002145   \n",
       "4     0.006499       0.003329   0.002266             0.003983   \n",
       "\n",
       "   aromatase_inhibitor  atm_kinase_inhibitor  \\\n",
       "0             0.005183              0.000573   \n",
       "1             0.001290              0.001525   \n",
       "2             0.000000              0.000000   \n",
       "3             0.002870              0.001364   \n",
       "4             0.003289              0.000864   \n",
       "\n",
       "   atp-sensitive_potassium_channel_antagonist  atp_synthase_inhibitor  \\\n",
       "0                                    0.000702                0.001370   \n",
       "1                                    0.000775                0.000600   \n",
       "2                                    0.000000                0.000000   \n",
       "3                                    0.000883                0.000785   \n",
       "4                                    0.000821                0.000902   \n",
       "\n",
       "   atpase_inhibitor  atr_kinase_inhibitor  aurora_kinase_inhibitor  \\\n",
       "0          0.005443              0.000604                 0.000819   \n",
       "1          0.002764              0.003373                 0.004592   \n",
       "2          0.000000              0.000000                 0.000000   \n",
       "3          0.002759              0.000963                 0.001045   \n",
       "4          0.002440              0.000910                 0.000744   \n",
       "\n",
       "   autotaxin_inhibitor  bacterial_30s_ribosomal_subunit_inhibitor  \\\n",
       "0             0.000766                                   0.005091   \n",
       "1             0.001799                                   0.001382   \n",
       "2             0.000000                                   0.000000   \n",
       "3             0.001214                                   0.004056   \n",
       "4             0.001122                                   0.006659   \n",
       "\n",
       "   bacterial_50s_ribosomal_subunit_inhibitor  bacterial_antifolate  \\\n",
       "0                                   0.010059              0.001931   \n",
       "1                                   0.001060              0.000513   \n",
       "2                                   0.000000              0.000000   \n",
       "3                                   0.006454              0.002991   \n",
       "4                                   0.007119              0.002696   \n",
       "\n",
       "   bacterial_cell_wall_synthesis_inhibitor  bacterial_dna_gyrase_inhibitor  \\\n",
       "0                                 0.007161                        0.008064   \n",
       "1                                 0.002440                        0.001001   \n",
       "2                                 0.000000                        0.000000   \n",
       "3                                 0.013265                        0.006367   \n",
       "4                                 0.010467                        0.007131   \n",
       "\n",
       "   bacterial_dna_inhibitor  bacterial_membrane_integrity_inhibitor  \\\n",
       "0                 0.006672                                0.001179   \n",
       "1                 0.001625                                0.000859   \n",
       "2                 0.000000                                0.000000   \n",
       "3                 0.006604                                0.001007   \n",
       "4                 0.009252                                0.001011   \n",
       "\n",
       "   bcl_inhibitor  bcr-abl_inhibitor  benzodiazepine_receptor_agonist  \\\n",
       "0       0.003512           0.000803                         0.003224   \n",
       "1       0.001626           0.002516                         0.004549   \n",
       "2       0.000000           0.000000                         0.000000   \n",
       "3       0.001012           0.001138                         0.003696   \n",
       "4       0.002641           0.000921                         0.003978   \n",
       "\n",
       "   beta_amyloid_inhibitor  bromodomain_inhibitor  btk_inhibitor  \\\n",
       "0                0.002067               0.004094       0.000771   \n",
       "1                0.001614               0.011117       0.014283   \n",
       "2                0.000000               0.000000       0.000000   \n",
       "3                0.002115               0.000848       0.001426   \n",
       "4                0.002129               0.001994       0.001334   \n",
       "\n",
       "   calcineurin_inhibitor  calcium_channel_blocker  \\\n",
       "0               0.000943                 0.017807   \n",
       "1               0.000666                 0.004058   \n",
       "2               0.000000                 0.000000   \n",
       "3               0.001495                 0.030979   \n",
       "4               0.001204                 0.006945   \n",
       "\n",
       "   cannabinoid_receptor_agonist  cannabinoid_receptor_antagonist  \\\n",
       "0                      0.002107                         0.002200   \n",
       "1                      0.003701                         0.003389   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.002269                         0.005029   \n",
       "4                      0.002685                         0.002787   \n",
       "\n",
       "   carbonic_anhydrase_inhibitor  casein_kinase_inhibitor  caspase_activator  \\\n",
       "0                      0.002449                 0.001749           0.002820   \n",
       "1                      0.001487                 0.003811           0.000947   \n",
       "2                      0.000000                 0.000000           0.000000   \n",
       "3                      0.002245                 0.002615           0.001493   \n",
       "4                      0.002872                 0.002863           0.001610   \n",
       "\n",
       "   catechol_o_methyltransferase_inhibitor  cc_chemokine_receptor_antagonist  \\\n",
       "0                                0.001595                          0.006084   \n",
       "1                                0.000932                          0.003972   \n",
       "2                                0.000000                          0.000000   \n",
       "3                                0.001680                          0.008130   \n",
       "4                                0.001484                          0.005327   \n",
       "\n",
       "   cck_receptor_antagonist  cdk_inhibitor  chelating_agent  chk_inhibitor  \\\n",
       "0                 0.002007       0.000581         0.004096       0.000666   \n",
       "1                 0.000983       0.006538         0.001830       0.003253   \n",
       "2                 0.000000       0.000000         0.000000       0.000000   \n",
       "3                 0.001479       0.001091         0.002908       0.000708   \n",
       "4                 0.001977       0.002287         0.004200       0.000800   \n",
       "\n",
       "   chloride_channel_blocker  cholesterol_inhibitor  \\\n",
       "0                  0.002987               0.004083   \n",
       "1                  0.001032               0.004154   \n",
       "2                  0.000000               0.000000   \n",
       "3                  0.004596               0.003574   \n",
       "4                  0.004797               0.002782   \n",
       "\n",
       "   cholinergic_receptor_antagonist  coagulation_factor_inhibitor  \\\n",
       "0                         0.006264                      0.000866   \n",
       "1                         0.000922                      0.000807   \n",
       "2                         0.000000                      0.000000   \n",
       "3                         0.003421                      0.001163   \n",
       "4                         0.003147                      0.001042   \n",
       "\n",
       "   corticosteroid_agonist  cyclooxygenase_inhibitor  \\\n",
       "0                0.001420                  0.048529   \n",
       "1                0.001034                  0.006441   \n",
       "2                0.000000                  0.000000   \n",
       "3                0.000973                  0.014572   \n",
       "4                0.001319                  0.016704   \n",
       "\n",
       "   cytochrome_p450_inhibitor  dihydrofolate_reductase_inhibitor  \\\n",
       "0                   0.005657                           0.001224   \n",
       "1                   0.002031                           0.000689   \n",
       "2                   0.000000                           0.000000   \n",
       "3                   0.004592                           0.001906   \n",
       "4                   0.006005                           0.002248   \n",
       "\n",
       "   dipeptidyl_peptidase_inhibitor  diuretic  dna_alkylating_agent  \\\n",
       "0                        0.003256  0.000918              0.004145   \n",
       "1                        0.001619  0.000731              0.001142   \n",
       "2                        0.000000  0.000000              0.000000   \n",
       "3                        0.001470  0.000983              0.003028   \n",
       "4                        0.002174  0.001165              0.004799   \n",
       "\n",
       "   dna_inhibitor  dopamine_receptor_agonist  dopamine_receptor_antagonist  \\\n",
       "0       0.016388                   0.007638                      0.012645   \n",
       "1       0.001649                   0.001437                      0.000984   \n",
       "2       0.000000                   0.000000                      0.000000   \n",
       "3       0.014815                   0.012356                      0.046277   \n",
       "4       0.035387                   0.004459                      0.006531   \n",
       "\n",
       "   egfr_inhibitor  elastase_inhibitor  erbb2_inhibitor  \\\n",
       "0        0.000656            0.001305         0.000669   \n",
       "1        0.001856            0.001021         0.000874   \n",
       "2        0.000000            0.000000         0.000000   \n",
       "3        0.009026            0.001013         0.000911   \n",
       "4        0.001462            0.000950         0.000798   \n",
       "\n",
       "   estrogen_receptor_agonist  estrogen_receptor_antagonist  faah_inhibitor  \\\n",
       "0                   0.021728                      0.002088        0.005727   \n",
       "1                   0.003048                      0.002421        0.006547   \n",
       "2                   0.000000                      0.000000        0.000000   \n",
       "3                   0.005703                      0.004115        0.001699   \n",
       "4                   0.011244                      0.002132        0.001741   \n",
       "\n",
       "   farnesyltransferase_inhibitor  fatty_acid_receptor_agonist  fgfr_inhibitor  \\\n",
       "0                       0.000904                     0.001882        0.000781   \n",
       "1                       0.000824                     0.003041        0.008265   \n",
       "2                       0.000000                     0.000000        0.000000   \n",
       "3                       0.001164                     0.001384        0.001897   \n",
       "4                       0.001043                     0.002149        0.001445   \n",
       "\n",
       "   flt3_inhibitor  focal_adhesion_kinase_inhibitor  free_radical_scavenger  \\\n",
       "0        0.000611                         0.001189                0.001679   \n",
       "1        0.006019                         0.001392                0.000930   \n",
       "2        0.000000                         0.000000                0.000000   \n",
       "3        0.001010                         0.000736                0.001548   \n",
       "4        0.001628                         0.000671                0.001557   \n",
       "\n",
       "   fungal_squalene_epoxidase_inhibitor  gaba_receptor_agonist  \\\n",
       "0                             0.002323               0.019139   \n",
       "1                             0.001012               0.001918   \n",
       "2                             0.000000               0.000000   \n",
       "3                             0.001643               0.004172   \n",
       "4                             0.001085               0.005848   \n",
       "\n",
       "   gaba_receptor_antagonist  gamma_secretase_inhibitor  \\\n",
       "0                  0.019502                   0.001772   \n",
       "1                  0.004510                   0.003325   \n",
       "2                  0.000000                   0.000000   \n",
       "3                  0.005656                   0.001349   \n",
       "4                  0.005856                   0.000810   \n",
       "\n",
       "   glucocorticoid_receptor_agonist  glutamate_inhibitor  \\\n",
       "0                         0.002178             0.001561   \n",
       "1                         0.001475             0.001576   \n",
       "2                         0.000000             0.000000   \n",
       "3                         0.000934             0.001468   \n",
       "4                         0.001367             0.002258   \n",
       "\n",
       "   glutamate_receptor_agonist  glutamate_receptor_antagonist  \\\n",
       "0                    0.007255                       0.023766   \n",
       "1                    0.001209                       0.004756   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.004416                       0.014574   \n",
       "4                    0.004137                       0.018271   \n",
       "\n",
       "   gonadotropin_receptor_agonist  gsk_inhibitor  hcv_inhibitor  \\\n",
       "0                       0.001814       0.001011       0.007321   \n",
       "1                       0.001328       0.004700       0.003880   \n",
       "2                       0.000000       0.000000       0.000000   \n",
       "3                       0.001978       0.001029       0.003810   \n",
       "4                       0.001935       0.001164       0.004369   \n",
       "\n",
       "   hdac_inhibitor  histamine_receptor_agonist  histamine_receptor_antagonist  \\\n",
       "0        0.002003                    0.005040                       0.008422   \n",
       "1        0.002144                    0.000749                       0.001863   \n",
       "2        0.000000                    0.000000                       0.000000   \n",
       "3        0.001911                    0.003526                       0.029076   \n",
       "4        0.002445                    0.002949                       0.008645   \n",
       "\n",
       "   histone_lysine_demethylase_inhibitor  \\\n",
       "0                              0.000774   \n",
       "1                              0.002133   \n",
       "2                              0.000000   \n",
       "3                              0.001883   \n",
       "4                              0.000693   \n",
       "\n",
       "   histone_lysine_methyltransferase_inhibitor  hiv_inhibitor  hmgcr_inhibitor  \\\n",
       "0                                    0.001430       0.006130         0.001048   \n",
       "1                                    0.003797       0.001422         0.001287   \n",
       "2                                    0.000000       0.000000         0.000000   \n",
       "3                                    0.002752       0.005198         0.002820   \n",
       "4                                    0.001483       0.003910         0.000991   \n",
       "\n",
       "   hsp_inhibitor  igf-1_inhibitor  ikk_inhibitor  \\\n",
       "0       0.001133         0.000806       0.001843   \n",
       "1       0.001521         0.007256       0.002602   \n",
       "2       0.000000         0.000000       0.000000   \n",
       "3       0.000732         0.001858       0.001036   \n",
       "4       0.001617         0.000770       0.002692   \n",
       "\n",
       "   imidazoline_receptor_agonist  immunosuppressant  insulin_secretagogue  \\\n",
       "0                      0.002851           0.002298              0.002828   \n",
       "1                      0.001546           0.002115              0.004151   \n",
       "2                      0.000000           0.000000              0.000000   \n",
       "3                      0.002917           0.002843              0.001938   \n",
       "4                      0.002286           0.003222              0.002851   \n",
       "\n",
       "   insulin_sensitizer  integrin_inhibitor  jak_inhibitor  kit_inhibitor  \\\n",
       "0            0.001551            0.003642       0.000952       0.000677   \n",
       "1            0.045096            0.007188       0.006881       0.003211   \n",
       "2            0.000000            0.000000       0.000000       0.000000   \n",
       "3            0.001179            0.001878       0.000799       0.001005   \n",
       "4            0.003466            0.002294       0.001065       0.001320   \n",
       "\n",
       "   laxative  leukotriene_inhibitor  leukotriene_receptor_antagonist  \\\n",
       "0  0.001045               0.000975                         0.005267   \n",
       "1  0.000862               0.000791                         0.004370   \n",
       "2  0.000000               0.000000                         0.000000   \n",
       "3  0.001086               0.001211                         0.002251   \n",
       "4  0.001191               0.001163                         0.002952   \n",
       "\n",
       "   lipase_inhibitor  lipoxygenase_inhibitor  lxr_agonist  mdm_inhibitor  \\\n",
       "0          0.001564                0.003916     0.001213       0.000913   \n",
       "1          0.001461                0.003191     0.000908       0.001226   \n",
       "2          0.000000                0.000000     0.000000       0.000000   \n",
       "3          0.001396                0.002423     0.001495       0.000674   \n",
       "4          0.001410                0.003256     0.000848       0.000897   \n",
       "\n",
       "   mek_inhibitor  membrane_integrity_inhibitor  \\\n",
       "0       0.000735                      0.006935   \n",
       "1       0.001967                      0.001842   \n",
       "2       0.000000                      0.000000   \n",
       "3       0.001276                      0.004629   \n",
       "4       0.000855                      0.005754   \n",
       "\n",
       "   mineralocorticoid_receptor_antagonist  monoacylglycerol_lipase_inhibitor  \\\n",
       "0                               0.002503                           0.001987   \n",
       "1                               0.001290                           0.000882   \n",
       "2                               0.000000                           0.000000   \n",
       "3                               0.002897                           0.001123   \n",
       "4                               0.001788                           0.001211   \n",
       "\n",
       "   monoamine_oxidase_inhibitor  monopolar_spindle_1_kinase_inhibitor  \\\n",
       "0                     0.004584                              0.000978   \n",
       "1                     0.001433                              0.001119   \n",
       "2                     0.000000                              0.000000   \n",
       "3                     0.006943                              0.001305   \n",
       "4                     0.005060                              0.001065   \n",
       "\n",
       "   mtor_inhibitor  mucolytic_agent  neuropeptide_receptor_antagonist  \\\n",
       "0        0.000939         0.004518                          0.000990   \n",
       "1        0.003938         0.000894                          0.001317   \n",
       "2        0.000000         0.000000                          0.000000   \n",
       "3        0.001882         0.003403                          0.004359   \n",
       "4        0.002420         0.003316                          0.001667   \n",
       "\n",
       "   nfkb_inhibitor  nicotinic_receptor_agonist  nitric_oxide_donor  \\\n",
       "0        0.005916                    0.001168            0.001518   \n",
       "1        0.001465                    0.000793            0.000734   \n",
       "2        0.000000                    0.000000            0.000000   \n",
       "3        0.004487                    0.001003            0.002645   \n",
       "4        0.005945                    0.001117            0.002725   \n",
       "\n",
       "   nitric_oxide_production_inhibitor  nitric_oxide_synthase_inhibitor  \\\n",
       "0                           0.001249                         0.001426   \n",
       "1                           0.001339                         0.000707   \n",
       "2                           0.000000                         0.000000   \n",
       "3                           0.000996                         0.002671   \n",
       "4                           0.001548                         0.001985   \n",
       "\n",
       "   norepinephrine_reuptake_inhibitor  nrf2_activator  opioid_receptor_agonist  \\\n",
       "0                           0.000867        0.001408                 0.002420   \n",
       "1                           0.000788        0.001061                 0.001026   \n",
       "2                           0.000000        0.000000                 0.000000   \n",
       "3                           0.001325        0.000775                 0.006404   \n",
       "4                           0.001043        0.001397                 0.003267   \n",
       "\n",
       "   opioid_receptor_antagonist  orexin_receptor_antagonist  p38_mapk_inhibitor  \\\n",
       "0                    0.005504                    0.001782            0.000756   \n",
       "1                    0.001963                    0.001446            0.001305   \n",
       "2                    0.000000                    0.000000            0.000000   \n",
       "3                    0.007006                    0.003371            0.005211   \n",
       "4                    0.005092                    0.003289            0.001454   \n",
       "\n",
       "   p-glycoprotein_inhibitor  parp_inhibitor  pdgfr_inhibitor  pdk_inhibitor  \\\n",
       "0                  0.001270        0.002470         0.000629       0.001408   \n",
       "1                  0.004224        0.004987         0.005188       0.002010   \n",
       "2                  0.000000        0.000000         0.000000       0.000000   \n",
       "3                  0.001466        0.001629         0.000942       0.001112   \n",
       "4                  0.001522        0.002907         0.001672       0.001395   \n",
       "\n",
       "   phosphodiesterase_inhibitor  phospholipase_inhibitor  pi3k_inhibitor  \\\n",
       "0                     0.016983                 0.002461        0.001875   \n",
       "1                     0.017528                 0.000783        0.014835   \n",
       "2                     0.000000                 0.000000        0.000000   \n",
       "3                     0.007621                 0.002447        0.004322   \n",
       "4                     0.011309                 0.001679        0.002787   \n",
       "\n",
       "   pkc_inhibitor  potassium_channel_activator  potassium_channel_antagonist  \\\n",
       "0       0.001277                     0.002930                      0.009511   \n",
       "1       0.003752                     0.002477                      0.001929   \n",
       "2       0.000000                     0.000000                      0.000000   \n",
       "3       0.002053                     0.004129                      0.007369   \n",
       "4       0.001736                     0.005339                      0.003553   \n",
       "\n",
       "   ppar_receptor_agonist  ppar_receptor_antagonist  \\\n",
       "0               0.003242                  0.003001   \n",
       "1               0.065709                  0.016018   \n",
       "2               0.000000                  0.000000   \n",
       "3               0.001390                  0.001561   \n",
       "4               0.007197                  0.002079   \n",
       "\n",
       "   progesterone_receptor_agonist  progesterone_receptor_antagonist  \\\n",
       "0                       0.017534                          0.002640   \n",
       "1                       0.001485                          0.017228   \n",
       "2                       0.000000                          0.000000   \n",
       "3                       0.002175                          0.001094   \n",
       "4                       0.004945                          0.002043   \n",
       "\n",
       "   prostaglandin_inhibitor  prostanoid_receptor_antagonist  \\\n",
       "0                 0.004980                        0.008243   \n",
       "1                 0.001510                        0.002627   \n",
       "2                 0.000000                        0.000000   \n",
       "3                 0.002573                        0.003714   \n",
       "4                 0.003408                        0.004117   \n",
       "\n",
       "   proteasome_inhibitor  protein_kinase_inhibitor  \\\n",
       "0              0.000773                  0.004087   \n",
       "1              0.001128                  0.002496   \n",
       "2              0.000000                  0.000000   \n",
       "3              0.000872                  0.002640   \n",
       "4              0.001260                  0.002238   \n",
       "\n",
       "   protein_phosphatase_inhibitor  protein_synthesis_inhibitor  \\\n",
       "0                       0.000736                     0.003678   \n",
       "1                       0.000924                     0.000895   \n",
       "2                       0.000000                     0.000000   \n",
       "3                       0.000970                     0.005509   \n",
       "4                       0.001352                     0.005363   \n",
       "\n",
       "   protein_tyrosine_kinase_inhibitor  radiopaque_medium  raf_inhibitor  \\\n",
       "0                           0.001073           0.004100       0.001155   \n",
       "1                           0.001821           0.000856       0.000722   \n",
       "2                           0.000000           0.000000       0.000000   \n",
       "3                           0.001781           0.003476       0.001232   \n",
       "4                           0.001601           0.004731       0.002077   \n",
       "\n",
       "   ras_gtpase_inhibitor  retinoid_receptor_agonist  \\\n",
       "0              0.001150                   0.003252   \n",
       "1              0.001394                   0.002343   \n",
       "2              0.000000                   0.000000   \n",
       "3              0.001655                   0.000608   \n",
       "4              0.001009                   0.001104   \n",
       "\n",
       "   retinoid_receptor_antagonist  rho_associated_kinase_inhibitor  \\\n",
       "0                      0.000858                         0.001062   \n",
       "1                      0.001868                         0.029378   \n",
       "2                      0.000000                         0.000000   \n",
       "3                      0.001106                         0.001288   \n",
       "4                      0.001440                         0.002359   \n",
       "\n",
       "   ribonucleoside_reductase_inhibitor  rna_polymerase_inhibitor  \\\n",
       "0                            0.001324                  0.002012   \n",
       "1                            0.001283                  0.002884   \n",
       "2                            0.000000                  0.000000   \n",
       "3                            0.001659                  0.001337   \n",
       "4                            0.003691                  0.002216   \n",
       "\n",
       "   serotonin_receptor_agonist  serotonin_receptor_antagonist  \\\n",
       "0                    0.010128                       0.006374   \n",
       "1                    0.006120                       0.000960   \n",
       "2                    0.000000                       0.000000   \n",
       "3                    0.016395                       0.042332   \n",
       "4                    0.009359                       0.009396   \n",
       "\n",
       "   serotonin_reuptake_inhibitor  sigma_receptor_agonist  \\\n",
       "0                      0.003956                0.003465   \n",
       "1                      0.000723                0.001078   \n",
       "2                      0.000000                0.000000   \n",
       "3                      0.003465                0.002883   \n",
       "4                      0.002345                0.002130   \n",
       "\n",
       "   sigma_receptor_antagonist  smoothened_receptor_antagonist  \\\n",
       "0                   0.001515                        0.001575   \n",
       "1                   0.001032                        0.002925   \n",
       "2                   0.000000                        0.000000   \n",
       "3                   0.004400                        0.002179   \n",
       "4                   0.001592                        0.002198   \n",
       "\n",
       "   sodium_channel_inhibitor  sphingosine_receptor_agonist  src_inhibitor  \\\n",
       "0                  0.021697                      0.003108       0.000694   \n",
       "1                  0.004674                      0.000998       0.013017   \n",
       "2                  0.000000                      0.000000       0.000000   \n",
       "3                  0.008949                      0.002125       0.002160   \n",
       "4                  0.015851                      0.002187       0.001060   \n",
       "\n",
       "    steroid  syk_inhibitor  tachykinin_antagonist  \\\n",
       "0  0.000955       0.000812               0.001729   \n",
       "1  0.000944       0.003377               0.002583   \n",
       "2  0.000000       0.000000               0.000000   \n",
       "3  0.001345       0.001172               0.004387   \n",
       "4  0.001510       0.001643               0.003504   \n",
       "\n",
       "   tgf-beta_receptor_inhibitor  thrombin_inhibitor  \\\n",
       "0                     0.000399            0.001040   \n",
       "1                     0.004184            0.000723   \n",
       "2                     0.000000            0.000000   \n",
       "3                     0.001059            0.002198   \n",
       "4                     0.001078            0.001423   \n",
       "\n",
       "   thymidylate_synthase_inhibitor  tlr_agonist  tlr_antagonist  tnf_inhibitor  \\\n",
       "0                        0.001517     0.001798        0.001173       0.003992   \n",
       "1                        0.000444     0.001201        0.001747       0.002955   \n",
       "2                        0.000000     0.000000        0.000000       0.000000   \n",
       "3                        0.002583     0.002595        0.001031       0.001768   \n",
       "4                        0.003557     0.002397        0.001180       0.001948   \n",
       "\n",
       "   topoisomerase_inhibitor  transient_receptor_potential_channel_antagonist  \\\n",
       "0                 0.001492                                         0.001432   \n",
       "1                 0.005852                                         0.001497   \n",
       "2                 0.000000                                         0.000000   \n",
       "3                 0.001014                                         0.001789   \n",
       "4                 0.003254                                         0.001346   \n",
       "\n",
       "   tropomyosin_receptor_kinase_inhibitor  trpv_agonist  trpv_antagonist  \\\n",
       "0                               0.001144      0.002809         0.004760   \n",
       "1                               0.001118      0.001064         0.004335   \n",
       "2                               0.000000      0.000000         0.000000   \n",
       "3                               0.001015      0.001296         0.002656   \n",
       "4                               0.001177      0.001059         0.003724   \n",
       "\n",
       "   tubulin_inhibitor  tyrosine_kinase_inhibitor  \\\n",
       "0           0.001233                   0.000837   \n",
       "1           0.000547                   0.007064   \n",
       "2           0.000000                   0.000000   \n",
       "3           0.008923                   0.003779   \n",
       "4           0.001987                   0.001718   \n",
       "\n",
       "   ubiquitin_specific_protease_inhibitor  vegfr_inhibitor  vitamin_b  \\\n",
       "0                               0.001044         0.001179   0.002011   \n",
       "1                               0.000722         0.011016   0.001292   \n",
       "2                               0.000000         0.000000   0.000000   \n",
       "3                               0.000997         0.001647   0.002328   \n",
       "4                               0.001147         0.001882   0.002267   \n",
       "\n",
       "   vitamin_d_receptor_agonist  wnt_inhibitor  \n",
       "0                    0.016355       0.001657  \n",
       "1                    0.003983       0.003168  \n",
       "2                    0.000000       0.000000  \n",
       "3                    0.000927       0.002869  \n",
       "4                    0.000635       0.002229  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "papermill": {
   "duration": 628.6909,
   "end_time": "2020-12-06T00:04:39.952303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-12-05T23:54:11.261403",
   "version": "2.1.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "0d5efe9081e74b77828ae34616bb3c75": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e737c0c94904a34a8b81494d6f3f447": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "170d2bf78dea45488bc1aa238303a3df": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2008a9c2664f46abaa7ddfaa742f1bb0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "3993e335724944449fc3171dfbaa1712": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8f15acc0584a489bbe7f704ec7999f11",
        "IPY_MODEL_8dc4849cefa843d0b78c00ea35d0258b"
       ],
       "layout": "IPY_MODEL_992619d15967462399ffb2af220629e3"
      }
     },
     "3bbf9d7e8f774b948cb9c5c8981b2752": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "52cfa16f13e74da9be1f8bab9a8ff9eb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5eb5898276284e07901b24f76c7f2b0b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "693d398e9ab64773bc21b344b1f6eb10": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6dfe440ba04d443093074e2030fc9e79": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "7903dfefa48948e28aa27fe505eb03b6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_978739fe4fbb43719d75ac1af3a1a10e",
        "IPY_MODEL_dd9b0ad38d0541bdb6f58e0242d976ef"
       ],
       "layout": "IPY_MODEL_d0414f14f1244995bd874b305154ec7b"
      }
     },
     "82e0bb2e8d214cd7840740aa9a8651dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ba495d1646de46a2b1f2df85243fb3b7",
       "placeholder": "",
       "style": "IPY_MODEL_d6f769b1678043c993f3a6b6fad3c3fe",
       "value": " 206/206 [07:03&lt;00:00,  2.06s/it]"
      }
     },
     "86d3e0091891442a8f43e076ebac9fff": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8dc4849cefa843d0b78c00ea35d0258b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_0d5efe9081e74b77828ae34616bb3c75",
       "placeholder": "",
       "style": "IPY_MODEL_6dfe440ba04d443093074e2030fc9e79",
       "value": " 16/16 [00:30&lt;00:00,  1.91s/it]"
      }
     },
     "8f15acc0584a489bbe7f704ec7999f11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_a58ce001cde04afaae932d2a8354433a",
       "max": 16.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_3bbf9d7e8f774b948cb9c5c8981b2752",
       "value": 16.0
      }
     },
     "928692fc98984c058effff3876a37acc": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_170d2bf78dea45488bc1aa238303a3df",
       "placeholder": "",
       "style": "IPY_MODEL_a811de10e1124ad68a905de7bb388d66",
       "value": " 16/16 [00:26&lt;00:00,  1.69s/it]"
      }
     },
     "9311e45919ff4634bc9e0b673e8ccf6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_86d3e0091891442a8f43e076ebac9fff",
       "max": 206.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_2008a9c2664f46abaa7ddfaa742f1bb0",
       "value": 206.0
      }
     },
     "95452bdb7d20414ba3e6fcdc1f213289": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": "initial"
      }
     },
     "978739fe4fbb43719d75ac1af3a1a10e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_b6b5a1742a84465e9bd31e253e0f5d6c",
       "max": 16.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_0e737c0c94904a34a8b81494d6f3f447",
       "value": 16.0
      }
     },
     "992619d15967462399ffb2af220629e3": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a58ce001cde04afaae932d2a8354433a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a811de10e1124ad68a905de7bb388d66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "b6b5a1742a84465e9bd31e253e0f5d6c": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba495d1646de46a2b1f2df85243fb3b7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "c5e6cb77112b4fa384a4c36fb14b3a66": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "d0414f14f1244995bd874b305154ec7b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d32f77f619b342bc821b669ded8126a6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d6f769b1678043c993f3a6b6fad3c3fe": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "dd9b0ad38d0541bdb6f58e0242d976ef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_d32f77f619b342bc821b669ded8126a6",
       "placeholder": "",
       "style": "IPY_MODEL_c5e6cb77112b4fa384a4c36fb14b3a66",
       "value": " 16/16 [01:32&lt;00:00,  5.76s/it]"
      }
     },
     "dfe0d773d7e74599a214c7aae3ab962c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9311e45919ff4634bc9e0b673e8ccf6e",
        "IPY_MODEL_82e0bb2e8d214cd7840740aa9a8651dd"
       ],
       "layout": "IPY_MODEL_693d398e9ab64773bc21b344b1f6eb10"
      }
     },
     "e180dfe3b04d4105bfc7e4e3b0e40878": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "100%",
       "description_tooltip": null,
       "layout": "IPY_MODEL_52cfa16f13e74da9be1f8bab9a8ff9eb",
       "max": 16.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_95452bdb7d20414ba3e6fcdc1f213289",
       "value": 16.0
      }
     },
     "ffe622ad72e34ea5b51fb6c644335336": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_e180dfe3b04d4105bfc7e4e3b0e40878",
        "IPY_MODEL_928692fc98984c058effff3876a37acc"
       ],
       "layout": "IPY_MODEL_5eb5898276284e07901b24f76c7f2b0b"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
